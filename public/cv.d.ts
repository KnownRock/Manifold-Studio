// Generated by dts-bundle-generator v9.5.1
// npx dts-bundle-generator .\src\types\opencv\index.ts

/**
 * It represents a 4x4 homogeneous transformation matrix `$T$`
 *
 * `\\[T = \\begin{bmatrix} R & t\\\\ 0 & 1\\\\ \\end{bmatrix} \\]`
 *
 * where `$R$` is a 3x3 rotation matrix and `$t$` is a 3x1 translation vector.
 *
 * You can specify `$R$` either by a 3x3 rotation matrix or by a 3x1 rotation vector, which is
 * converted to a 3x3 rotation matrix by the Rodrigues formula.
 *
 * To construct a matrix `$T$` representing first rotation around the axis `$r$` with rotation angle
 * `$|r|$` in radian (right hand rule) and then translation by the vector `$t$`, you can use
 *
 * ```cpp
 * cv::Vec3f r, t;
 * cv::Affine3f T(r, t);
 * ```
 *
 * If you already have the rotation matrix `$R$`, then you can use
 *
 * ```cpp
 * cv::Matx33f R;
 * cv::Affine3f T(R, t);
 * ```
 *
 * To extract the rotation matrix `$R$` from `$T$`, use
 *
 * ```cpp
 * cv::Matx33f R = T.rotation();
 * ```
 *
 * To extract the translation vector `$t$` from `$T$`, use
 *
 * ```cpp
 * cv::Vec3f t = T.translation();
 * ```
 *
 * To extract the rotation vector `$r$` from `$T$`, use
 *
 * ```cpp
 * cv::Vec3f r = T.rvec();
 * ```
 *
 * Note that since the mapping from rotation vectors to rotation matrices is many to one. The returned
 * rotation vector is not necessarily the one you used before to set the matrix.
 *
 * If you have two transformations `$T = T_1 * T_2$`, use
 *
 * ```cpp
 * cv::Affine3f T, T1, T2;
 * T = T2.concatenate(T1);
 * ```
 *
 * To get the inverse transform of `$T$`, use
 *
 * ```cpp
 * cv::Affine3f T, T_inv;
 * T_inv = T.inv();
 * ```
 *
 * Source:
 * [opencv2/core/affine.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/affine.hpp#L129).
 *
 */
export declare class Affine3 {
	matrix: Mat4;
	constructor();
	constructor(affine: Mat4);
	/**
	 *   The resulting 4x4 matrix is
	 *
	 *   `\\[ \\begin{bmatrix} R & t\\\\ 0 & 1\\\\ \\end{bmatrix} \\]`
	 *
	 * @param R 3x3 rotation matrix.
	 *
	 * @param t 3x1 translation vector.
	 */
	constructor(R: Mat3, t?: Vec3);
	/**
	 *   Rodrigues vector.
	 *
	 *   The last row of the current matrix is set to [0,0,0,1].
	 *
	 * @param rvec 3x1 rotation vector. Its direction indicates the rotation axis and its length
	 * indicates the rotation angle in radian (using right hand rule).
	 *
	 * @param t 3x1 translation vector.
	 */
	constructor(rvec: Vec3, t?: Vec3);
	/**
	 *   Combines all constructors above. Supports 4x4, 3x4, 3x3, 1x3, 3x1 sizes of data matrix.
	 *
	 *   The last row of the current matrix is set to [0,0,0,1] when data is not 4x4.
	 *
	 * @param data 1-channel matrix. when it is 4x4, it is copied to the current matrix and t is not
	 * used. When it is 3x4, it is copied to the upper part 3x4 of the current matrix and t is not used.
	 * When it is 3x3, it is copied to the upper left 3x3 part of the current matrix. When it is 3x1 or
	 * 1x3, it is treated as a rotation vector and the Rodrigues formula is used to compute a 3x3 rotation
	 * matrix.
	 *
	 * @param t 3x1 translation vector. It is used only when data is neither 4x4 nor 3x4.
	 */
	constructor(data: Mat, t?: Vec3);
	constructor(vals: float_type);
	cast(arg401: any): Affine3;
	concatenate(affine: Affine3): Affine3;
	/**
	 *   the inverse of the current matrix.
	 */
	inv(method?: int): Affine3;
	/**
	 *   Copy the 3x3 matrix L to the upper left part of the current matrix
	 *
	 *   It sets the upper left 3x3 part of the matrix. The remaining part is unaffected.
	 *
	 * @param L 3x3 matrix.
	 */
	linear(L: Mat3): Mat3;
	/**
	 *   the upper left 3x3 part
	 */
	linear(): Mat3;
	rotate(R: Mat3): Affine3;
	rotate(rvec: Vec3): Affine3;
	/**
	 *   Rotation matrix.
	 *
	 *   Copy the rotation matrix to the upper left 3x3 part of the current matrix. The remaining elements
	 * of the current matrix are not changed.
	 *
	 * @param R 3x3 rotation matrix.
	 */
	rotation(R: Mat3): Mat3;
	/**
	 *   Rodrigues vector.
	 *
	 *   It sets the upper left 3x3 part of the matrix. The remaining part is unaffected.
	 *
	 * @param rvec 3x1 rotation vector. The direction indicates the rotation axis and its length
	 * indicates the rotation angle in radian (using the right thumb convention).
	 */
	rotation(rvec: Vec3): Vec3;
	/**
	 *   Combines rotation methods above. Supports 3x3, 1x3, 3x1 sizes of data matrix.
	 *
	 *   It sets the upper left 3x3 part of the matrix. The remaining part is unaffected.
	 *
	 * @param data 1-channel matrix. When it is a 3x3 matrix, it sets the upper left 3x3 part of the
	 * current matrix. When it is a 1x3 or 3x1 matrix, it is used as a rotation vector. The Rodrigues
	 * formula is used to compute the rotation matrix and sets the upper left 3x3 part of the current
	 * matrix.
	 */
	rotation(data: Mat): Mat;
	/**
	 *   the upper left 3x3 part
	 */
	rotation(): Mat3;
	/**
	 *   Rodrigues vector.
	 *
	 *   a vector representing the upper left 3x3 rotation matrix of the current matrix.
	 *
	 *   Since the mapping between rotation vectors and rotation matrices is many to one, this function
	 * returns only one rotation vector that represents the current rotation matrix, which is not
	 * necessarily the same one set by `[rotation(const Vec3& rvec)]`.
	 */
	rvec(): Vec3;
	translate(t: Vec3): Affine3;
	/**
	 *   Copy t to the first three elements of the last column of the current matrix
	 *
	 *   It sets the upper right 3x1 part of the matrix. The remaining part is unaffected.
	 *
	 * @param t 3x1 translation vector.
	 */
	translation(t: Vec3): Vec3;
	/**
	 *   the upper right 3x1 part
	 */
	translation(): Vec3;
	static Identity(): Affine3;
}
/**
 * especially for classes of algorithms, for which there can be multiple implementations. The examples
 * are stereo correspondence (for which there are algorithms like block matching, semi-global block
 * matching, graph-cut etc.), background subtraction (which can be done using mixture-of-gaussians
 * models, codebook-based algorithm etc.), optical flow (block matching, Lucas-Kanade, Horn-Schunck
 * etc.).
 *
 * Here is example of [SimpleBlobDetector](#d0/d7a/classcv_1_1SimpleBlobDetector}) use in your
 * application via [Algorithm](#d3/d46/classcv_1_1Algorithm}) interface:
 *
 * ```cpp
 *     Ptr<Feature2D> sbd = SimpleBlobDetector::create();
 *     FileStorage fs_read("SimpleBlobDetector_params.xml", FileStorage::READ);
 *
 *     if (fs_read.isOpened()) // if we have file with parameters, read them
 *     {
 *         sbd->read(fs_read.root());
 *         fs_read.release();
 *     }
 *     else // else modify the parameters and store them; user can later edit the file to use different
 * parameters
 *     {
 *         fs_read.release();
 *         FileStorage fs_write("SimpleBlobDetector_params.xml", FileStorage::WRITE);
 *         sbd->write(fs_write);
 *         fs_write.release();
 *     }
 *
 *     Mat result, image = imread("../data/detect_blob.png", IMREAD_COLOR);
 *     vector<KeyPoint> keypoints;
 *     sbd->detect(image, keypoints, Mat());
 *
 *     drawKeypoints(image, keypoints, result);
 *     for (vector<KeyPoint>::iterator k = keypoints.begin(); k != keypoints.end(); ++k)
 *         circle(result, k->pt, (int)k->size, Scalar(0, 0, 255), 2);
 *
 *     imshow("result", result);
 *     waitKey(0);
 * ```
 *
 * Source:
 * [opencv2/core.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core.hpp#L3077).
 *
 */
declare class Algorithm$1 extends EmscriptenEmbindInstance {
	constructor();
	clear(): void;
	empty(): bool;
	/**
	 *   Returns the algorithm string identifier. This string is used as top level xml/yml node tag when
	 * the object is saved to a file or string.
	 */
	getDefaultName(): String;
	read(fn: FileNode): FileNode;
	/**
	 *   Saves the algorithm to a file. In order to make this method work, the derived class must implement
	 * Algorithm::write(FileStorage& fs).
	 */
	save(filename: String): String;
	write(fs: FileStorage): FileStorage;
	write(fs: Ptr, name?: String): Ptr;
	/**
	 *   This is static template method of [Algorithm]. It's usage is following (in the case of SVM):
	 *
	 *   ```cpp
	 *   Ptr<SVM> svm = Algorithm::load<SVM>("my_svm_model.xml");
	 *   ```
	 *
	 *    In order to make this method work, the derived class must overwrite [Algorithm::read](const
	 * [FileNode]& fn).
	 *
	 * @param filename Name of the file to read.
	 *
	 * @param objname The optional name of the node to read (if empty, the first top-level node will be
	 * used)
	 */
	static load(arg0: any, filename: String, objname?: String): Ptr;
	/**
	 *   This is static template method of [Algorithm]. It's usage is following (in the case of SVM):
	 *
	 *   ```cpp
	 *   Ptr<SVM> svm = Algorithm::loadFromString<SVM>(myStringModel);
	 *   ```
	 *
	 * @param strModel The string variable containing the model you want to load.
	 *
	 * @param objname The optional name of the node to read (if empty, the first top-level node will be
	 * used)
	 */
	static loadFromString(arg1: any, strModel: String, objname?: String): Ptr;
	/**
	 *   This is static template method of [Algorithm]. It's usage is following (in the case of SVM):
	 *
	 *   ```cpp
	 *   cv::FileStorage fsRead("example.xml", FileStorage::READ);
	 *   Ptr<SVM> svm = Algorithm::read<SVM>(fsRead.root());
	 *   ```
	 *
	 *    In order to make this method work, the derived class must overwrite [Algorithm::read](const
	 * [FileNode]& fn) and also have static create() method without parameters (or with all the optional
	 * parameters)
	 */
	static read(arg2: any, fn: FileNode): Ptr;
}
/**
 * The class is used for temporary buffers in functions and methods. If a temporary buffer is usually
 * small (a few K's of memory), but its size depends on the parameters, it makes sense to create a
 * small fixed-size array on stack and use it if it's large enough. If the required buffer size is
 * larger than the fixed size, another buffer of sufficient size is allocated dynamically and released
 * after the processing. Therefore, in typical cases, when the buffer size is small, there is no
 * overhead associated with malloc()/free(). At the same time, there is no limit on the size of
 * processed data.
 *
 * This is what [AutoBuffer](#d8/dd0/classcv_1_1AutoBuffer}) does. The template takes 2 parameters -
 * type of the buffer elements and the number of stack-allocated elements. Here is how the class is
 * used:
 *
 * ```cpp
 * void my_func(const cv::Mat& m)
 * {
 *    cv::AutoBuffer<float> buf(1000); // create automatic buffer containing 1000 floats
 *
 *    buf.allocate(m.rows); // if m.rows <= 1000, the pre-allocated buffer is used,
 *                          // otherwise the buffer of "m.rows" floats will be allocated
 *                          // dynamically and deallocated in cv::AutoBuffer destructor
 *    ...
 * }
 * ```
 *
 * Source:
 * [opencv2/core/utility.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/utility.hpp#L128).
 *
 */
export declare class AutoBuffer {
	constructor();
	constructor(_size: size_t);
	constructor(buf: AutoBuffer);
	allocate(_size: size_t): void;
	data(): any;
	data(): any;
	deallocate(): void;
	resize(_size: size_t): void;
	size(): size_t;
}
/**
 * For each descriptor in the first set, this matcher finds the closest descriptor in the second set by
 * trying each one. This descriptor matcher supports masking permissible matches of descriptor sets.
 *
 * Source:
 * [opencv2/features2d.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/features2d.hpp#L1140).
 *
 */
export declare class BFMatcher extends DescriptorMatcher {
	constructor(normType?: int, crossCheck?: bool);
	/**
	 * @param emptyTrainData If emptyTrainData is false, the method creates a deep copy of the object,
	 * that is, copies both parameters and train data. If emptyTrainData is true, the method creates an
	 * object copy with the current parameters but with empty train data.
	 */
	clone(emptyTrainData?: bool): Ptr;
	isMaskSupported(): bool;
	/**
	 * @param normType One of NORM_L1, NORM_L2, NORM_HAMMING, NORM_HAMMING2. L1 and L2 norms are
	 * preferable choices for SIFT and SURF descriptors, NORM_HAMMING should be used with ORB, BRISK and
	 * BRIEF, NORM_HAMMING2 should be used with ORB when WTA_K==3 or 4 (see ORB::ORB constructor
	 * description).
	 *
	 * @param crossCheck If it is false, this is will be default BFMatcher behaviour when it finds the k
	 * nearest neighbors for each query descriptor. If crossCheck==true, then the knnMatch() method with
	 * k=1 will only return pairs (i,j) such that for i-th query descriptor the j-th descriptor in the
	 * matcher's collection is the nearest and vice versa, i.e. the BFMatcher will only return consistent
	 * pairs. Such technique usually produces best results with minimal number of outliers when there are
	 * enough matches. This is alternative to the ratio test, used by D. Lowe in SIFT paper.
	 */
	static create(normType?: int, crossCheck?: bool): Ptr;
}
/**
 * For details, see, for example, *Visual Categorization with Bags of Keypoints* by Gabriella Csurka,
 * Christopher R. Dance, Lixin Fan, Jutta Willamowski, Cedric Bray, 2004. :
 *
 * Source:
 * [opencv2/features2d.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/features2d.hpp#L1339).
 *
 */
export declare class BOWTrainer {
	constructor();
	/**
	 *   The training set is clustered using clustermethod to construct the vocabulary.
	 *
	 * @param descriptors Descriptors to add to a training set. Each row of the descriptors matrix is a
	 * descriptor.
	 */
	add(descriptors: Mat): Mat;
	clear(): void;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	cluster(): Mat;
	/**
	 *   The vocabulary consists of cluster centers. So, this method returns the vocabulary. In the first
	 * variant of the method, train descriptors stored in the object are clustered. In the second variant,
	 * input descriptors are clustered.
	 *
	 * @param descriptors Descriptors to cluster. Each row of the descriptors matrix is a descriptor.
	 * Descriptors are not added to the inner train descriptor set.
	 */
	cluster(descriptors: Mat): Mat;
	descriptorsCount(): int;
	getDescriptors(): Mat;
}
/**
 * the overall RMS re-projection error.
 * The function estimates the intrinsic camera parameters and extrinsic parameters for each of the
 * views. The algorithm is based on Zhang2000 and BouguetMCT . The coordinates of 3D object points and
 * their corresponding 2D projections in each view must be specified. That may be achieved by using an
 * object with a known geometry and easily detectable feature points. Such an object is called a
 * calibration rig or calibration pattern, and OpenCV has built-in support for a chessboard as a
 * calibration rig (see findChessboardCorners ). Currently, initialization of intrinsic parameters
 * (when CALIB_USE_INTRINSIC_GUESS is not set) is only implemented for planar calibration patterns
 * (where Z-coordinates of the object points must be all zeros). 3D calibration rigs can also be used
 * as long as initial cameraMatrix is provided.
 *
 * The algorithm performs the following steps:
 *
 * Compute the initial intrinsic parameters (the option only available for planar calibration patterns)
 * or read them from the input parameters. The distortion coefficients are all set to zeros initially
 * unless some of CALIB_FIX_K? are specified.
 * Estimate the initial camera pose as if the intrinsic parameters have been already known. This is
 * done using solvePnP .
 * Run the global Levenberg-Marquardt optimization algorithm to minimize the reprojection error, that
 * is, the total sum of squared distances between the observed feature points imagePoints and the
 * projected (using the current estimates for camera parameters and the poses) object points
 * objectPoints. See projectPoints for details.
 *
 * If you use a non-square (=non-NxN) grid and findChessboardCorners for calibration, and
 * calibrateCamera returns bad values (zero distortion coefficients, an image center very far from
 * (w/2-0.5,h/2-0.5), and/or large differences between `$f_x$` and `$f_y$` (ratios of 10:1 or more)),
 * then you have probably used patternSize=cvSize(rows,cols) instead of using
 * patternSize=cvSize(cols,rows) in findChessboardCorners .
 *
 * [calibrateCameraRO], [findChessboardCorners], [solvePnP], [initCameraMatrix2D], [stereoCalibrate],
 * [undistort]
 *
 * @param objectPoints In the new interface it is a vector of vectors of calibration pattern points in
 * the calibration pattern coordinate space (e.g. std::vector<std::vector<cv::Vec3f>>). The outer
 * vector contains as many elements as the number of the pattern views. If the same calibration pattern
 * is shown in each view and it is fully visible, all the vectors will be the same. Although, it is
 * possible to use partially occluded patterns, or even different patterns in different views. Then,
 * the vectors will be different. The points are 3D, but since they are in a pattern coordinate system,
 * then, if the rig is planar, it may make sense to put the model to a XY coordinate plane so that
 * Z-coordinate of each input object point is 0. In the old interface all the vectors of object points
 * from different views are concatenated together.
 *
 * @param imagePoints In the new interface it is a vector of vectors of the projections of calibration
 * pattern points (e.g. std::vector<std::vector<cv::Vec2f>>). imagePoints.size() and
 * objectPoints.size() and imagePoints[i].size() must be equal to objectPoints[i].size() for each i. In
 * the old interface all the vectors of object points from different views are concatenated together.
 *
 * @param imageSize Size of the image used only to initialize the intrinsic camera matrix.
 *
 * @param cameraMatrix Output 3x3 floating-point camera matrix $A =
 * \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ . If CV_CALIB_USE_INTRINSIC_GUESS and/or
 * CALIB_FIX_ASPECT_RATIO are specified, some or all of fx, fy, cx, cy must be initialized before
 * calling the function.
 *
 * @param distCoeffs Output vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5,
 * k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])$ of 4, 5, 8, 12 or 14 elements.
 *
 * @param rvecs Output vector of rotation vectors (see Rodrigues ) estimated for each pattern view
 * (e.g. std::vector<cv::Mat>>). That is, each k-th rotation vector together with the corresponding
 * k-th translation vector (see the next output parameter description) brings the calibration pattern
 * from the model coordinate space (in which object points are specified) to the world coordinate
 * space, that is, a real position of the calibration pattern in the k-th pattern view (k=0.. M -1).
 *
 * @param tvecs Output vector of translation vectors estimated for each pattern view.
 *
 * @param stdDeviationsIntrinsics Output vector of standard deviations estimated for intrinsic
 * parameters. Order of deviations values: $(f_x, f_y, c_x, c_y, k_1, k_2, p_1, p_2, k_3, k_4, k_5, k_6
 * , s_1, s_2, s_3, s_4, \tau_x, \tau_y)$ If one of parameters is not estimated, it's deviation is
 * equals to zero.
 *
 * @param stdDeviationsExtrinsics Output vector of standard deviations estimated for extrinsic
 * parameters. Order of deviations values: $(R_1, T_1, \dotsc , R_M, T_M)$ where M is number of pattern
 * views, $R_i, T_i$ are concatenated 1x3 vectors.
 *
 * @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.
 *
 * @param flags Different flags that may be zero or a combination of the following values:
 * CALIB_USE_INTRINSIC_GUESS cameraMatrix contains valid initial values of fx, fy, cx, cy that are
 * optimized further. Otherwise, (cx, cy) is initially set to the image center ( imageSize is used),
 * and focal distances are computed in a least-squares fashion. Note, that if intrinsic parameters are
 * known, there is no need to use this function just to estimate extrinsic parameters. Use solvePnP
 * instead.CALIB_FIX_PRINCIPAL_POINT The principal point is not changed during the global optimization.
 * It stays at the center or at a different location specified when CALIB_USE_INTRINSIC_GUESS is set
 * too.CALIB_FIX_ASPECT_RATIO The functions considers only fy as a free parameter. The ratio fx/fy
 * stays the same as in the input cameraMatrix . When CALIB_USE_INTRINSIC_GUESS is not set, the actual
 * input values of fx and fy are ignored, only their ratio is computed and used
 * further.CALIB_ZERO_TANGENT_DIST Tangential distortion coefficients $(p_1, p_2)$ are set to zeros and
 * stay zero.CALIB_FIX_K1,...,CALIB_FIX_K6 The corresponding radial distortion coefficient is not
 * changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the
 * supplied distCoeffs matrix is used. Otherwise, it is set to 0.CALIB_RATIONAL_MODEL Coefficients k4,
 * k5, and k6 are enabled. To provide the backward compatibility, this extra flag should be explicitly
 * specified to make the calibration function use the rational model and return 8 coefficients. If the
 * flag is not set, the function computes and returns only 5 distortion
 * coefficients.CALIB_THIN_PRISM_MODEL Coefficients s1, s2, s3 and s4 are enabled. To provide the
 * backward compatibility, this extra flag should be explicitly specified to make the calibration
 * function use the thin prism model and return 12 coefficients. If the flag is not set, the function
 * computes and returns only 5 distortion coefficients.CALIB_FIX_S1_S2_S3_S4 The thin prism distortion
 * coefficients are not changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the
 * coefficient from the supplied distCoeffs matrix is used. Otherwise, it is set to
 * 0.CALIB_TILTED_MODEL Coefficients tauX and tauY are enabled. To provide the backward compatibility,
 * this extra flag should be explicitly specified to make the calibration function use the tilted
 * sensor model and return 14 coefficients. If the flag is not set, the function computes and returns
 * only 5 distortion coefficients.CALIB_FIX_TAUX_TAUY The coefficients of the tilted sensor model are
 * not changed during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the
 * supplied distCoeffs matrix is used. Otherwise, it is set to 0.
 *
 * @param criteria Termination criteria for the iterative optimization algorithm.
 */
export declare function calibrateCamera(objectPoints: MatVector, imagePoints: MatVector, imageSize: Size, cameraMatrix: Mat, distCoeffs: Mat, rvecs: MatVector, tvecs: MatVector, stdDeviationsIntrinsics: Mat, stdDeviationsExtrinsics: Mat, perViewErrors: Mat, flags?: int, criteria?: TermCriteria): double;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function calibrateCamera(objectPoints: MatVector, imagePoints: MatVector, imageSize: Size, cameraMatrix: Mat, distCoeffs: Mat, rvecs: MatVector, tvecs: MatVector, flags?: int, criteria?: TermCriteria): double;
/**
 * This function is an extension of [calibrateCamera()] with the method of releasing object which was
 * proposed in strobl2011iccv. In many common cases with inaccurate, unmeasured, roughly planar targets
 * (calibration plates), this method can dramatically improve the precision of the estimated camera
 * parameters. Both the object-releasing method and standard method are supported by this function. Use
 * the parameter **iFixedPoint** for method selection. In the internal implementation,
 * [calibrateCamera()] is a wrapper for this function.
 *
 * the overall RMS re-projection error.
 * The function estimates the intrinsic camera parameters and extrinsic parameters for each of the
 * views. The algorithm is based on Zhang2000, BouguetMCT and strobl2011iccv. See [calibrateCamera()]
 * for other detailed explanations.
 *
 * [calibrateCamera], [findChessboardCorners], [solvePnP], [initCameraMatrix2D], [stereoCalibrate],
 * [undistort]
 *
 * @param objectPoints Vector of vectors of calibration pattern points in the calibration pattern
 * coordinate space. See calibrateCamera() for details. If the method of releasing object to be used,
 * the identical calibration board must be used in each view and it must be fully visible, and all
 * objectPoints[i] must be the same and all points should be roughly close to a plane. The calibration
 * target has to be rigid, or at least static if the camera (rather than the calibration target) is
 * shifted for grabbing images.
 *
 * @param imagePoints Vector of vectors of the projections of calibration pattern points. See
 * calibrateCamera() for details.
 *
 * @param imageSize Size of the image used only to initialize the intrinsic camera matrix.
 *
 * @param iFixedPoint The index of the 3D object point in objectPoints[0] to be fixed. It also acts as
 * a switch for calibration method selection. If object-releasing method to be used, pass in the
 * parameter in the range of [1, objectPoints[0].size()-2], otherwise a value out of this range will
 * make standard calibration method selected. Usually the top-right corner point of the calibration
 * board grid is recommended to be fixed when object-releasing method being utilized. According to
 * strobl2011iccv, two other points are also fixed. In this implementation, objectPoints[0].front and
 * objectPoints[0].back.z are used. With object-releasing method, accurate rvecs, tvecs and
 * newObjPoints are only possible if coordinates of these three fixed points are accurate enough.
 *
 * @param cameraMatrix Output 3x3 floating-point camera matrix. See calibrateCamera() for details.
 *
 * @param distCoeffs Output vector of distortion coefficients. See calibrateCamera() for details.
 *
 * @param rvecs Output vector of rotation vectors estimated for each pattern view. See
 * calibrateCamera() for details.
 *
 * @param tvecs Output vector of translation vectors estimated for each pattern view.
 *
 * @param newObjPoints The updated output vector of calibration pattern points. The coordinates might
 * be scaled based on three fixed points. The returned coordinates are accurate only if the above
 * mentioned three fixed points are accurate. If not needed, noArray() can be passed in. This parameter
 * is ignored with standard calibration method.
 *
 * @param stdDeviationsIntrinsics Output vector of standard deviations estimated for intrinsic
 * parameters. See calibrateCamera() for details.
 *
 * @param stdDeviationsExtrinsics Output vector of standard deviations estimated for extrinsic
 * parameters. See calibrateCamera() for details.
 *
 * @param stdDeviationsObjPoints Output vector of standard deviations estimated for refined coordinates
 * of calibration pattern points. It has the same size and order as objectPoints[0] vector. This
 * parameter is ignored with standard calibration method.
 *
 * @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.
 *
 * @param flags Different flags that may be zero or a combination of some predefined values. See
 * calibrateCamera() for details. If the method of releasing object is used, the calibration time may
 * be much longer. CALIB_USE_QR or CALIB_USE_LU could be used for faster calibration with potentially
 * less precise and less stable in some rare cases.
 *
 * @param criteria Termination criteria for the iterative optimization algorithm.
 */
export declare function calibrateCameraRO(objectPoints: MatVector, imagePoints: MatVector, imageSize: Size, iFixedPoint: int, cameraMatrix: Mat, distCoeffs: Mat, rvecs: MatVector, tvecs: MatVector, newObjPoints: Mat, stdDeviationsIntrinsics: Mat, stdDeviationsExtrinsics: Mat, stdDeviationsObjPoints: Mat, perViewErrors: Mat, flags?: int, criteria?: TermCriteria): double;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function calibrateCameraRO(objectPoints: MatVector, imagePoints: MatVector, imageSize: Size, iFixedPoint: int, cameraMatrix: Mat, distCoeffs: Mat, rvecs: MatVector, tvecs: MatVector, newObjPoints: Mat, flags?: int, criteria?: TermCriteria): double;
/**
 * The function performs the Hand-Eye calibration using various methods. One approach consists in
 * estimating the rotation then the translation (separable solutions) and the following methods are
 * implemented:
 *
 * R. Tsai, R. Lenz A New Technique for Fully Autonomous and Efficient 3D Robotics Hand/EyeCalibration
 * Tsai89
 * F. Park, B. Martin Robot Sensor Calibration: Solving AX = XB on the Euclidean Group Park94
 * R. Horaud, F. Dornaika Hand-Eye Calibration Horaud95
 *
 * Another approach consists in estimating simultaneously the rotation and the translation
 * (simultaneous solutions), with the following implemented method:
 *
 * N. Andreff, R. Horaud, B. Espiau On-line Hand-Eye Calibration Andreff99
 * K. Daniilidis Hand-Eye Calibration Using Dual Quaternions Daniilidis98
 *
 * The following picture describes the Hand-Eye calibration problem where the transformation between a
 * camera ("eye") mounted on a robot gripper ("hand") has to be estimated.
 *
 * The calibration procedure is the following:
 *
 * a static calibration pattern is used to estimate the transformation between the target frame and the
 * camera frame
 * the robot gripper is moved in order to acquire several poses
 * for each pose, the homogeneous transformation between the gripper frame and the robot base frame is
 * recorded using for instance the robot kinematics `\\[ \\begin{bmatrix} X_b\\\\ Y_b\\\\ Z_b\\\\ 1
 * \\end{bmatrix} = \\begin{bmatrix} _{}^{b}\\textrm{R}_g & _{}^{b}\\textrm{t}_g \\\\ 0_{1 \\times 3} &
 * 1 \\end{bmatrix} \\begin{bmatrix} X_g\\\\ Y_g\\\\ Z_g\\\\ 1 \\end{bmatrix} \\]`
 * for each pose, the homogeneous transformation between the calibration target frame and the camera
 * frame is recorded using for instance a pose estimation method (PnP) from 2D-3D point correspondences
 * `\\[ \\begin{bmatrix} X_c\\\\ Y_c\\\\ Z_c\\\\ 1 \\end{bmatrix} = \\begin{bmatrix}
 * _{}^{c}\\textrm{R}_t & _{}^{c}\\textrm{t}_t \\\\ 0_{1 \\times 3} & 1 \\end{bmatrix} \\begin{bmatrix}
 * X_t\\\\ Y_t\\\\ Z_t\\\\ 1 \\end{bmatrix} \\]`
 *
 * The Hand-Eye calibration procedure returns the following homogeneous transformation `\\[
 * \\begin{bmatrix} X_g\\\\ Y_g\\\\ Z_g\\\\ 1 \\end{bmatrix} = \\begin{bmatrix} _{}^{g}\\textrm{R}_c &
 * _{}^{g}\\textrm{t}_c \\\\ 0_{1 \\times 3} & 1 \\end{bmatrix} \\begin{bmatrix} X_c\\\\ Y_c\\\\
 * Z_c\\\\ 1 \\end{bmatrix} \\]`
 *
 * This problem is also known as solving the `$\\mathbf{A}\\mathbf{X}=\\mathbf{X}\\mathbf{B}$`
 * equation: `\\[ \\begin{align*} ^{b}{\\textrm{T}_g}^{(1)} \\hspace{0.2em} ^{g}\\textrm{T}_c
 * \\hspace{0.2em} ^{c}{\\textrm{T}_t}^{(1)} &= \\hspace{0.1em} ^{b}{\\textrm{T}_g}^{(2)}
 * \\hspace{0.2em} ^{g}\\textrm{T}_c \\hspace{0.2em} ^{c}{\\textrm{T}_t}^{(2)} \\\\
 * (^{b}{\\textrm{T}_g}^{(2)})^{-1} \\hspace{0.2em} ^{b}{\\textrm{T}_g}^{(1)} \\hspace{0.2em}
 * ^{g}\\textrm{T}_c &= \\hspace{0.1em} ^{g}\\textrm{T}_c \\hspace{0.2em} ^{c}{\\textrm{T}_t}^{(2)}
 * (^{c}{\\textrm{T}_t}^{(1)})^{-1} \\\\ \\textrm{A}_i \\textrm{X} &= \\textrm{X} \\textrm{B}_i \\\\
 * \\end{align*} \\]`
 *
 * Additional information can be found on this .
 *
 * A minimum of 2 motions with non parallel rotation axes are necessary to determine the hand-eye
 * transformation. So at least 3 different poses are required, but it is strongly recommended to use
 * many more poses.
 *
 * @param R_gripper2base Rotation part extracted from the homogeneous matrix that transforms a point
 * expressed in the gripper frame to the robot base frame ( $_{}^{b}\textrm{T}_g$). This is a vector
 * (vector<Mat>) that contains the rotation matrices for all the transformations from gripper frame to
 * robot base frame.
 *
 * @param t_gripper2base Translation part extracted from the homogeneous matrix that transforms a point
 * expressed in the gripper frame to the robot base frame ( $_{}^{b}\textrm{T}_g$). This is a vector
 * (vector<Mat>) that contains the translation vectors for all the transformations from gripper frame
 * to robot base frame.
 *
 * @param R_target2cam Rotation part extracted from the homogeneous matrix that transforms a point
 * expressed in the target frame to the camera frame ( $_{}^{c}\textrm{T}_t$). This is a vector
 * (vector<Mat>) that contains the rotation matrices for all the transformations from calibration
 * target frame to camera frame.
 *
 * @param t_target2cam Rotation part extracted from the homogeneous matrix that transforms a point
 * expressed in the target frame to the camera frame ( $_{}^{c}\textrm{T}_t$). This is a vector
 * (vector<Mat>) that contains the translation vectors for all the transformations from calibration
 * target frame to camera frame.
 *
 * @param R_cam2gripper Estimated rotation part extracted from the homogeneous matrix that transforms a
 * point expressed in the camera frame to the gripper frame ( $_{}^{g}\textrm{T}_c$).
 *
 * @param t_cam2gripper Estimated translation part extracted from the homogeneous matrix that
 * transforms a point expressed in the camera frame to the gripper frame ( $_{}^{g}\textrm{T}_c$).
 *
 * @param method One of the implemented Hand-Eye calibration method, see cv::HandEyeCalibrationMethod
 */
export declare function calibrateHandEye(R_gripper2base: MatVector, t_gripper2base: MatVector, R_target2cam: MatVector, t_target2cam: MatVector, R_cam2gripper: Mat, t_cam2gripper: Mat, method?: HandEyeCalibrationMethod): void;
/**
 * The function computes various useful camera characteristics from the previously estimated camera
 * matrix.
 *
 * Do keep in mind that the unity measure 'mm' stands for whatever unit of measure one chooses for the
 * chessboard pitch (it can thus be any value).
 *
 * @param cameraMatrix Input camera matrix that can be estimated by calibrateCamera or stereoCalibrate
 * .
 *
 * @param imageSize Input image size in pixels.
 *
 * @param apertureWidth Physical width in mm of the sensor.
 *
 * @param apertureHeight Physical height in mm of the sensor.
 *
 * @param fovx Output field of view in degrees along the horizontal sensor axis.
 *
 * @param fovy Output field of view in degrees along the vertical sensor axis.
 *
 * @param focalLength Focal length of the lens in mm.
 *
 * @param principalPoint Principal point in mm.
 *
 * @param aspectRatio $f_y/f_x$
 */
export declare function calibrationMatrixValues(cameraMatrix: Mat, imageSize: Size, apertureWidth: double, apertureHeight: double, fovx: any, fovy: any, focalLength: any, principalPoint: any, aspectRatio: any): void;
export declare function checkChessboard(img: Mat, size: Size): bool;
/**
 * The functions compute:
 *
 * `\\[\\begin{array}{l} \\texttt{rvec3} = \\mathrm{rodrigues} ^{-1} \\left ( \\mathrm{rodrigues} (
 * \\texttt{rvec2} ) \\cdot \\mathrm{rodrigues} ( \\texttt{rvec1} ) \\right ) \\\\ \\texttt{tvec3} =
 * \\mathrm{rodrigues} ( \\texttt{rvec2} ) \\cdot \\texttt{tvec1} + \\texttt{tvec2} \\end{array} ,\\]`
 *
 * where `$\\mathrm{rodrigues}$` denotes a rotation vector to a rotation matrix transformation, and
 * `$\\mathrm{rodrigues}^{-1}$` denotes the inverse transformation. See Rodrigues for details.
 *
 * Also, the functions can compute the derivatives of the output vectors with regards to the input
 * vectors (see matMulDeriv ). The functions are used inside stereoCalibrate but can also be used in
 * your own code where Levenberg-Marquardt or another gradient-based solver is used to optimize a
 * function that contains a matrix multiplication.
 *
 * @param rvec1 First rotation vector.
 *
 * @param tvec1 First translation vector.
 *
 * @param rvec2 Second rotation vector.
 *
 * @param tvec2 Second translation vector.
 *
 * @param rvec3 Output rotation vector of the superposition.
 *
 * @param tvec3 Output translation vector of the superposition.
 *
 * @param dr3dr1 Optional output derivative of rvec3 with regard to rvec1
 *
 * @param dr3dt1 Optional output derivative of rvec3 with regard to tvec1
 *
 * @param dr3dr2 Optional output derivative of rvec3 with regard to rvec2
 *
 * @param dr3dt2 Optional output derivative of rvec3 with regard to tvec2
 *
 * @param dt3dr1 Optional output derivative of tvec3 with regard to rvec1
 *
 * @param dt3dt1 Optional output derivative of tvec3 with regard to tvec1
 *
 * @param dt3dr2 Optional output derivative of tvec3 with regard to rvec2
 *
 * @param dt3dt2 Optional output derivative of tvec3 with regard to tvec2
 */
export declare function composeRT(rvec1: Mat, tvec1: Mat, rvec2: Mat, tvec2: Mat, rvec3: Mat, tvec3: Mat, dr3dr1?: Mat, dr3dt1?: Mat, dr3dr2?: Mat, dr3dt2?: Mat, dt3dr1?: Mat, dt3dt1?: Mat, dt3dr2?: Mat, dt3dt2?: Mat): void;
/**
 * For every point in one of the two images of a stereo pair, the function finds the equation of the
 * corresponding epipolar line in the other image.
 *
 * From the fundamental matrix definition (see findFundamentalMat ), line `$l^{(2)}_i$` in the second
 * image for the point `$p^{(1)}_i$` in the first image (when whichImage=1 ) is computed as:
 *
 * `\\[l^{(2)}_i = F p^{(1)}_i\\]`
 *
 * And vice versa, when whichImage=2, `$l^{(1)}_i$` is computed from `$p^{(2)}_i$` as:
 *
 * `\\[l^{(1)}_i = F^T p^{(2)}_i\\]`
 *
 * Line coefficients are defined up to a scale. They are normalized so that `$a_i^2+b_i^2=1$` .
 *
 * @param points Input points. $N \times 1$ or $1 \times N$ matrix of type CV_32FC2 or vector<Point2f>
 * .
 *
 * @param whichImage Index of the image (1 or 2) that contains the points .
 *
 * @param F Fundamental matrix that can be estimated using findFundamentalMat or stereoRectify .
 *
 * @param lines Output vector of the epipolar lines corresponding to the points in the other image.
 * Each line $ax + by + c=0$ is encoded by 3 numbers $(a, b, c)$ .
 */
export declare function computeCorrespondEpilines(points: Mat, whichImage: int, F: Mat, lines: Mat): void;
/**
 * The function converts points homogeneous to Euclidean space using perspective projection. That is,
 * each point (x1, x2, ... x(n-1), xn) is converted to (x1/xn, x2/xn, ..., x(n-1)/xn). When xn=0, the
 * output point coordinates will be (0,0,0,...).
 *
 * @param src Input vector of N-dimensional points.
 *
 * @param dst Output vector of N-1-dimensional points.
 */
export declare function convertPointsFromHomogeneous(src: Mat, dst: Mat): void;
/**
 * The function converts 2D or 3D points from/to homogeneous coordinates by calling either
 * convertPointsToHomogeneous or convertPointsFromHomogeneous.
 *
 * The function is obsolete. Use one of the previous two functions instead.
 *
 * @param src Input array or vector of 2D, 3D, or 4D points.
 *
 * @param dst Output vector of 2D, 3D, or 4D points.
 */
export declare function convertPointsHomogeneous(src: Mat, dst: Mat): void;
/**
 * The function converts points from Euclidean to homogeneous space by appending 1's to the tuple of
 * point coordinates. That is, each point (x1, x2, ..., xn) is converted to (x1, x2, ..., xn, 1).
 *
 * @param src Input vector of N-dimensional points.
 *
 * @param dst Output vector of N+1-dimensional points.
 */
export declare function convertPointsToHomogeneous(src: Mat, dst: Mat): void;
/**
 * The function implements the Optimal Triangulation Method (see Multiple View Geometry for details).
 * For each given point correspondence points1[i] <-> points2[i], and a fundamental matrix F, it
 * computes the corrected correspondences newPoints1[i] <-> newPoints2[i] that minimize the geometric
 * error `$d(points1[i], newPoints1[i])^2 + d(points2[i],newPoints2[i])^2$` (where `$d(a,b)$` is the
 * geometric distance between points `$a$` and `$b$` ) subject to the epipolar constraint
 * `$newPoints2^T * F * newPoints1 = 0$` .
 *
 * @param F 3x3 fundamental matrix.
 *
 * @param points1 1xN array containing the first set of points.
 *
 * @param points2 1xN array containing the second set of points.
 *
 * @param newPoints1 The optimized points1.
 *
 * @param newPoints2 The optimized points2.
 */
export declare function correctMatches(F: Mat, points1: Mat, points2: Mat, newPoints1: Mat, newPoints2: Mat): void;
/**
 * This function decompose an essential matrix E using svd decomposition HartleyZ00 . Generally 4
 * possible poses exists for a given E. They are `$[R_1, t]$`, `$[R_1, -t]$`, `$[R_2, t]$`, `$[R_2,
 * -t]$`. By decomposing E, you can only get the direction of the translation, so the function returns
 * unit t.
 *
 * @param E The input essential matrix.
 *
 * @param R1 One possible rotation matrix.
 *
 * @param R2 Another possible rotation matrix.
 *
 * @param t One possible translation.
 */
export declare function decomposeEssentialMat(E: Mat, R1: Mat, R2: Mat, t: Mat): void;
/**
 * This function extracts relative camera motion between two views observing a planar object from the
 * homography H induced by the plane. The intrinsic camera matrix K must also be provided. The function
 * may return up to four mathematical solution sets. At least two of the solutions may further be
 * invalidated if point correspondences are available by applying positive depth constraint (all points
 * must be in front of the camera). The decomposition method is described in detail in Malis .
 *
 * @param H The input homography matrix between two images.
 *
 * @param K The input intrinsic camera calibration matrix.
 *
 * @param rotations Array of rotation matrices.
 *
 * @param translations Array of translation matrices.
 *
 * @param normals Array of plane normal matrices.
 */
export declare function decomposeHomographyMat(H: Mat, K: Mat, rotations: MatVector, translations: MatVector, normals: MatVector): int;
/**
 * The function computes a decomposition of a projection matrix into a calibration and a rotation
 * matrix and the position of a camera.
 *
 * It optionally returns three rotation matrices, one for each axis, and three Euler angles that could
 * be used in OpenGL. Note, there is always more than one sequence of rotations about the three
 * principal axes that results in the same orientation of an object, e.g. see Slabaugh . Returned tree
 * rotation matrices and corresponding three Euler angles are only one of the possible solutions.
 *
 * The function is based on RQDecomp3x3 .
 *
 * @param projMatrix 3x4 input projection matrix P.
 *
 * @param cameraMatrix Output 3x3 camera matrix K.
 *
 * @param rotMatrix Output 3x3 external rotation matrix R.
 *
 * @param transVect Output 4x1 translation vector T.
 *
 * @param rotMatrixX Optional 3x3 rotation matrix around x-axis.
 *
 * @param rotMatrixY Optional 3x3 rotation matrix around y-axis.
 *
 * @param rotMatrixZ Optional 3x3 rotation matrix around z-axis.
 *
 * @param eulerAngles Optional three-element vector containing three Euler angles of rotation in
 * degrees.
 */
export declare function decomposeProjectionMatrix(projMatrix: Mat, cameraMatrix: Mat, rotMatrix: Mat, transVect: Mat, rotMatrixX?: Mat, rotMatrixY?: Mat, rotMatrixZ?: Mat, eulerAngles?: Mat): void;
/**
 * The function draws individual chessboard corners detected either as red circles if the board was not
 * found, or as colored corners connected with lines if the board was found.
 *
 * @param image Destination image. It must be an 8-bit color image.
 *
 * @param patternSize Number of inner corners per a chessboard row and column (patternSize =
 * cv::Size(points_per_row,points_per_column)).
 *
 * @param corners Array of detected corners, the output of findChessboardCorners.
 *
 * @param patternWasFound Parameter indicating whether the complete board was found or not. The return
 * value of findChessboardCorners should be passed here.
 */
export declare function drawChessboardCorners(image: Mat, patternSize: Size, corners: Mat, patternWasFound: bool): void;
/**
 * [solvePnP]
 *
 * This function draws the axes of the world/object coordinate system w.r.t. to the camera frame. OX is
 * drawn in red, OY in green and OZ in blue.
 *
 * @param image Input/output image. It must have 1 or 3 channels. The number of channels is not
 * altered.
 *
 * @param cameraMatrix Input 3x3 floating-point matrix of camera intrinsic parameters. $A =
 * \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$
 *
 * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6
 * [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is empty,
 * the zero distortion coefficients are assumed.
 *
 * @param rvec Rotation vector (see Rodrigues ) that, together with tvec, brings points from the model
 * coordinate system to the camera coordinate system.
 *
 * @param tvec Translation vector.
 *
 * @param length Length of the painted axes in the same unit than tvec (usually in meters).
 *
 * @param thickness Line thickness of the painted axes.
 */
export declare function drawFrameAxes(image: Mat, cameraMatrix: Mat, distCoeffs: Mat, rvec: Mat, tvec: Mat, length: float, thickness?: int): void;
/**
 * It computes `\\[ \\begin{bmatrix} x\\\\ y\\\\ \\end{bmatrix} = \\begin{bmatrix} a_{11} & a_{12}\\\\
 * a_{21} & a_{22}\\\\ \\end{bmatrix} \\begin{bmatrix} X\\\\ Y\\\\ \\end{bmatrix} + \\begin{bmatrix}
 * b_1\\\\ b_2\\\\ \\end{bmatrix} \\]`
 *
 * Output 2D affine transformation matrix `$2 \\times 3$` or empty matrix if transformation could not
 * be estimated. The returned matrix has the following form: `\\[ \\begin{bmatrix} a_{11} & a_{12} &
 * b_1\\\\ a_{21} & a_{22} & b_2\\\\ \\end{bmatrix} \\]`
 * The function estimates an optimal 2D affine transformation between two 2D point sets using the
 * selected robust algorithm.
 *
 * The computed transformation is then refined further (using only inliers) with the
 * Levenberg-Marquardt method to reduce the re-projection error even more.
 *
 * The RANSAC method can handle practically any ratio of outliers but needs a threshold to distinguish
 * inliers from outliers. The method LMeDS does not need any threshold but it works correctly only when
 * there are more than 50% of inliers.
 *
 * [estimateAffinePartial2D], [getAffineTransform]
 *
 * @param from First input 2D point set containing $(X,Y)$.
 *
 * @param to Second input 2D point set containing $(x,y)$.
 *
 * @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).
 *
 * @param method Robust method used to compute transformation. The following methods are possible:
 * cv::RANSAC - RANSAC-based robust methodcv::LMEDS - Least-Median robust method RANSAC is the default
 * method.
 *
 * @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider a point
 * as an inlier. Applies only to RANSAC.
 *
 * @param maxIters The maximum number of robust method iterations.
 *
 * @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything
 * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
 * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
 *
 * @param refineIters Maximum number of iterations of refining algorithm (Levenberg-Marquardt). Passing
 * 0 will disable refining, so the output matrix will be output of robust method.
 */
export declare function estimateAffine2D(from: Mat, to: Mat, inliers?: Mat, method?: int, ransacReprojThreshold?: double, maxIters?: size_t, confidence?: double, refineIters?: size_t): any;
/**
 * It computes `\\[ \\begin{bmatrix} x\\\\ y\\\\ z\\\\ \\end{bmatrix} = \\begin{bmatrix} a_{11} &
 * a_{12} & a_{13}\\\\ a_{21} & a_{22} & a_{23}\\\\ a_{31} & a_{32} & a_{33}\\\\ \\end{bmatrix}
 * \\begin{bmatrix} X\\\\ Y\\\\ Z\\\\ \\end{bmatrix} + \\begin{bmatrix} b_1\\\\ b_2\\\\ b_3\\\\
 * \\end{bmatrix} \\]`
 *
 * The function estimates an optimal 3D affine transformation between two 3D point sets using the
 * RANSAC algorithm.
 *
 * @param src First input 3D point set containing $(X,Y,Z)$.
 *
 * @param dst Second input 3D point set containing $(x,y,z)$.
 *
 * @param out Output 3D affine transformation matrix $3 \times 4$ of the form \[ \begin{bmatrix} a_{11}
 * & a_{12} & a_{13} & b_1\\ a_{21} & a_{22} & a_{23} & b_2\\ a_{31} & a_{32} & a_{33} & b_3\\
 * \end{bmatrix} \]
 *
 * @param inliers Output vector indicating which points are inliers (1-inlier, 0-outlier).
 *
 * @param ransacThreshold Maximum reprojection error in the RANSAC algorithm to consider a point as an
 * inlier.
 *
 * @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything
 * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
 * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
 */
export declare function estimateAffine3D(src: Mat, dst: Mat, out: Mat, inliers: Mat, ransacThreshold?: double, confidence?: double): int;
/**
 * Output 2D affine transformation (4 degrees of freedom) matrix `$2 \\times 3$` or empty matrix if
 * transformation could not be estimated.
 * The function estimates an optimal 2D affine transformation with 4 degrees of freedom limited to
 * combinations of translation, rotation, and uniform scaling. Uses the selected algorithm for robust
 * estimation.
 *
 * The computed transformation is then refined further (using only inliers) with the
 * Levenberg-Marquardt method to reduce the re-projection error even more.
 *
 * Estimated transformation matrix is: `\\[ \\begin{bmatrix} \\cos(\\theta) \\cdot s & -\\sin(\\theta)
 * \\cdot s & t_x \\\\ \\sin(\\theta) \\cdot s & \\cos(\\theta) \\cdot s & t_y \\end{bmatrix} \\]`
 * Where `$ \\theta $` is the rotation angle, `$ s $` the scaling factor and `$ t_x, t_y $` are
 * translations in `$ x, y $` axes respectively.
 *
 * The RANSAC method can handle practically any ratio of outliers but need a threshold to distinguish
 * inliers from outliers. The method LMeDS does not need any threshold but it works correctly only when
 * there are more than 50% of inliers.
 *
 * [estimateAffine2D], [getAffineTransform]
 *
 * @param from First input 2D point set.
 *
 * @param to Second input 2D point set.
 *
 * @param inliers Output vector indicating which points are inliers.
 *
 * @param method Robust method used to compute transformation. The following methods are possible:
 * cv::RANSAC - RANSAC-based robust methodcv::LMEDS - Least-Median robust method RANSAC is the default
 * method.
 *
 * @param ransacReprojThreshold Maximum reprojection error in the RANSAC algorithm to consider a point
 * as an inlier. Applies only to RANSAC.
 *
 * @param maxIters The maximum number of robust method iterations.
 *
 * @param confidence Confidence level, between 0 and 1, for the estimated transformation. Anything
 * between 0.95 and 0.99 is usually good enough. Values too close to 1 can slow down the estimation
 * significantly. Values lower than 0.8-0.9 can result in an incorrectly estimated transformation.
 *
 * @param refineIters Maximum number of iterations of refining algorithm (Levenberg-Marquardt). Passing
 * 0 will disable refining, so the output matrix will be output of robust method.
 */
export declare function estimateAffinePartial2D(from: Mat, to: Mat, inliers?: Mat, method?: int, ransacReprojThreshold?: double, maxIters?: size_t, confidence?: double, refineIters?: size_t): any;
/**
 * This function is intended to filter the output of the decomposeHomographyMat based on additional
 * information as described in Malis . The summary of the method: the decomposeHomographyMat function
 * returns 2 unique solutions and their "opposites" for a total of 4 solutions. If we have access to
 * the sets of points visible in the camera frame before and after the homography transformation is
 * applied, we can determine which are the true potential solutions and which are the opposites by
 * verifying which homographies are consistent with all visible reference points being in front of the
 * camera. The inputs are left unchanged; the filtered solution set is returned as indices into the
 * existing one.
 *
 * @param rotations Vector of rotation matrices.
 *
 * @param normals Vector of plane normal matrices.
 *
 * @param beforePoints Vector of (rectified) visible reference points before the homography is applied
 *
 * @param afterPoints Vector of (rectified) visible reference points after the homography is applied
 *
 * @param possibleSolutions Vector of int indices representing the viable solution set after filtering
 *
 * @param pointsMask optional Mat/Vector of 8u type representing the mask for the inliers as given by
 * the findHomography function
 */
export declare function filterHomographyDecompByVisibleRefpoints(rotations: MatVector, normals: MatVector, beforePoints: Mat, afterPoints: Mat, possibleSolutions: Mat, pointsMask?: Mat): void;
/**
 * @param img The input 16-bit signed disparity image
 *
 * @param newVal The disparity value used to paint-off the speckles
 *
 * @param maxSpeckleSize The maximum speckle size to consider it a speckle. Larger blobs are not
 * affected by the algorithm
 *
 * @param maxDiff Maximum difference between neighbor disparity pixels to put them into the same blob.
 * Note that since StereoBM, StereoSGBM and may be other algorithms return a fixed-point disparity map,
 * where disparity values are multiplied by 16, this scale factor should be taken into account when
 * specifying this parameter value.
 *
 * @param buf The optional temporary buffer to avoid memory allocation within the function.
 */
export declare function filterSpeckles(img: Mat, newVal: double, maxSpeckleSize: int, maxDiff: double, buf?: Mat): void;
export declare function find4QuadCornerSubpix(img: Mat, corners: Mat, region_size: Size): bool;
/**
 * The function attempts to determine whether the input image is a view of the chessboard pattern and
 * locate the internal chessboard corners. The function returns a non-zero value if all of the corners
 * are found and they are placed in a certain order (row by row, left to right in every row).
 * Otherwise, if the function fails to find all the corners or reorder them, it returns 0. For example,
 * a regular chessboard has 8 x 8 squares and 7 x 7 internal corners, that is, points where the black
 * squares touch each other. The detected coordinates are approximate, and to determine their positions
 * more accurately, the function calls cornerSubPix. You also may use the function cornerSubPix with
 * different parameters if returned coordinates are not accurate enough.
 *
 * Sample usage of detecting and drawing chessboard corners: :
 *
 * ```cpp
 * Size patternsize(8,6); //interior number of corners
 * Mat gray = ....; //source image
 * vector<Point2f> corners; //this will be filled by the detected corners
 *
 * //CALIB_CB_FAST_CHECK saves a lot of time on images
 * //that do not contain any chessboard corners
 * bool patternfound = findChessboardCorners(gray, patternsize, corners,
 *         CALIB_CB_ADAPTIVE_THRESH + CALIB_CB_NORMALIZE_IMAGE
 *         + CALIB_CB_FAST_CHECK);
 *
 * if(patternfound)
 *   cornerSubPix(gray, corners, Size(11, 11), Size(-1, -1),
 *     TermCriteria(CV_TERMCRIT_EPS + CV_TERMCRIT_ITER, 30, 0.1));
 *
 * drawChessboardCorners(img, patternsize, Mat(corners), patternfound);
 * ```
 *
 * The function requires white space (like a square-thick border, the wider the better) around the
 * board to make the detection more robust in various environments. Otherwise, if there is no border
 * and the background is dark, the outer black squares cannot be segmented properly and so the square
 * grouping and ordering algorithm fails.
 *
 * @param image Source chessboard view. It must be an 8-bit grayscale or color image.
 *
 * @param patternSize Number of inner corners per a chessboard row and column ( patternSize =
 * cv::Size(points_per_row,points_per_colum) = cv::Size(columns,rows) ).
 *
 * @param corners Output array of detected corners.
 *
 * @param flags Various operation flags that can be zero or a combination of the following values:
 * CALIB_CB_ADAPTIVE_THRESH Use adaptive thresholding to convert the image to black and white, rather
 * than a fixed threshold level (computed from the average image brightness).CALIB_CB_NORMALIZE_IMAGE
 * Normalize the image gamma with equalizeHist before applying fixed or adaptive
 * thresholding.CALIB_CB_FILTER_QUADS Use additional criteria (like contour area, perimeter,
 * square-like shape) to filter out false quads extracted at the contour retrieval
 * stage.CALIB_CB_FAST_CHECK Run a fast check on the image that looks for chessboard corners, and
 * shortcut the call if none is found. This can drastically speed up the call in the degenerate
 * condition when no chessboard is observed.
 */
export declare function findChessboardCorners(image: Mat, patternSize: Size, corners: Mat, flags?: int): bool;
/**
 * The function is analog to findchessboardCorners but uses a localized radon transformation
 * approximated by box filters being more robust to all sort of noise, faster on larger images and is
 * able to directly return the sub-pixel position of the internal chessboard corners. The Method is
 * based on the paper duda2018 "Accurate Detection and Localization of Checkerboard Corners for
 * Calibration" demonstrating that the returned sub-pixel positions are more accurate than the one
 * returned by cornerSubPix allowing a precise camera calibration for demanding applications.
 *
 * The function requires a white boarder with roughly the same width as one of the checkerboard fields
 * around the whole board to improve the detection in various environments. In addition, because of the
 * localized radon transformation it is beneficial to use round corners for the field corners which are
 * located on the outside of the board. The following figure illustrates a sample checkerboard
 * optimized for the detection. However, any other checkerboard can be used as well.
 *
 * @param image Source chessboard view. It must be an 8-bit grayscale or color image.
 *
 * @param patternSize Number of inner corners per a chessboard row and column ( patternSize =
 * cv::Size(points_per_row,points_per_colum) = cv::Size(columns,rows) ).
 *
 * @param corners Output array of detected corners.
 *
 * @param flags Various operation flags that can be zero or a combination of the following values:
 * CALIB_CB_NORMALIZE_IMAGE Normalize the image gamma with equalizeHist before
 * detection.CALIB_CB_EXHAUSTIVE Run an exhaustive search to improve detection rate.CALIB_CB_ACCURACY
 * Up sample input image to improve sub-pixel accuracy due to aliasing effects. This should be used if
 * an accurate camera calibration is required.
 */
export declare function findChessboardCornersSB(image: Mat, patternSize: Size, corners: Mat, flags?: int): bool;
/**
 * The function attempts to determine whether the input image contains a grid of circles. If it is, the
 * function locates centers of the circles. The function returns a non-zero value if all of the centers
 * have been found and they have been placed in a certain order (row by row, left to right in every
 * row). Otherwise, if the function fails to find all the corners or reorder them, it returns 0.
 *
 * Sample usage of detecting and drawing the centers of circles: :
 *
 * ```cpp
 * Size patternsize(7,7); //number of centers
 * Mat gray = ....; //source image
 * vector<Point2f> centers; //this will be filled by the detected centers
 *
 * bool patternfound = findCirclesGrid(gray, patternsize, centers);
 *
 * drawChessboardCorners(img, patternsize, Mat(centers), patternfound);
 * ```
 *
 * The function requires white space (like a square-thick border, the wider the better) around the
 * board to make the detection more robust in various environments.
 *
 * @param image grid view of input circles; it must be an 8-bit grayscale or color image.
 *
 * @param patternSize number of circles per row and column ( patternSize = Size(points_per_row,
 * points_per_colum) ).
 *
 * @param centers output array of detected centers.
 *
 * @param flags various operation flags that can be one of the following values:
 * CALIB_CB_SYMMETRIC_GRID uses symmetric pattern of circles.CALIB_CB_ASYMMETRIC_GRID uses asymmetric
 * pattern of circles.CALIB_CB_CLUSTERING uses a special algorithm for grid detection. It is more
 * robust to perspective distortions but much more sensitive to background clutter.
 *
 * @param blobDetector feature detector that finds blobs like dark circles on light background.
 *
 * @param parameters struct for finding circles in a grid pattern.
 */
export declare function findCirclesGrid(image: Mat, patternSize: Size, centers: Mat, flags: int, blobDetector: any, parameters: any): bool;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function findCirclesGrid(image: Mat, patternSize: Size, centers: Mat, flags?: int, blobDetector?: any): bool;
/**
 * This function estimates essential matrix based on the five-point algorithm solver in Nister03 .
 * SteweniusCFS is also a related. The epipolar geometry is described by the following equation:
 *
 * `\\[[p_2; 1]^T K^{-T} E K^{-1} [p_1; 1] = 0\\]`
 *
 * where `$E$` is an essential matrix, `$p_1$` and `$p_2$` are corresponding points in the first and
 * the second images, respectively. The result of this function may be passed further to
 * decomposeEssentialMat or recoverPose to recover the relative pose between cameras.
 *
 * @param points1 Array of N (N >= 5) 2D points from the first image. The point coordinates should be
 * floating-point (single or double precision).
 *
 * @param points2 Array of the second image points of the same size and format as points1 .
 *
 * @param cameraMatrix Camera matrix $K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ . Note
 * that this function assumes that points1 and points2 are feature points from cameras with the same
 * camera matrix.
 *
 * @param method Method for computing an essential matrix.
 * RANSAC for the RANSAC algorithm.LMEDS for the LMedS algorithm.
 *
 * @param prob Parameter used for the RANSAC or LMedS methods only. It specifies a desirable level of
 * confidence (probability) that the estimated matrix is correct.
 *
 * @param threshold Parameter used for RANSAC. It is the maximum distance from a point to an epipolar
 * line in pixels, beyond which the point is considered an outlier and is not used for computing the
 * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
 * point localization, image resolution, and the image noise.
 *
 * @param mask Output array of N elements, every element of which is set to 0 for outliers and to 1 for
 * the other points. The array is computed only in the RANSAC and LMedS methods.
 */
export declare function findEssentialMat(points1: Mat, points2: Mat, cameraMatrix: Mat, method?: int, prob?: double, threshold?: double, mask?: Mat): Mat;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 * This function differs from the one above that it computes camera matrix from focal length and
 * principal point:
 *
 * `\\[K = \\begin{bmatrix} f & 0 & x_{pp} \\\\ 0 & f & y_{pp} \\\\ 0 & 0 & 1 \\end{bmatrix}\\]`
 *
 * @param points1 Array of N (N >= 5) 2D points from the first image. The point coordinates should be
 * floating-point (single or double precision).
 *
 * @param points2 Array of the second image points of the same size and format as points1 .
 *
 * @param focal focal length of the camera. Note that this function assumes that points1 and points2
 * are feature points from cameras with same focal length and principal point.
 *
 * @param pp principal point of the camera.
 *
 * @param method Method for computing a fundamental matrix.
 * RANSAC for the RANSAC algorithm.LMEDS for the LMedS algorithm.
 *
 * @param prob Parameter used for the RANSAC or LMedS methods only. It specifies a desirable level of
 * confidence (probability) that the estimated matrix is correct.
 *
 * @param threshold Parameter used for RANSAC. It is the maximum distance from a point to an epipolar
 * line in pixels, beyond which the point is considered an outlier and is not used for computing the
 * final fundamental matrix. It can be set to something like 1-3, depending on the accuracy of the
 * point localization, image resolution, and the image noise.
 *
 * @param mask Output array of N elements, every element of which is set to 0 for outliers and to 1 for
 * the other points. The array is computed only in the RANSAC and LMedS methods.
 */
export declare function findEssentialMat(points1: Mat, points2: Mat, focal?: double, pp?: Size, method?: int, prob?: double, threshold?: double, mask?: Mat): Mat;
/**
 * `\\[[p_2; 1]^T F [p_1; 1] = 0\\]`
 *
 * where `$F$` is a fundamental matrix, `$p_1$` and `$p_2$` are corresponding points in the first and
 * the second images, respectively.
 *
 * The function calculates the fundamental matrix using one of four methods listed above and returns
 * the found fundamental matrix. Normally just one matrix is found. But in case of the 7-point
 * algorithm, the function may return up to 3 solutions ( `$9 \\times 3$` matrix that stores all 3
 * matrices sequentially).
 *
 * The calculated fundamental matrix may be passed further to computeCorrespondEpilines that finds the
 * epipolar lines corresponding to the specified points. It can also be passed to
 * stereoRectifyUncalibrated to compute the rectification transformation. :
 *
 * ```cpp
 * // Example. Estimation of fundamental matrix using the RANSAC algorithm
 * int point_count = 100;
 * vector<Point2f> points1(point_count);
 * vector<Point2f> points2(point_count);
 *
 * // initialize the points here ...
 * for( int i = 0; i < point_count; i++ )
 * {
 *     points1[i] = ...;
 *     points2[i] = ...;
 * }
 *
 * Mat fundamental_matrix =
 *  findFundamentalMat(points1, points2, FM_RANSAC, 3, 0.99);
 * ```
 *
 * @param points1 Array of N points from the first image. The point coordinates should be
 * floating-point (single or double precision).
 *
 * @param points2 Array of the second image points of the same size and format as points1 .
 *
 * @param method Method for computing a fundamental matrix.
 * CV_FM_7POINT for a 7-point algorithm. $N = 7$CV_FM_8POINT for an 8-point algorithm. $N \ge
 * 8$CV_FM_RANSAC for the RANSAC algorithm. $N \ge 8$CV_FM_LMEDS for the LMedS algorithm. $N \ge 8$
 *
 * @param ransacReprojThreshold Parameter used only for RANSAC. It is the maximum distance from a point
 * to an epipolar line in pixels, beyond which the point is considered an outlier and is not used for
 * computing the final fundamental matrix. It can be set to something like 1-3, depending on the
 * accuracy of the point localization, image resolution, and the image noise.
 *
 * @param confidence Parameter used for the RANSAC and LMedS methods only. It specifies a desirable
 * level of confidence (probability) that the estimated matrix is correct.
 *
 * @param mask The epipolar geometry is described by the following equation:
 */
export declare function findFundamentalMat(points1: Mat, points2: Mat, method?: int, ransacReprojThreshold?: double, confidence?: double, mask?: Mat): Mat;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function findFundamentalMat(points1: Mat, points2: Mat, mask: Mat, method?: int, ransacReprojThreshold?: double, confidence?: double): Mat;
/**
 * The function finds and returns the perspective transformation `$H$` between the source and the
 * destination planes:
 *
 * `\\[s_i \\vecthree{x'_i}{y'_i}{1} \\sim H \\vecthree{x_i}{y_i}{1}\\]`
 *
 * so that the back-projection error
 *
 * `\\[\\sum _i \\left ( x'_i- \\frac{h_{11} x_i + h_{12} y_i + h_{13}}{h_{31} x_i + h_{32} y_i +
 * h_{33}} \\right )^2+ \\left ( y'_i- \\frac{h_{21} x_i + h_{22} y_i + h_{23}}{h_{31} x_i + h_{32} y_i
 * + h_{33}} \\right )^2\\]`
 *
 * is minimized. If the parameter method is set to the default value 0, the function uses all the point
 * pairs to compute an initial homography estimate with a simple least-squares scheme.
 *
 * However, if not all of the point pairs ( `$srcPoints_i$`, `$dstPoints_i$` ) fit the rigid
 * perspective transformation (that is, there are some outliers), this initial estimate will be poor.
 * In this case, you can use one of the three robust methods. The methods RANSAC, LMeDS and RHO try
 * many different random subsets of the corresponding point pairs (of four pairs each, collinear pairs
 * are discarded), estimate the homography matrix using this subset and a simple least-squares
 * algorithm, and then compute the quality/goodness of the computed homography (which is the number of
 * inliers for RANSAC or the least median re-projection error for LMeDS). The best subset is then used
 * to produce the initial estimate of the homography matrix and the mask of inliers/outliers.
 *
 * Regardless of the method, robust or not, the computed homography matrix is refined further (using
 * inliers only in case of a robust method) with the Levenberg-Marquardt method to reduce the
 * re-projection error even more.
 *
 * The methods RANSAC and RHO can handle practically any ratio of outliers but need a threshold to
 * distinguish inliers from outliers. The method LMeDS does not need any threshold but it works
 * correctly only when there are more than 50% of inliers. Finally, if there are no outliers and the
 * noise is rather small, use the default method (method=0).
 *
 * The function is used to find initial intrinsic and extrinsic matrices. Homography matrix is
 * determined up to a scale. Thus, it is normalized so that `$h_{33}=1$`. Note that whenever an `$H$`
 * matrix cannot be estimated, an empty one will be returned.
 *
 * [getAffineTransform], [estimateAffine2D], [estimateAffinePartial2D], [getPerspectiveTransform],
 * [warpPerspective], [perspectiveTransform]
 *
 * @param srcPoints Coordinates of the points in the original plane, a matrix of the type CV_32FC2 or
 * vector<Point2f> .
 *
 * @param dstPoints Coordinates of the points in the target plane, a matrix of the type CV_32FC2 or a
 * vector<Point2f> .
 *
 * @param method Method used to compute a homography matrix. The following methods are possible:
 * 0 - a regular method using all the points, i.e., the least squares methodRANSAC - RANSAC-based
 * robust methodLMEDS - Least-Median robust methodRHO - PROSAC-based robust method
 *
 * @param ransacReprojThreshold Maximum allowed reprojection error to treat a point pair as an inlier
 * (used in the RANSAC and RHO methods only). That is, if \[\| \texttt{dstPoints} _i -
 * \texttt{convertPointsHomogeneous} ( \texttt{H} * \texttt{srcPoints} _i) \|_2 >
 * \texttt{ransacReprojThreshold}\] then the point $i$ is considered as an outlier. If srcPoints and
 * dstPoints are measured in pixels, it usually makes sense to set this parameter somewhere in the
 * range of 1 to 10.
 *
 * @param mask Optional output mask set by a robust method ( RANSAC or LMEDS ). Note that the input
 * mask values are ignored.
 *
 * @param maxIters The maximum number of RANSAC iterations.
 *
 * @param confidence Confidence level, between 0 and 1.
 */
export declare function findHomography(srcPoints: Mat, dstPoints: Mat, method?: int, ransacReprojThreshold?: double, mask?: Mat, maxIters?: any, confidence?: any): Mat;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function findHomography(srcPoints: Mat, dstPoints: Mat, mask: Mat, method?: int, ransacReprojThreshold?: double): Mat;
/**
 * The function returns the camera matrix that is either an exact copy of the input cameraMatrix (when
 * centerPrinicipalPoint=false ), or the modified one (when centerPrincipalPoint=true).
 *
 * In the latter case, the new camera matrix will be:
 *
 * `\\[\\begin{bmatrix} f_x && 0 && ( \\texttt{imgSize.width} -1)*0.5 \\\\ 0 && f_y && (
 * \\texttt{imgSize.height} -1)*0.5 \\\\ 0 && 0 && 1 \\end{bmatrix} ,\\]`
 *
 * where `$f_x$` and `$f_y$` are `$(0,0)$` and `$(1,1)$` elements of cameraMatrix, respectively.
 *
 * By default, the undistortion functions in OpenCV (see [initUndistortRectifyMap], [undistort]) do not
 * move the principal point. However, when you work with stereo, it is important to move the principal
 * points in both views to the same y-coordinate (which is required by most of stereo correspondence
 * algorithms), and may be to the same x-coordinate too. So, you can form the new camera matrix for
 * each view where the principal points are located at the center.
 *
 * @param cameraMatrix Input camera matrix.
 *
 * @param imgsize Camera view image size in pixels.
 *
 * @param centerPrincipalPoint Location of the principal point in the new camera matrix. The parameter
 * indicates whether this location should be at the image center or not.
 */
export declare function getDefaultNewCameraMatrix(cameraMatrix: Mat, imgsize?: Size, centerPrincipalPoint?: bool): Mat;
/**
 * new_camera_matrix Output new camera matrix.
 * The function computes and returns the optimal new camera matrix based on the free scaling parameter.
 * By varying this parameter, you may retrieve only sensible pixels alpha=0 , keep all the original
 * image pixels if there is valuable information in the corners alpha=1 , or get something in between.
 * When alpha>0 , the undistorted result is likely to have some black pixels corresponding to "virtual"
 * pixels outside of the captured distorted image. The original camera matrix, distortion coefficients,
 * the computed new camera matrix, and newImageSize should be passed to initUndistortRectifyMap to
 * produce the maps for remap .
 *
 * @param cameraMatrix Input camera matrix.
 *
 * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6
 * [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is
 * NULL/empty, the zero distortion coefficients are assumed.
 *
 * @param imageSize Original image size.
 *
 * @param alpha Free scaling parameter between 0 (when all the pixels in the undistorted image are
 * valid) and 1 (when all the source image pixels are retained in the undistorted image). See
 * stereoRectify for details.
 *
 * @param newImgSize Image size after rectification. By default, it is set to imageSize .
 *
 * @param validPixROI Optional output rectangle that outlines all-good-pixels region in the undistorted
 * image. See roi1, roi2 description in stereoRectify .
 *
 * @param centerPrincipalPoint Optional flag that indicates whether in the new camera matrix the
 * principal point should be at the image center or not. By default, the principal point is chosen to
 * best fit a subset of the source image (determined by alpha) to the corrected image.
 */
export declare function getOptimalNewCameraMatrix(cameraMatrix: Mat, distCoeffs: Mat, imageSize: Size, alpha: double, newImgSize?: Size, validPixROI?: any, centerPrincipalPoint?: bool): Mat;
export declare function getValidDisparityROI(roi1: Rect, roi2: Rect, minDisparity: int, numberOfDisparities: int, SADWindowSize: int): Rect;
/**
 * The function estimates and returns an initial camera matrix for the camera calibration process.
 * Currently, the function only supports planar calibration patterns, which are patterns where each
 * object point has z-coordinate =0.
 *
 * @param objectPoints Vector of vectors of the calibration pattern points in the calibration pattern
 * coordinate space. In the old interface all the per-view vectors are concatenated. See
 * calibrateCamera for details.
 *
 * @param imagePoints Vector of vectors of the projections of the calibration pattern points. In the
 * old interface all the per-view vectors are concatenated.
 *
 * @param imageSize Image size in pixels used to initialize the principal point.
 *
 * @param aspectRatio If it is zero or negative, both $f_x$ and $f_y$ are estimated independently.
 * Otherwise, $f_x = f_y * \texttt{aspectRatio}$ .
 */
export declare function initCameraMatrix2D(objectPoints: MatVector, imagePoints: MatVector, imageSize: Size, aspectRatio?: double): Mat;
/**
 * The function computes the joint undistortion and rectification transformation and represents the
 * result in the form of maps for remap. The undistorted image looks like original, as if it is
 * captured with a camera using the camera matrix =newCameraMatrix and zero distortion. In case of a
 * monocular camera, newCameraMatrix is usually equal to cameraMatrix, or it can be computed by
 * [getOptimalNewCameraMatrix] for a better control over scaling. In case of a stereo camera,
 * newCameraMatrix is normally set to P1 or P2 computed by [stereoRectify] .
 *
 * Also, this new camera is oriented differently in the coordinate space, according to R. That, for
 * example, helps to align two heads of a stereo camera so that the epipolar lines on both images
 * become horizontal and have the same y- coordinate (in case of a horizontally aligned stereo camera).
 *
 * The function actually builds the maps for the inverse mapping algorithm that is used by remap. That
 * is, for each pixel `$(u, v)$` in the destination (corrected and rectified) image, the function
 * computes the corresponding coordinates in the source image (that is, in the original image from
 * camera). The following process is applied: `\\[ \\begin{array}{l} x \\leftarrow (u - {c'}_x)/{f'}_x
 * \\\\ y \\leftarrow (v - {c'}_y)/{f'}_y \\\\ {[X\\,Y\\,W]} ^T \\leftarrow R^{-1}*[x \\, y \\, 1]^T
 * \\\\ x' \\leftarrow X/W \\\\ y' \\leftarrow Y/W \\\\ r^2 \\leftarrow x'^2 + y'^2 \\\\ x''
 * \\leftarrow x' \\frac{1 + k_1 r^2 + k_2 r^4 + k_3 r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + 2p_1 x' y'
 * + p_2(r^2 + 2 x'^2) + s_1 r^2 + s_2 r^4\\\\ y'' \\leftarrow y' \\frac{1 + k_1 r^2 + k_2 r^4 + k_3
 * r^6}{1 + k_4 r^2 + k_5 r^4 + k_6 r^6} + p_1 (r^2 + 2 y'^2) + 2 p_2 x' y' + s_3 r^2 + s_4 r^4 \\\\
 * s\\vecthree{x'''}{y'''}{1} = \\vecthreethree{R_{33}(\\tau_x, \\tau_y)}{0}{-R_{13}((\\tau_x,
 * \\tau_y)} {0}{R_{33}(\\tau_x, \\tau_y)}{-R_{23}(\\tau_x, \\tau_y)} {0}{0}{1} R(\\tau_x, \\tau_y)
 * \\vecthree{x''}{y''}{1}\\\\ map_x(u,v) \\leftarrow x''' f_x + c_x \\\\ map_y(u,v) \\leftarrow y'''
 * f_y + c_y \\end{array} \\]` where `$(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6[, s_1, s_2, s_3, s_4[,
 * \\tau_x, \\tau_y]]]])$` are the distortion coefficients.
 *
 * In case of a stereo camera, this function is called twice: once for each camera head, after
 * stereoRectify, which in its turn is called after [stereoCalibrate]. But if the stereo camera was not
 * calibrated, it is still possible to compute the rectification transformations directly from the
 * fundamental matrix using [stereoRectifyUncalibrated]. For each camera, the function computes
 * homography H as the rectification transformation in a pixel domain, not a rotation matrix R in 3D
 * space. R can be computed from H as `\\[\\texttt{R} = \\texttt{cameraMatrix} ^{-1} \\cdot \\texttt{H}
 * \\cdot \\texttt{cameraMatrix}\\]` where cameraMatrix can be chosen arbitrarily.
 *
 * @param cameraMatrix Input camera matrix $A=\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ .
 *
 * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5,
 * k_6[, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is
 * NULL/empty, the zero distortion coefficients are assumed.
 *
 * @param R Optional rectification transformation in the object space (3x3 matrix). R1 or R2 , computed
 * by stereoRectify can be passed here. If the matrix is empty, the identity transformation is assumed.
 * In cvInitUndistortMap R assumed to be an identity matrix.
 *
 * @param newCameraMatrix New camera matrix $A'=\vecthreethree{f_x'}{0}{c_x'}{0}{f_y'}{c_y'}{0}{0}{1}$.
 *
 * @param size Undistorted image size.
 *
 * @param m1type Type of the first output map that can be CV_32FC1, CV_32FC2 or CV_16SC2, see
 * convertMaps
 *
 * @param map1 The first output map.
 *
 * @param map2 The second output map.
 */
export declare function initUndistortRectifyMap(cameraMatrix: Mat, distCoeffs: Mat, R: Mat, newCameraMatrix: Mat, size: Size, m1type: int, map1: Mat, map2: Mat): void;
export declare function initWideAngleProjMap(cameraMatrix: Mat, distCoeffs: Mat, imageSize: Size, destImageWidth: int, m1type: int, map1: Mat, map2: Mat, projType?: any, alpha?: double): float;
export declare function initWideAngleProjMap(cameraMatrix: Mat, distCoeffs: Mat, imageSize: Size, destImageWidth: int, m1type: int, map1: Mat, map2: Mat, projType: int, alpha?: double): float;
/**
 * The function computes partial derivatives of the elements of the matrix product `$A*B$` with regard
 * to the elements of each of the two input matrices. The function is used to compute the Jacobian
 * matrices in stereoCalibrate but can also be used in any other similar optimization function.
 *
 * @param A First multiplied matrix.
 *
 * @param B Second multiplied matrix.
 *
 * @param dABdA First output derivative matrix d(A*B)/dA of size $\texttt{A.rows*B.cols} \times
 * {A.rows*A.cols}$ .
 *
 * @param dABdB Second output derivative matrix d(A*B)/dB of size $\texttt{A.rows*B.cols} \times
 * {B.rows*B.cols}$ .
 */
export declare function matMulDeriv(A: Mat, B: Mat, dABdA: Mat, dABdB: Mat): void;
/**
 * The function computes projections of 3D points to the image plane given intrinsic and extrinsic
 * camera parameters. Optionally, the function computes Jacobians - matrices of partial derivatives of
 * image points coordinates (as functions of all the input parameters) with respect to the particular
 * parameters, intrinsic and/or extrinsic. The Jacobians are used during the global optimization in
 * calibrateCamera, solvePnP, and stereoCalibrate . The function itself can also be used to compute a
 * re-projection error given the current intrinsic and extrinsic parameters.
 *
 * By setting rvec=tvec=(0,0,0) or by setting cameraMatrix to a 3x3 identity matrix, or by passing zero
 * distortion coefficients, you can get various useful partial cases of the function. This means that
 * you can compute the distorted coordinates for a sparse set of points or apply a perspective
 * transformation (and also compute the derivatives) in the ideal zero-distortion setup.
 *
 * @param objectPoints Array of object points, 3xN/Nx3 1-channel or 1xN/Nx1 3-channel (or
 * vector<Point3f> ), where N is the number of points in the view.
 *
 * @param rvec Rotation vector. See Rodrigues for details.
 *
 * @param tvec Translation vector.
 *
 * @param cameraMatrix Camera matrix $A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{_1}$ .
 *
 * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6
 * [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is empty,
 * the zero distortion coefficients are assumed.
 *
 * @param imagePoints Output array of image points, 1xN/Nx1 2-channel, or vector<Point2f> .
 *
 * @param jacobian Optional output 2Nx(10+<numDistCoeffs>) jacobian matrix of derivatives of image
 * points with respect to components of the rotation vector, translation vector, focal lengths,
 * coordinates of the principal point and the distortion coefficients. In the old interface different
 * components of the jacobian are returned via different output parameters.
 *
 * @param aspectRatio Optional "fixed aspect ratio" parameter. If the parameter is not 0, the function
 * assumes that the aspect ratio (fx/fy) is fixed and correspondingly adjusts the jacobian matrix.
 */
export declare function projectPoints(objectPoints: Mat, rvec: Mat, tvec: Mat, cameraMatrix: Mat, distCoeffs: Mat, imagePoints: Mat, jacobian?: Mat, aspectRatio?: double): void;
/**
 * This function can be used to process output E and mask from findEssentialMat. In this scenario,
 * points1 and points2 are the same input for findEssentialMat. :
 *
 * ```cpp
 * // Example. Estimation of fundamental matrix using the RANSAC algorithm
 * int point_count = 100;
 * vector<Point2f> points1(point_count);
 * vector<Point2f> points2(point_count);
 *
 * // initialize the points here ...
 * for( int i = 0; i < point_count; i++ )
 * {
 *     points1[i] = ...;
 *     points2[i] = ...;
 * }
 *
 * // cametra matrix with both focal lengths = 1, and principal point = (0, 0)
 * Mat cameraMatrix = Mat::eye(3, 3, CV_64F);
 *
 * Mat E, R, t, mask;
 *
 * E = findEssentialMat(points1, points2, cameraMatrix, RANSAC, 0.999, 1.0, mask);
 * recoverPose(E, points1, points2, cameraMatrix, R, t, mask);
 * ```
 *
 * @param E The input essential matrix.
 *
 * @param points1 Array of N 2D points from the first image. The point coordinates should be
 * floating-point (single or double precision).
 *
 * @param points2 Array of the second image points of the same size and format as points1 .
 *
 * @param cameraMatrix Camera matrix $K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ . Note
 * that this function assumes that points1 and points2 are feature points from cameras with the same
 * camera matrix.
 *
 * @param R Recovered relative rotation.
 *
 * @param t Recovered relative translation.
 *
 * @param mask Input/output mask for inliers in points1 and points2. : If it is not empty, then it
 * marks inliers in points1 and points2 for then given essential matrix E. Only these inliers will be
 * used to recover pose. In the output mask only inliers which pass the cheirality check. This function
 * decomposes an essential matrix using decomposeEssentialMat and then verifies possible pose
 * hypotheses by doing cheirality check. The cheirality check basically means that the triangulated 3D
 * points should have positive depth. Some details can be found in Nister03 .
 */
export declare function recoverPose(E: Mat, points1: Mat, points2: Mat, cameraMatrix: Mat, R: Mat, t: Mat, mask?: Mat): int;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 * This function differs from the one above that it computes camera matrix from focal length and
 * principal point:
 *
 * `\\[K = \\begin{bmatrix} f & 0 & x_{pp} \\\\ 0 & f & y_{pp} \\\\ 0 & 0 & 1 \\end{bmatrix}\\]`
 *
 * @param E The input essential matrix.
 *
 * @param points1 Array of N 2D points from the first image. The point coordinates should be
 * floating-point (single or double precision).
 *
 * @param points2 Array of the second image points of the same size and format as points1 .
 *
 * @param R Recovered relative rotation.
 *
 * @param t Recovered relative translation.
 *
 * @param focal Focal length of the camera. Note that this function assumes that points1 and points2
 * are feature points from cameras with same focal length and principal point.
 *
 * @param pp principal point of the camera.
 *
 * @param mask Input/output mask for inliers in points1 and points2. : If it is not empty, then it
 * marks inliers in points1 and points2 for then given essential matrix E. Only these inliers will be
 * used to recover pose. In the output mask only inliers which pass the cheirality check.
 */
export declare function recoverPose(E: Mat, points1: Mat, points2: Mat, R: Mat, t: Mat, focal?: double, pp?: Size, mask?: Mat): int;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param E The input essential matrix.
 *
 * @param points1 Array of N 2D points from the first image. The point coordinates should be
 * floating-point (single or double precision).
 *
 * @param points2 Array of the second image points of the same size and format as points1.
 *
 * @param cameraMatrix Camera matrix $K = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ . Note
 * that this function assumes that points1 and points2 are feature points from cameras with the same
 * camera matrix.
 *
 * @param R Recovered relative rotation.
 *
 * @param t Recovered relative translation.
 *
 * @param distanceThresh threshold distance which is used to filter out far away points (i.e. infinite
 * points).
 *
 * @param mask Input/output mask for inliers in points1 and points2. : If it is not empty, then it
 * marks inliers in points1 and points2 for then given essential matrix E. Only these inliers will be
 * used to recover pose. In the output mask only inliers which pass the cheirality check.
 *
 * @param triangulatedPoints 3d points which were reconstructed by triangulation.
 */
export declare function recoverPose(E: Mat, points1: Mat, points2: Mat, cameraMatrix: Mat, R: Mat, t: Mat, distanceThresh: double, mask?: Mat, triangulatedPoints?: Mat): int;
export declare function rectify3Collinear(cameraMatrix1: Mat, distCoeffs1: Mat, cameraMatrix2: Mat, distCoeffs2: Mat, cameraMatrix3: Mat, distCoeffs3: Mat, imgpt1: MatVector, imgpt3: MatVector, imageSize: Size, R12: Mat, T12: Mat, R13: Mat, T13: Mat, R1: Mat, R2: Mat, R3: Mat, P1: Mat, P2: Mat, P3: Mat, Q: Mat, alpha: double, newImgSize: Size, roi1: any, roi2: any, flags: int): float;
/**
 * The function transforms a single-channel disparity map to a 3-channel image representing a 3D
 * surface. That is, for each pixel (x,y) and the corresponding disparity d=disparity(x,y) , it
 * computes:
 *
 * `\\[\\begin{array}{l} [X \\; Y \\; Z \\; W]^T = \\texttt{Q} *[x \\; y \\; \\texttt{disparity} (x,y)
 * \\; 1]^T \\\\ \\texttt{\\_3dImage} (x,y) = (X/W, \\; Y/W, \\; Z/W) \\end{array}\\]`
 *
 * The matrix Q can be an arbitrary `$4 \\times 4$` matrix (for example, the one computed by
 * stereoRectify). To reproject a sparse set of points {(x,y,d),...} to 3D space, use
 * perspectiveTransform .
 *
 * @param disparity Input single-channel 8-bit unsigned, 16-bit signed, 32-bit signed or 32-bit
 * floating-point disparity image. If 16-bit signed format is used, the values are assumed to have no
 * fractional bits.
 *
 * @param _3dImage Output 3-channel floating-point image of the same size as disparity . Each element
 * of _3dImage(x,y) contains 3D coordinates of the point (x,y) computed from the disparity map.
 *
 * @param Q $4 \times 4$ perspective transformation matrix that can be obtained with stereoRectify.
 *
 * @param handleMissingValues Indicates, whether the function should handle missing values (i.e. points
 * where the disparity was not computed). If handleMissingValues=true, then pixels with the minimal
 * disparity that corresponds to the outliers (see StereoMatcher::compute ) are transformed to 3D
 * points with a very large Z value (currently set to 10000).
 *
 * @param ddepth The optional output array depth. If it is -1, the output image will have CV_32F depth.
 * ddepth can also be set to CV_16S, CV_32S or CV_32F.
 */
export declare function reprojectImageTo3D(disparity: Mat, _3dImage: Mat, Q: Mat, handleMissingValues?: bool, ddepth?: int): void;
/**
 * `\\[\\begin{array}{l} \\theta \\leftarrow norm(r) \\\\ r \\leftarrow r/ \\theta \\\\ R =
 * \\cos{\\theta} I + (1- \\cos{\\theta} ) r r^T + \\sin{\\theta}
 * \\vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} \\end{array}\\]`
 *
 * Inverse transformation can be also done easily, since
 *
 * `\\[\\sin ( \\theta ) \\vecthreethree{0}{-r_z}{r_y}{r_z}{0}{-r_x}{-r_y}{r_x}{0} = \\frac{R -
 * R^T}{2}\\]`
 *
 * A rotation vector is a convenient and most compact representation of a rotation matrix (since any
 * rotation matrix has just 3 degrees of freedom). The representation is used in the global 3D geometry
 * optimization procedures like calibrateCamera, stereoCalibrate, or solvePnP .
 *
 * @param src Input rotation vector (3x1 or 1x3) or rotation matrix (3x3).
 *
 * @param dst Output rotation matrix (3x3) or rotation vector (3x1 or 1x3), respectively.
 *
 * @param jacobian Optional output Jacobian matrix, 3x9 or 9x3, which is a matrix of partial
 * derivatives of the output array components with respect to the input array components.
 */
export declare function Rodrigues(src: Mat, dst: Mat, jacobian?: Mat): void;
/**
 * The function computes a RQ decomposition using the given rotations. This function is used in
 * decomposeProjectionMatrix to decompose the left 3x3 submatrix of a projection matrix into a camera
 * and a rotation matrix.
 *
 * It optionally returns three rotation matrices, one for each axis, and the three Euler angles in
 * degrees (as the return value) that could be used in OpenGL. Note, there is always more than one
 * sequence of rotations about the three principal axes that results in the same orientation of an
 * object, e.g. see Slabaugh . Returned tree rotation matrices and corresponding three Euler angles are
 * only one of the possible solutions.
 *
 * @param src 3x3 input matrix.
 *
 * @param mtxR Output 3x3 upper-triangular matrix.
 *
 * @param mtxQ Output 3x3 orthogonal matrix.
 *
 * @param Qx Optional output 3x3 rotation matrix around x-axis.
 *
 * @param Qy Optional output 3x3 rotation matrix around y-axis.
 *
 * @param Qz Optional output 3x3 rotation matrix around z-axis.
 */
export declare function RQDecomp3x3(src: Mat, mtxR: Mat, mtxQ: Mat, Qx?: Mat, Qy?: Mat, Qz?: Mat): Vec3d;
/**
 * The function [cv::sampsonDistance] calculates and returns the first order approximation of the
 * geometric error as: `\\[ sd( \\texttt{pt1} , \\texttt{pt2} )= \\frac{(\\texttt{pt2}^t \\cdot
 * \\texttt{F} \\cdot \\texttt{pt1})^2} {((\\texttt{F} \\cdot \\texttt{pt1})(0))^2 + ((\\texttt{F}
 * \\cdot \\texttt{pt1})(1))^2 + ((\\texttt{F}^t \\cdot \\texttt{pt2})(0))^2 + ((\\texttt{F}^t \\cdot
 * \\texttt{pt2})(1))^2} \\]` The fundamental matrix may be calculated using the
 * [cv::findFundamentalMat] function. See HartleyZ00 11.4.3 for details.
 *
 * The computed Sampson distance.
 *
 * @param pt1 first homogeneous 2d point
 *
 * @param pt2 second homogeneous 2d point
 *
 * @param F fundamental matrix
 */
export declare function sampsonDistance(pt1: Mat, pt2: Mat, F: Mat): double;
/**
 * The function estimates the object pose given 3 object points, their corresponding image projections,
 * as well as the camera matrix and the distortion coefficients.
 *
 * The solutions are sorted by reprojection errors (lowest to highest).
 *
 * @param objectPoints Array of object points in the object coordinate space, 3x3 1-channel or 1x3/3x1
 * 3-channel. vector<Point3f> can be also passed here.
 *
 * @param imagePoints Array of corresponding image points, 3x2 1-channel or 1x3/3x1 2-channel.
 * vector<Point2f> can be also passed here.
 *
 * @param cameraMatrix Input camera matrix $A = \vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .
 *
 * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6
 * [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is
 * NULL/empty, the zero distortion coefficients are assumed.
 *
 * @param rvecs Output rotation vectors (see Rodrigues ) that, together with tvecs, brings points from
 * the model coordinate system to the camera coordinate system. A P3P problem has up to 4 solutions.
 *
 * @param tvecs Output translation vectors.
 *
 * @param flags Method for solving a P3P problem:
 * SOLVEPNP_P3P Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang "Complete
 * Solution Classification for the Perspective-Three-Point Problem" (gao2003complete).SOLVEPNP_AP3P
 * Method is based on the paper of T. Ke and S. Roumeliotis. "An Efficient Algebraic Solution to the
 * Perspective-Three-Point Problem" (Ke17).
 */
export declare function solveP3P(objectPoints: Mat, imagePoints: Mat, cameraMatrix: Mat, distCoeffs: Mat, rvecs: MatVector, tvecs: MatVector, flags: int): int;
/**
 * P3P methods ([SOLVEPNP_P3P], [SOLVEPNP_AP3P]): need 4 input points to return a unique solution.
 * [SOLVEPNP_IPPE] Input points must be >= 4 and object points must be coplanar.
 * [SOLVEPNP_IPPE_SQUARE] Special case suitable for marker pose estimation. Number of input points must
 * be 4. Object points must be defined in the following order:
 *
 * point 0: [-squareLength / 2, squareLength / 2, 0]
 * point 1: [ squareLength / 2, squareLength / 2, 0]
 * point 2: [ squareLength / 2, -squareLength / 2, 0]
 * point 3: [-squareLength / 2, -squareLength / 2, 0]
 *
 * for all the other flags, number of input points must be >= 4 and object points can be in any
 * configuration.
 *
 * The function estimates the object pose given a set of object points, their corresponding image
 * projections, as well as the camera matrix and the distortion coefficients, see the figure below
 * (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward and
 * the Z-axis forward).
 *
 * Points expressed in the world frame `$ \\bf{X}_w $` are projected into the image plane `$ \\left[ u,
 * v \\right] $` using the perspective projection model `$ \\Pi $` and the camera intrinsic parameters
 * matrix `$ \\bf{A} $`:
 *
 * `\\[ \\begin{align*} \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} &= \\bf{A} \\hspace{0.1em} \\Pi
 * \\hspace{0.2em} ^{c}\\bf{M}_w \\begin{bmatrix} X_{w} \\\\ Y_{w} \\\\ Z_{w} \\\\ 1 \\end{bmatrix}
 * \\\\ \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y
 * & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1
 * & 0 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} &
 * t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} X_{w}
 * \\\\ Y_{w} \\\\ Z_{w} \\\\ 1 \\end{bmatrix} \\end{align*} \\]`
 *
 * The estimated pose is thus the rotation (`rvec`) and the translation (`tvec`) vectors that allow
 * transforming a 3D point expressed in the world frame into the camera frame:
 *
 * `\\[ \\begin{align*} \\begin{bmatrix} X_c \\\\ Y_c \\\\ Z_c \\\\ 1 \\end{bmatrix} &= \\hspace{0.2em}
 * ^{c}\\bf{M}_w \\begin{bmatrix} X_{w} \\\\ Y_{w} \\\\ Z_{w} \\\\ 1 \\end{bmatrix} \\\\
 * \\begin{bmatrix} X_c \\\\ Y_c \\\\ Z_c \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} r_{11} & r_{12} &
 * r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0
 * & 1 \\end{bmatrix} \\begin{bmatrix} X_{w} \\\\ Y_{w} \\\\ Z_{w} \\\\ 1 \\end{bmatrix} \\end{align*}
 * \\]`
 *
 * An example of how to use solvePnP for planar augmented reality can be found at
 * opencv_source_code/samples/python/plane_ar.py
 * If you are using Python:
 *
 * Numpy array slices won't work as input because solvePnP requires contiguous arrays (enforced by the
 * assertion using [cv::Mat::checkVector()] around line 55 of modules/calib3d/src/solvepnp.cpp version
 * 2.4.9)
 * The P3P algorithm requires image points to be in an array of shape (N,1,2) due to its calling of
 * [cv::undistortPoints] (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9) which
 * requires 2-channel information.
 * Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of it as,
 * e.g., imagePoints, one must effectively copy it into a new array: imagePoints =
 * np.ascontiguousarray(D[:,:2]).reshape((N,1,2))
 *
 * The methods **SOLVEPNP_DLS** and **SOLVEPNP_UPNP** cannot be used as the current implementations are
 * unstable and sometimes give completely wrong results. If you pass one of these two flags,
 * **SOLVEPNP_EPNP** method will be used instead.
 * The minimum number of points is 4 in the general case. In the case of **SOLVEPNP_P3P** and
 * **SOLVEPNP_AP3P** methods, it is required to use exactly 4 points (the first 3 points are used to
 * estimate all the solutions of the P3P problem, the last one is used to retain the best solution that
 * minimizes the reprojection error).
 * With **SOLVEPNP_ITERATIVE** method and `useExtrinsicGuess=true`, the minimum number of points is 3
 * (3 points are sufficient to compute a pose but there are up to 4 solutions). The initial solution
 * should be close to the global solution to converge.
 * With **SOLVEPNP_IPPE** input points must be >= 4 and object points must be coplanar.
 * With **SOLVEPNP_IPPE_SQUARE** this is a special case suitable for marker pose estimation. Number of
 * input points must be 4. Object points must be defined in the following order:
 *
 * point 0: [-squareLength / 2, squareLength / 2, 0]
 * point 1: [ squareLength / 2, squareLength / 2, 0]
 * point 2: [ squareLength / 2, -squareLength / 2, 0]
 * point 3: [-squareLength / 2, -squareLength / 2, 0]
 *
 * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1
 * 3-channel, where N is the number of points. vector<Point3f> can be also passed here.
 *
 * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N
 * is the number of points. vector<Point2f> can be also passed here.
 *
 * @param cameraMatrix Input camera matrix $A = \vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .
 *
 * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6
 * [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is
 * NULL/empty, the zero distortion coefficients are assumed.
 *
 * @param rvec Output rotation vector (see Rodrigues ) that, together with tvec, brings points from the
 * model coordinate system to the camera coordinate system.
 *
 * @param tvec Output translation vector.
 *
 * @param useExtrinsicGuess Parameter used for SOLVEPNP_ITERATIVE. If true (1), the function uses the
 * provided rvec and tvec values as initial approximations of the rotation and translation vectors,
 * respectively, and further optimizes them.
 *
 * @param flags Method for solving a PnP problem:
 * SOLVEPNP_ITERATIVE Iterative method is based on a Levenberg-Marquardt optimization. In this case the
 * function finds such a pose that minimizes reprojection error, that is the sum of squared distances
 * between the observed projections imagePoints and the projected (using projectPoints ) objectPoints
 * .SOLVEPNP_P3P Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang "Complete
 * Solution Classification for the Perspective-Three-Point Problem" (gao2003complete). In this case the
 * function requires exactly four object and image points.SOLVEPNP_AP3P Method is based on the paper of
 * T. Ke, S. Roumeliotis "An Efficient Algebraic Solution to the Perspective-Three-Point Problem"
 * (Ke17). In this case the function requires exactly four object and image points.SOLVEPNP_EPNP Method
 * has been introduced by F. Moreno-Noguer, V. Lepetit and P. Fua in the paper "EPnP: Efficient
 * Perspective-n-Point Camera Pose Estimation" (lepetit2009epnp).SOLVEPNP_DLS Method is based on the
 * paper of J. Hesch and S. Roumeliotis. "A Direct Least-Squares (DLS) Method for PnP"
 * (hesch2011direct).SOLVEPNP_UPNP Method is based on the paper of A. Penate-Sanchez, J. Andrade-Cetto,
 * F. Moreno-Noguer. "Exhaustive Linearization for Robust Camera Pose and Focal Length
 * Estimation" (penate2013exhaustive). In this case the function also estimates the parameters $f_x$
 * and $f_y$ assuming that both have the same value. Then the cameraMatrix is updated with the
 * estimated focal length.SOLVEPNP_IPPE Method is based on the paper of T. Collins and A. Bartoli.
 * "Infinitesimal Plane-Based Pose Estimation" (Collins14). This method requires coplanar object
 * points.SOLVEPNP_IPPE_SQUARE Method is based on the paper of Toby Collins and Adrien Bartoli.
 * "Infinitesimal Plane-Based Pose Estimation" (Collins14). This method is suitable for marker pose
 * estimation. It requires 4 coplanar object points defined in the following order:
 * point 0: [-squareLength / 2, squareLength / 2, 0]point 1: [ squareLength / 2, squareLength / 2,
 * 0]point 2: [ squareLength / 2, -squareLength / 2, 0]point 3: [-squareLength / 2, -squareLength / 2,
 * 0]
 */
export declare function solvePnP(objectPoints: Mat, imagePoints: Mat, cameraMatrix: Mat, distCoeffs: Mat, rvec: Mat, tvec: Mat, useExtrinsicGuess?: bool, flags?: int): bool;
/**
 * P3P methods ([SOLVEPNP_P3P], [SOLVEPNP_AP3P]): 3 or 4 input points. Number of returned solutions can
 * be between 0 and 4 with 3 input points.
 * [SOLVEPNP_IPPE] Input points must be >= 4 and object points must be coplanar. Returns 2 solutions.
 * [SOLVEPNP_IPPE_SQUARE] Special case suitable for marker pose estimation. Number of input points must
 * be 4 and 2 solutions are returned. Object points must be defined in the following order:
 *
 * point 0: [-squareLength / 2, squareLength / 2, 0]
 * point 1: [ squareLength / 2, squareLength / 2, 0]
 * point 2: [ squareLength / 2, -squareLength / 2, 0]
 * point 3: [-squareLength / 2, -squareLength / 2, 0]
 *
 * for all the other flags, number of input points must be >= 4 and object points can be in any
 * configuration. Only 1 solution is returned.
 *
 * The function estimates the object pose given a set of object points, their corresponding image
 * projections, as well as the camera matrix and the distortion coefficients, see the figure below
 * (more precisely, the X-axis of the camera frame is pointing to the right, the Y-axis downward and
 * the Z-axis forward).
 *
 * Points expressed in the world frame `$ \\bf{X}_w $` are projected into the image plane `$ \\left[ u,
 * v \\right] $` using the perspective projection model `$ \\Pi $` and the camera intrinsic parameters
 * matrix `$ \\bf{A} $`:
 *
 * `\\[ \\begin{align*} \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} &= \\bf{A} \\hspace{0.1em} \\Pi
 * \\hspace{0.2em} ^{c}\\bf{M}_w \\begin{bmatrix} X_{w} \\\\ Y_{w} \\\\ Z_{w} \\\\ 1 \\end{bmatrix}
 * \\\\ \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y
 * & c_y \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1
 * & 0 \\end{bmatrix} \\begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} &
 * t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} X_{w}
 * \\\\ Y_{w} \\\\ Z_{w} \\\\ 1 \\end{bmatrix} \\end{align*} \\]`
 *
 * The estimated pose is thus the rotation (`rvec`) and the translation (`tvec`) vectors that allow
 * transforming a 3D point expressed in the world frame into the camera frame:
 *
 * `\\[ \\begin{align*} \\begin{bmatrix} X_c \\\\ Y_c \\\\ Z_c \\\\ 1 \\end{bmatrix} &= \\hspace{0.2em}
 * ^{c}\\bf{M}_w \\begin{bmatrix} X_{w} \\\\ Y_{w} \\\\ Z_{w} \\\\ 1 \\end{bmatrix} \\\\
 * \\begin{bmatrix} X_c \\\\ Y_c \\\\ Z_c \\\\ 1 \\end{bmatrix} &= \\begin{bmatrix} r_{11} & r_{12} &
 * r_{13} & t_x \\\\ r_{21} & r_{22} & r_{23} & t_y \\\\ r_{31} & r_{32} & r_{33} & t_z \\\\ 0 & 0 & 0
 * & 1 \\end{bmatrix} \\begin{bmatrix} X_{w} \\\\ Y_{w} \\\\ Z_{w} \\\\ 1 \\end{bmatrix} \\end{align*}
 * \\]`
 *
 * An example of how to use solvePnP for planar augmented reality can be found at
 * opencv_source_code/samples/python/plane_ar.py
 * If you are using Python:
 *
 * Numpy array slices won't work as input because solvePnP requires contiguous arrays (enforced by the
 * assertion using [cv::Mat::checkVector()] around line 55 of modules/calib3d/src/solvepnp.cpp version
 * 2.4.9)
 * The P3P algorithm requires image points to be in an array of shape (N,1,2) due to its calling of
 * [cv::undistortPoints] (around line 75 of modules/calib3d/src/solvepnp.cpp version 2.4.9) which
 * requires 2-channel information.
 * Thus, given some data D = np.array(...) where D.shape = (N,M), in order to use a subset of it as,
 * e.g., imagePoints, one must effectively copy it into a new array: imagePoints =
 * np.ascontiguousarray(D[:,:2]).reshape((N,1,2))
 *
 * The methods **SOLVEPNP_DLS** and **SOLVEPNP_UPNP** cannot be used as the current implementations are
 * unstable and sometimes give completely wrong results. If you pass one of these two flags,
 * **SOLVEPNP_EPNP** method will be used instead.
 * The minimum number of points is 4 in the general case. In the case of **SOLVEPNP_P3P** and
 * **SOLVEPNP_AP3P** methods, it is required to use exactly 4 points (the first 3 points are used to
 * estimate all the solutions of the P3P problem, the last one is used to retain the best solution that
 * minimizes the reprojection error).
 * With **SOLVEPNP_ITERATIVE** method and `useExtrinsicGuess=true`, the minimum number of points is 3
 * (3 points are sufficient to compute a pose but there are up to 4 solutions). The initial solution
 * should be close to the global solution to converge.
 * With **SOLVEPNP_IPPE** input points must be >= 4 and object points must be coplanar.
 * With **SOLVEPNP_IPPE_SQUARE** this is a special case suitable for marker pose estimation. Number of
 * input points must be 4. Object points must be defined in the following order:
 *
 * point 0: [-squareLength / 2, squareLength / 2, 0]
 * point 1: [ squareLength / 2, squareLength / 2, 0]
 * point 2: [ squareLength / 2, -squareLength / 2, 0]
 * point 3: [-squareLength / 2, -squareLength / 2, 0]
 *
 * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1
 * 3-channel, where N is the number of points. vector<Point3f> can be also passed here.
 *
 * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N
 * is the number of points. vector<Point2f> can be also passed here.
 *
 * @param cameraMatrix Input camera matrix $A = \vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .
 *
 * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6
 * [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is
 * NULL/empty, the zero distortion coefficients are assumed.
 *
 * @param rvecs Vector of output rotation vectors (see Rodrigues ) that, together with tvecs, brings
 * points from the model coordinate system to the camera coordinate system.
 *
 * @param tvecs Vector of output translation vectors.
 *
 * @param useExtrinsicGuess Parameter used for SOLVEPNP_ITERATIVE. If true (1), the function uses the
 * provided rvec and tvec values as initial approximations of the rotation and translation vectors,
 * respectively, and further optimizes them.
 *
 * @param flags Method for solving a PnP problem:
 * SOLVEPNP_ITERATIVE Iterative method is based on a Levenberg-Marquardt optimization. In this case the
 * function finds such a pose that minimizes reprojection error, that is the sum of squared distances
 * between the observed projections imagePoints and the projected (using projectPoints ) objectPoints
 * .SOLVEPNP_P3P Method is based on the paper of X.S. Gao, X.-R. Hou, J. Tang, H.-F. Chang "Complete
 * Solution Classification for the Perspective-Three-Point Problem" (gao2003complete). In this case the
 * function requires exactly four object and image points.SOLVEPNP_AP3P Method is based on the paper of
 * T. Ke, S. Roumeliotis "An Efficient Algebraic Solution to the Perspective-Three-Point Problem"
 * (Ke17). In this case the function requires exactly four object and image points.SOLVEPNP_EPNP Method
 * has been introduced by F.Moreno-Noguer, V.Lepetit and P.Fua in the paper "EPnP: Efficient
 * Perspective-n-Point Camera Pose Estimation" (lepetit2009epnp).SOLVEPNP_DLS Method is based on the
 * paper of Joel A. Hesch and Stergios I. Roumeliotis. "A Direct Least-Squares (DLS) Method for PnP"
 * (hesch2011direct).SOLVEPNP_UPNP Method is based on the paper of A.Penate-Sanchez, J.Andrade-Cetto,
 * F.Moreno-Noguer. "Exhaustive Linearization for Robust Camera Pose and Focal Length
 * Estimation" (penate2013exhaustive). In this case the function also estimates the parameters $f_x$
 * and $f_y$ assuming that both have the same value. Then the cameraMatrix is updated with the
 * estimated focal length.SOLVEPNP_IPPE Method is based on the paper of T. Collins and A. Bartoli.
 * "Infinitesimal Plane-Based Pose Estimation" (Collins14). This method requires coplanar object
 * points.SOLVEPNP_IPPE_SQUARE Method is based on the paper of Toby Collins and Adrien Bartoli.
 * "Infinitesimal Plane-Based Pose Estimation" (Collins14). This method is suitable for marker pose
 * estimation. It requires 4 coplanar object points defined in the following order:
 * point 0: [-squareLength / 2, squareLength / 2, 0]point 1: [ squareLength / 2, squareLength / 2,
 * 0]point 2: [ squareLength / 2, -squareLength / 2, 0]point 3: [-squareLength / 2, -squareLength / 2,
 * 0]
 *
 * @param rvec Rotation vector used to initialize an iterative PnP refinement algorithm, when flag is
 * SOLVEPNP_ITERATIVE and useExtrinsicGuess is set to true.
 *
 * @param tvec Translation vector used to initialize an iterative PnP refinement algorithm, when flag
 * is SOLVEPNP_ITERATIVE and useExtrinsicGuess is set to true.
 *
 * @param reprojectionError Optional vector of reprojection error, that is the RMS error ( $
 * \text{RMSE} = \sqrt{\frac{\sum_{i}^{N} \left ( \hat{y_i} - y_i \right )^2}{N}} $) between the input
 * image points and the 3D object points projected with the estimated pose.
 */
export declare function solvePnPGeneric(objectPoints: Mat, imagePoints: Mat, cameraMatrix: Mat, distCoeffs: Mat, rvecs: MatVector, tvecs: MatVector, useExtrinsicGuess?: bool, flags?: SolvePnPMethod, rvec?: Mat, tvec?: Mat, reprojectionError?: Mat): int;
/**
 * The function estimates an object pose given a set of object points, their corresponding image
 * projections, as well as the camera matrix and the distortion coefficients. This function finds such
 * a pose that minimizes reprojection error, that is, the sum of squared distances between the observed
 * projections imagePoints and the projected (using [projectPoints] ) objectPoints. The use of RANSAC
 * makes the function resistant to outliers.
 *
 * An example of how to use solvePNPRansac for object detection can be found at
 * opencv_source_code/samples/cpp/tutorial_code/calib3d/real_time_pose_estimation/
 * The default method used to estimate the camera pose for the Minimal Sample Sets step is
 * [SOLVEPNP_EPNP]. Exceptions are:
 *
 * if you choose [SOLVEPNP_P3P] or [SOLVEPNP_AP3P], these methods will be used.
 * if the number of input points is equal to 4, [SOLVEPNP_P3P] is used.
 *
 * The method used to estimate the camera pose using all the inliers is defined by the flags parameters
 * unless it is equal to [SOLVEPNP_P3P] or [SOLVEPNP_AP3P]. In this case, the method [SOLVEPNP_EPNP]
 * will be used instead.
 *
 * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1
 * 3-channel, where N is the number of points. vector<Point3f> can be also passed here.
 *
 * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N
 * is the number of points. vector<Point2f> can be also passed here.
 *
 * @param cameraMatrix Input camera matrix $A = \vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .
 *
 * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6
 * [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is
 * NULL/empty, the zero distortion coefficients are assumed.
 *
 * @param rvec Output rotation vector (see Rodrigues ) that, together with tvec, brings points from the
 * model coordinate system to the camera coordinate system.
 *
 * @param tvec Output translation vector.
 *
 * @param useExtrinsicGuess Parameter used for SOLVEPNP_ITERATIVE. If true (1), the function uses the
 * provided rvec and tvec values as initial approximations of the rotation and translation vectors,
 * respectively, and further optimizes them.
 *
 * @param iterationsCount Number of iterations.
 *
 * @param reprojectionError Inlier threshold value used by the RANSAC procedure. The parameter value is
 * the maximum allowed distance between the observed and computed point projections to consider it an
 * inlier.
 *
 * @param confidence The probability that the algorithm produces a useful result.
 *
 * @param inliers Output vector that contains indices of inliers in objectPoints and imagePoints .
 *
 * @param flags Method for solving a PnP problem (see solvePnP ).
 */
export declare function solvePnPRansac(objectPoints: Mat, imagePoints: Mat, cameraMatrix: Mat, distCoeffs: Mat, rvec: Mat, tvec: Mat, useExtrinsicGuess?: bool, iterationsCount?: int, reprojectionError?: float, confidence?: double, inliers?: Mat, flags?: int): bool;
/**
 * The function refines the object pose given at least 3 object points, their corresponding image
 * projections, an initial solution for the rotation and translation vector, as well as the camera
 * matrix and the distortion coefficients. The function minimizes the projection error with respect to
 * the rotation and the translation vectors, according to a Levenberg-Marquardt iterative minimization
 * Madsen04 Eade13 process.
 *
 * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1
 * 3-channel, where N is the number of points. vector<Point3f> can also be passed here.
 *
 * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N
 * is the number of points. vector<Point2f> can also be passed here.
 *
 * @param cameraMatrix Input camera matrix $A = \vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .
 *
 * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6
 * [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is
 * NULL/empty, the zero distortion coefficients are assumed.
 *
 * @param rvec Input/Output rotation vector (see Rodrigues ) that, together with tvec, brings points
 * from the model coordinate system to the camera coordinate system. Input values are used as an
 * initial solution.
 *
 * @param tvec Input/Output translation vector. Input values are used as an initial solution.
 *
 * @param criteria Criteria when to stop the Levenberg-Marquard iterative algorithm.
 */
export declare function solvePnPRefineLM(objectPoints: Mat, imagePoints: Mat, cameraMatrix: Mat, distCoeffs: Mat, rvec: Mat, tvec: Mat, criteria?: TermCriteria): void;
/**
 * The function refines the object pose given at least 3 object points, their corresponding image
 * projections, an initial solution for the rotation and translation vector, as well as the camera
 * matrix and the distortion coefficients. The function minimizes the projection error with respect to
 * the rotation and the translation vectors, using a virtual visual servoing (VVS) Chaumette06
 * Marchand16 scheme.
 *
 * @param objectPoints Array of object points in the object coordinate space, Nx3 1-channel or 1xN/Nx1
 * 3-channel, where N is the number of points. vector<Point3f> can also be passed here.
 *
 * @param imagePoints Array of corresponding image points, Nx2 1-channel or 1xN/Nx1 2-channel, where N
 * is the number of points. vector<Point2f> can also be passed here.
 *
 * @param cameraMatrix Input camera matrix $A = \vecthreethree{fx}{0}{cx}{0}{fy}{cy}{0}{0}{1}$ .
 *
 * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5, k_6
 * [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is
 * NULL/empty, the zero distortion coefficients are assumed.
 *
 * @param rvec Input/Output rotation vector (see Rodrigues ) that, together with tvec, brings points
 * from the model coordinate system to the camera coordinate system. Input values are used as an
 * initial solution.
 *
 * @param tvec Input/Output translation vector. Input values are used as an initial solution.
 *
 * @param criteria Criteria when to stop the Levenberg-Marquard iterative algorithm.
 *
 * @param VVSlambda Gain for the virtual visual servoing control law, equivalent to the $\alpha$ gain
 * in the Damped Gauss-Newton formulation.
 */
export declare function solvePnPRefineVVS(objectPoints: Mat, imagePoints: Mat, cameraMatrix: Mat, distCoeffs: Mat, rvec: Mat, tvec: Mat, criteria?: TermCriteria, VVSlambda?: double): void;
/**
 * The function estimates transformation between two cameras making a stereo pair. If you have a stereo
 * camera where the relative position and orientation of two cameras is fixed, and if you computed
 * poses of an object relative to the first camera and to the second camera, (R1, T1) and (R2, T2),
 * respectively (this can be done with solvePnP ), then those poses definitely relate to each other.
 * This means that, given ( `$R_1$`, `$T_1$` ), it should be possible to compute ( `$R_2$`, `$T_2$` ).
 * You only need to know the position and orientation of the second camera relative to the first
 * camera. This is what the described function does. It computes ( `$R$`, `$T$` ) so that:
 *
 * `\\[R_2=R*R_1\\]` `\\[T_2=R*T_1 + T,\\]`
 *
 * Optionally, it computes the essential matrix E:
 *
 * `\\[E= \\vecthreethree{0}{-T_2}{T_1}{T_2}{0}{-T_0}{-T_1}{T_0}{0} *R\\]`
 *
 * where `$T_i$` are components of the translation vector `$T$` : `$T=[T_0, T_1, T_2]^T$` . And the
 * function can also compute the fundamental matrix F:
 *
 * `\\[F = cameraMatrix2^{-T} E cameraMatrix1^{-1}\\]`
 *
 * Besides the stereo-related information, the function can also perform a full calibration of each of
 * two cameras. However, due to the high dimensionality of the parameter space and noise in the input
 * data, the function can diverge from the correct solution. If the intrinsic parameters can be
 * estimated with high accuracy for each of the cameras individually (for example, using
 * calibrateCamera ), you are recommended to do so and then pass CALIB_FIX_INTRINSIC flag to the
 * function along with the computed intrinsic parameters. Otherwise, if all the parameters are
 * estimated at once, it makes sense to restrict some parameters, for example, pass
 * CALIB_SAME_FOCAL_LENGTH and CALIB_ZERO_TANGENT_DIST flags, which is usually a reasonable assumption.
 *
 * Similarly to calibrateCamera , the function minimizes the total re-projection error for all the
 * points in all the available views from both cameras. The function returns the final value of the
 * re-projection error.
 *
 * @param objectPoints Vector of vectors of the calibration pattern points.
 *
 * @param imagePoints1 Vector of vectors of the projections of the calibration pattern points, observed
 * by the first camera.
 *
 * @param imagePoints2 Vector of vectors of the projections of the calibration pattern points, observed
 * by the second camera.
 *
 * @param cameraMatrix1 Input/output first camera matrix:
 * $\vecthreethree{f_x^{(j)}}{0}{c_x^{(j)}}{0}{f_y^{(j)}}{c_y^{(j)}}{0}{0}{1}$ , $j = 0,\, 1$ . If any
 * of CALIB_USE_INTRINSIC_GUESS , CALIB_FIX_ASPECT_RATIO , CALIB_FIX_INTRINSIC , or
 * CALIB_FIX_FOCAL_LENGTH are specified, some or all of the matrix components must be initialized. See
 * the flags description for details.
 *
 * @param distCoeffs1 Input/output vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4,
 * k_5, k_6 [, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. The output
 * vector length depends on the flags.
 *
 * @param cameraMatrix2 Input/output second camera matrix. The parameter is similar to cameraMatrix1
 *
 * @param distCoeffs2 Input/output lens distortion coefficients for the second camera. The parameter is
 * similar to distCoeffs1 .
 *
 * @param imageSize Size of the image used only to initialize intrinsic camera matrix.
 *
 * @param R Output rotation matrix between the 1st and the 2nd camera coordinate systems.
 *
 * @param T Output translation vector between the coordinate systems of the cameras.
 *
 * @param E Output essential matrix.
 *
 * @param F Output fundamental matrix.
 *
 * @param perViewErrors Output vector of the RMS re-projection error estimated for each pattern view.
 *
 * @param flags Different flags that may be zero or a combination of the following values:
 * CALIB_FIX_INTRINSIC Fix cameraMatrix? and distCoeffs? so that only R, T, E , and F matrices are
 * estimated.CALIB_USE_INTRINSIC_GUESS Optimize some or all of the intrinsic parameters according to
 * the specified flags. Initial values are provided by the user.CALIB_USE_EXTRINSIC_GUESS R, T contain
 * valid initial values that are optimized further. Otherwise R, T are initialized to the median value
 * of the pattern views (each dimension separately).CALIB_FIX_PRINCIPAL_POINT Fix the principal points
 * during the optimization.CALIB_FIX_FOCAL_LENGTH Fix $f^{(j)}_x$ and $f^{(j)}_y$
 * .CALIB_FIX_ASPECT_RATIO Optimize $f^{(j)}_y$ . Fix the ratio $f^{(j)}_x/f^{(j)}_y$
 *
 * CALIB_SAME_FOCAL_LENGTH Enforce $f^{(0)}_x=f^{(1)}_x$ and $f^{(0)}_y=f^{(1)}_y$
 * .CALIB_ZERO_TANGENT_DIST Set tangential distortion coefficients for each camera to zeros and fix
 * there.CALIB_FIX_K1,...,CALIB_FIX_K6 Do not change the corresponding radial distortion coefficient
 * during the optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the supplied
 * distCoeffs matrix is used. Otherwise, it is set to 0.CALIB_RATIONAL_MODEL Enable coefficients k4,
 * k5, and k6. To provide the backward compatibility, this extra flag should be explicitly specified to
 * make the calibration function use the rational model and return 8 coefficients. If the flag is not
 * set, the function computes and returns only 5 distortion coefficients.CALIB_THIN_PRISM_MODEL
 * Coefficients s1, s2, s3 and s4 are enabled. To provide the backward compatibility, this extra flag
 * should be explicitly specified to make the calibration function use the thin prism model and return
 * 12 coefficients. If the flag is not set, the function computes and returns only 5 distortion
 * coefficients.CALIB_FIX_S1_S2_S3_S4 The thin prism distortion coefficients are not changed during the
 * optimization. If CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the supplied distCoeffs
 * matrix is used. Otherwise, it is set to 0.CALIB_TILTED_MODEL Coefficients tauX and tauY are enabled.
 * To provide the backward compatibility, this extra flag should be explicitly specified to make the
 * calibration function use the tilted sensor model and return 14 coefficients. If the flag is not set,
 * the function computes and returns only 5 distortion coefficients.CALIB_FIX_TAUX_TAUY The
 * coefficients of the tilted sensor model are not changed during the optimization. If
 * CALIB_USE_INTRINSIC_GUESS is set, the coefficient from the supplied distCoeffs matrix is used.
 * Otherwise, it is set to 0.
 *
 * @param criteria Termination criteria for the iterative optimization algorithm.
 */
export declare function stereoCalibrate(objectPoints: MatVector, imagePoints1: MatVector, imagePoints2: MatVector, cameraMatrix1: Mat, distCoeffs1: Mat, cameraMatrix2: Mat, distCoeffs2: Mat, imageSize: Size, R: Mat, T: Mat, E: Mat, F: Mat, perViewErrors: Mat, flags?: int, criteria?: TermCriteria): double;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function stereoCalibrate(objectPoints: MatVector, imagePoints1: MatVector, imagePoints2: MatVector, cameraMatrix1: Mat, distCoeffs1: Mat, cameraMatrix2: Mat, distCoeffs2: Mat, imageSize: Size, R: Mat, T: Mat, E: Mat, F: Mat, flags?: int, criteria?: TermCriteria): double;
/**
 * The function computes the rotation matrices for each camera that (virtually) make both camera image
 * planes the same plane. Consequently, this makes all the epipolar lines parallel and thus simplifies
 * the dense stereo correspondence problem. The function takes the matrices computed by stereoCalibrate
 * as input. As output, it provides two rotation matrices and also two projection matrices in the new
 * coordinates. The function distinguishes the following two cases:
 *
 * **Horizontal stereo**: the first and the second camera views are shifted relative to each other
 * mainly along the x axis (with possible small vertical shift). In the rectified images, the
 * corresponding epipolar lines in the left and right cameras are horizontal and have the same
 * y-coordinate. P1 and P2 look like:`\\[\\texttt{P1} = \\begin{bmatrix} f & 0 & cx_1 & 0 \\\\ 0 & f &
 * cy & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix}\\]``\\[\\texttt{P2} = \\begin{bmatrix} f & 0 & cx_2 & T_x*f
 * \\\\ 0 & f & cy & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} ,\\]`where `$T_x$` is a horizontal shift
 * between the cameras and `$cx_1=cx_2$` if CALIB_ZERO_DISPARITY is set.
 * **Vertical stereo**: the first and the second camera views are shifted relative to each other mainly
 * in vertical direction (and probably a bit in the horizontal direction too). The epipolar lines in
 * the rectified images are vertical and have the same x-coordinate. P1 and P2 look
 * like:`\\[\\texttt{P1} = \\begin{bmatrix} f & 0 & cx & 0 \\\\ 0 & f & cy_1 & 0 \\\\ 0 & 0 & 1 & 0
 * \\end{bmatrix}\\]``\\[\\texttt{P2} = \\begin{bmatrix} f & 0 & cx & 0 \\\\ 0 & f & cy_2 & T_y*f \\\\
 * 0 & 0 & 1 & 0 \\end{bmatrix} ,\\]`where `$T_y$` is a vertical shift between the cameras and
 * `$cy_1=cy_2$` if CALIB_ZERO_DISPARITY is set.
 *
 * As you can see, the first three columns of P1 and P2 will effectively be the new "rectified" camera
 * matrices. The matrices, together with R1 and R2 , can then be passed to initUndistortRectifyMap to
 * initialize the rectification map for each camera.
 *
 * See below the screenshot from the stereo_calib.cpp sample. Some red horizontal lines pass through
 * the corresponding image regions. This means that the images are well rectified, which is what most
 * stereo correspondence algorithms rely on. The green rectangles are roi1 and roi2 . You see that
 * their interiors are all valid pixels.
 *
 * @param cameraMatrix1 First camera matrix.
 *
 * @param distCoeffs1 First camera distortion parameters.
 *
 * @param cameraMatrix2 Second camera matrix.
 *
 * @param distCoeffs2 Second camera distortion parameters.
 *
 * @param imageSize Size of the image used for stereo calibration.
 *
 * @param R Rotation matrix between the coordinate systems of the first and the second cameras.
 *
 * @param T Translation vector between coordinate systems of the cameras.
 *
 * @param R1 Output 3x3 rectification transform (rotation matrix) for the first camera.
 *
 * @param R2 Output 3x3 rectification transform (rotation matrix) for the second camera.
 *
 * @param P1 Output 3x4 projection matrix in the new (rectified) coordinate systems for the first
 * camera.
 *
 * @param P2 Output 3x4 projection matrix in the new (rectified) coordinate systems for the second
 * camera.
 *
 * @param Q Output $4 \times 4$ disparity-to-depth mapping matrix (see reprojectImageTo3D ).
 *
 * @param flags Operation flags that may be zero or CALIB_ZERO_DISPARITY . If the flag is set, the
 * function makes the principal points of each camera have the same pixel coordinates in the rectified
 * views. And if the flag is not set, the function may still shift the images in the horizontal or
 * vertical direction (depending on the orientation of epipolar lines) to maximize the useful image
 * area.
 *
 * @param alpha Free scaling parameter. If it is -1 or absent, the function performs the default
 * scaling. Otherwise, the parameter should be between 0 and 1. alpha=0 means that the rectified images
 * are zoomed and shifted so that only valid pixels are visible (no black areas after rectification).
 * alpha=1 means that the rectified image is decimated and shifted so that all the pixels from the
 * original images from the cameras are retained in the rectified images (no source image pixels are
 * lost). Obviously, any intermediate value yields an intermediate result between those two extreme
 * cases.
 *
 * @param newImageSize New image resolution after rectification. The same size should be passed to
 * initUndistortRectifyMap (see the stereo_calib.cpp sample in OpenCV samples directory). When (0,0) is
 * passed (default), it is set to the original imageSize . Setting it to larger value can help you
 * preserve details in the original image, especially when there is a big radial distortion.
 *
 * @param validPixROI1 Optional output rectangles inside the rectified images where all the pixels are
 * valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller (see
 * the picture below).
 *
 * @param validPixROI2 Optional output rectangles inside the rectified images where all the pixels are
 * valid. If alpha=0 , the ROIs cover the whole images. Otherwise, they are likely to be smaller (see
 * the picture below).
 */
export declare function stereoRectify(cameraMatrix1: Mat, distCoeffs1: Mat, cameraMatrix2: Mat, distCoeffs2: Mat, imageSize: Size, R: Mat, T: Mat, R1: Mat, R2: Mat, P1: Mat, P2: Mat, Q: Mat, flags?: int, alpha?: double, newImageSize?: Size, validPixROI1?: any, validPixROI2?: any): void;
/**
 * The function computes the rectification transformations without knowing intrinsic parameters of the
 * cameras and their relative position in the space, which explains the suffix "uncalibrated". Another
 * related difference from stereoRectify is that the function outputs not the rectification
 * transformations in the object (3D) space, but the planar perspective transformations encoded by the
 * homography matrices H1 and H2 . The function implements the algorithm Hartley99 .
 *
 * While the algorithm does not need to know the intrinsic parameters of the cameras, it heavily
 * depends on the epipolar geometry. Therefore, if the camera lenses have a significant distortion, it
 * would be better to correct it before computing the fundamental matrix and calling this function. For
 * example, distortion coefficients can be estimated for each head of stereo camera separately by using
 * calibrateCamera . Then, the images can be corrected using undistort , or just the point coordinates
 * can be corrected with undistortPoints .
 *
 * @param points1 Array of feature points in the first image.
 *
 * @param points2 The corresponding points in the second image. The same formats as in
 * findFundamentalMat are supported.
 *
 * @param F Input fundamental matrix. It can be computed from the same set of point pairs using
 * findFundamentalMat .
 *
 * @param imgSize Size of the image.
 *
 * @param H1 Output rectification homography matrix for the first image.
 *
 * @param H2 Output rectification homography matrix for the second image.
 *
 * @param threshold Optional threshold used to filter out the outliers. If the parameter is greater
 * than zero, all the point pairs that do not comply with the epipolar geometry (that is, the points
 * for which $|\texttt{points2[i]}^T*\texttt{F}*\texttt{points1[i]}|>\texttt{threshold}$ ) are rejected
 * prior to computing the homographies. Otherwise, all the points are considered inliers.
 */
export declare function stereoRectifyUncalibrated(points1: Mat, points2: Mat, F: Mat, imgSize: Size, H1: Mat, H2: Mat, threshold?: double): bool;
/**
 * The function reconstructs 3-dimensional points (in homogeneous coordinates) by using their
 * observations with a stereo camera. Projections matrices can be obtained from stereoRectify.
 *
 * Keep in mind that all input data should be of float type in order for this function to work.
 *
 * [reprojectImageTo3D]
 *
 * @param projMatr1 3x4 projection matrix of the first camera.
 *
 * @param projMatr2 3x4 projection matrix of the second camera.
 *
 * @param projPoints1 2xN array of feature points in the first image. In case of c++ version it can be
 * also a vector of feature points or two-channel matrix of size 1xN or Nx1.
 *
 * @param projPoints2 2xN array of corresponding points in the second image. In case of c++ version it
 * can be also a vector of feature points or two-channel matrix of size 1xN or Nx1.
 *
 * @param points4D 4xN array of reconstructed points in homogeneous coordinates.
 */
export declare function triangulatePoints(projMatr1: Mat, projMatr2: Mat, projPoints1: Mat, projPoints2: Mat, points4D: Mat): void;
/**
 * The function transforms an image to compensate radial and tangential lens distortion.
 *
 * The function is simply a combination of [initUndistortRectifyMap] (with unity R ) and [remap] (with
 * bilinear interpolation). See the former function for details of the transformation being performed.
 *
 * Those pixels in the destination image, for which there is no correspondent pixels in the source
 * image, are filled with zeros (black color).
 *
 * A particular subset of the source image that will be visible in the corrected image can be regulated
 * by newCameraMatrix. You can use [getOptimalNewCameraMatrix] to compute the appropriate
 * newCameraMatrix depending on your requirements.
 *
 * The camera matrix and the distortion parameters can be determined using [calibrateCamera]. If the
 * resolution of images is different from the resolution used at the calibration stage, `$f_x, f_y,
 * c_x$` and `$c_y$` need to be scaled accordingly, while the distortion coefficients remain the same.
 *
 * @param src Input (distorted) image.
 *
 * @param dst Output (corrected) image that has the same size and type as src .
 *
 * @param cameraMatrix Input camera matrix $A = \vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ .
 *
 * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5,
 * k_6[, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is
 * NULL/empty, the zero distortion coefficients are assumed.
 *
 * @param newCameraMatrix Camera matrix of the distorted image. By default, it is the same as
 * cameraMatrix but you may additionally scale and shift the result by using a different matrix.
 */
export declare function undistort(src: Mat, dst: Mat, cameraMatrix: Mat, distCoeffs: Mat, newCameraMatrix?: Mat): void;
/**
 * The function is similar to [undistort] and [initUndistortRectifyMap] but it operates on a sparse set
 * of points instead of a raster image. Also the function performs a reverse transformation to
 * projectPoints. In case of a 3D object, it does not reconstruct its 3D coordinates, but for a planar
 * object, it does, up to a translation vector, if the proper R is specified.
 *
 * For each observed point coordinate `$(u, v)$` the function computes: `\\[ \\begin{array}{l} x^{"}
 * \\leftarrow (u - c_x)/f_x \\\\ y^{"} \\leftarrow (v - c_y)/f_y \\\\ (x',y') = undistort(x^{"},y^{"},
 * \\texttt{distCoeffs}) \\\\ {[X\\,Y\\,W]} ^T \\leftarrow R*[x' \\, y' \\, 1]^T \\\\ x \\leftarrow X/W
 * \\\\ y \\leftarrow Y/W \\\\ \\text{only performed if P is specified:} \\\\ u' \\leftarrow x {f'}_x +
 * {c'}_x \\\\ v' \\leftarrow y {f'}_y + {c'}_y \\end{array} \\]`
 *
 * where *undistort* is an approximate iterative algorithm that estimates the normalized original point
 * coordinates out of the normalized distorted point coordinates ("normalized" means that the
 * coordinates do not depend on the camera matrix).
 *
 * The function can be used for both a stereo camera head or a monocular camera (when R is empty).
 *
 * @param src Observed point coordinates, 2xN/Nx2 1-channel or 1xN/Nx1 2-channel (CV_32FC2 or CV_64FC2)
 * (or vector<Point2f> ).
 *
 * @param dst Output ideal point coordinates (1xN/Nx1 2-channel or vector<Point2f> ) after undistortion
 * and reverse perspective transformation. If matrix P is identity or omitted, dst will contain
 * normalized point coordinates.
 *
 * @param cameraMatrix Camera matrix $\vecthreethree{f_x}{0}{c_x}{0}{f_y}{c_y}{0}{0}{1}$ .
 *
 * @param distCoeffs Input vector of distortion coefficients $(k_1, k_2, p_1, p_2[, k_3[, k_4, k_5,
 * k_6[, s_1, s_2, s_3, s_4[, \tau_x, \tau_y]]]])$ of 4, 5, 8, 12 or 14 elements. If the vector is
 * NULL/empty, the zero distortion coefficients are assumed.
 *
 * @param R Rectification transformation in the object space (3x3 matrix). R1 or R2 computed by
 * stereoRectify can be passed here. If the matrix is empty, the identity transformation is used.
 *
 * @param P New camera matrix (3x3) or new projection matrix (3x4) $\begin{bmatrix} {f'}_x & 0 & {c'}_x
 * & t_x \\ 0 & {f'}_y & {c'}_y & t_y \\ 0 & 0 & 1 & t_z \end{bmatrix}$. P1 or P2 computed by
 * stereoRectify can be passed here. If the matrix is empty, the identity new camera matrix is used.
 */
export declare function undistortPoints(src: Mat, dst: Mat, cameraMatrix: Mat, distCoeffs: Mat, R?: Mat, P?: Mat): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * Default version of [undistortPoints] does 5 iterations to compute undistorted points.
 */
export declare function undistortPoints(src: Mat, dst: Mat, cameraMatrix: Mat, distCoeffs: Mat, R: Mat, P: Mat, criteria: TermCriteria): void;
export declare function validateDisparity(disparity: Mat, cost: Mat, minDisparity: int, numberOfDisparities: int, disp12MaxDisp?: int): void;
export declare const LMEDS: any;
export declare const RANSAC: any;
export declare const RHO: any;
export declare const CALIB_CB_ADAPTIVE_THRESH: any;
export declare const CALIB_CB_NORMALIZE_IMAGE: any;
export declare const CALIB_CB_FILTER_QUADS: any;
export declare const CALIB_CB_FAST_CHECK: any;
export declare const CALIB_CB_EXHAUSTIVE: any;
export declare const CALIB_CB_ACCURACY: any;
export declare const CALIB_CB_SYMMETRIC_GRID: any;
export declare const CALIB_CB_ASYMMETRIC_GRID: any;
export declare const CALIB_CB_CLUSTERING: any;
export declare const CALIB_NINTRINSIC: any;
export declare const CALIB_USE_INTRINSIC_GUESS: any;
export declare const CALIB_FIX_ASPECT_RATIO: any;
export declare const CALIB_FIX_PRINCIPAL_POINT: any;
export declare const CALIB_ZERO_TANGENT_DIST: any;
export declare const CALIB_FIX_FOCAL_LENGTH: any;
export declare const CALIB_FIX_K1: any;
export declare const CALIB_FIX_K2: any;
export declare const CALIB_FIX_K3: any;
export declare const CALIB_FIX_K4: any;
export declare const CALIB_FIX_K5: any;
export declare const CALIB_FIX_K6: any;
export declare const CALIB_RATIONAL_MODEL: any;
export declare const CALIB_THIN_PRISM_MODEL: any;
export declare const CALIB_FIX_S1_S2_S3_S4: any;
export declare const CALIB_TILTED_MODEL: any;
export declare const CALIB_FIX_TAUX_TAUY: any;
export declare const CALIB_USE_QR: any;
export declare const CALIB_FIX_TANGENT_DIST: any;
export declare const CALIB_FIX_INTRINSIC: any;
export declare const CALIB_SAME_FOCAL_LENGTH: any;
export declare const CALIB_ZERO_DISPARITY: any;
export declare const CALIB_USE_LU: any;
export declare const CALIB_USE_EXTRINSIC_GUESS: any;
export declare const FM_7POINT: any;
export declare const FM_8POINT: any;
export declare const FM_LMEDS: any;
export declare const FM_RANSAC: any;
export declare const CALIB_HAND_EYE_TSAI: HandEyeCalibrationMethod;
export declare const CALIB_HAND_EYE_PARK: HandEyeCalibrationMethod;
export declare const CALIB_HAND_EYE_HORAUD: HandEyeCalibrationMethod;
export declare const CALIB_HAND_EYE_ANDREFF: HandEyeCalibrationMethod;
export declare const CALIB_HAND_EYE_DANIILIDIS: HandEyeCalibrationMethod;
export declare const SOLVEPNP_ITERATIVE: SolvePnPMethod;
export declare const SOLVEPNP_EPNP: SolvePnPMethod;
export declare const SOLVEPNP_P3P: SolvePnPMethod;
export declare const SOLVEPNP_DLS: SolvePnPMethod;
export declare const SOLVEPNP_UPNP: SolvePnPMethod;
export declare const SOLVEPNP_AP3P: SolvePnPMethod;
/**
 * Infinitesimal Plane-Based Pose Estimation Collins14
 *  Object points must be coplanar.
 *
 */
export declare const SOLVEPNP_IPPE: SolvePnPMethod;
/**
 * Infinitesimal Plane-Based Pose Estimation Collins14
 *  This is a special case suitable for marker pose estimation.
 *  4 coplanar object points must be defined in the following order:
 *
 * point 0: [-squareLength / 2, squareLength / 2, 0]
 * point 1: [ squareLength / 2, squareLength / 2, 0]
 * point 2: [ squareLength / 2, -squareLength / 2, 0]
 * point 3: [-squareLength / 2, -squareLength / 2, 0]
 *
 */
export declare const SOLVEPNP_IPPE_SQUARE: SolvePnPMethod;
export declare const PROJ_SPHERICAL_ORTHO: UndistortTypes;
export declare const PROJ_SPHERICAL_EQRECT: UndistortTypes;
export type HandEyeCalibrationMethod = any;
export type SolvePnPMethod = any;
export type UndistortTypes = any;
export declare class CascadeClassifier extends Mat {
	cc: Ptr;
	constructor();
	/**
	 * @param filename Name of the file from which the classifier is loaded.
	 */
	constructor(filename: String);
	/**
	 *   The function is parallelized with the TBB library.
	 *
	 * (Python) A face detection example using cascade classifiers can be found at
	 * opencv_source_code/samples/python/facedetect.py
	 *
	 * @param image Matrix of the type CV_8U containing an image where objects are detected.
	 *
	 * @param objects Vector of rectangles where each rectangle contains the detected object, the
	 * rectangles may be partially outside the original image.
	 *
	 * @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.
	 *
	 * @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have
	 * to retain it.
	 *
	 * @param flags Parameter with the same meaning for an old cascade as in the function
	 * cvHaarDetectObjects. It is not used for a new cascade.
	 *
	 * @param minSize Minimum possible object size. Objects smaller than that are ignored.
	 *
	 * @param maxSize Maximum possible object size. Objects larger than that are ignored. If maxSize ==
	 * minSize model is evaluated on single scale.
	 */
	detectMultiScale(image: Mat, objects: any, scaleFactor?: double, minNeighbors?: int, flags?: int, minSize?: Size, maxSize?: Size): Mat;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param image Matrix of the type CV_8U containing an image where objects are detected.
	 *
	 * @param objects Vector of rectangles where each rectangle contains the detected object, the
	 * rectangles may be partially outside the original image.
	 *
	 * @param numDetections Vector of detection numbers for the corresponding objects. An object's number
	 * of detections is the number of neighboring positively classified rectangles that were joined
	 * together to form the object.
	 *
	 * @param scaleFactor Parameter specifying how much the image size is reduced at each image scale.
	 *
	 * @param minNeighbors Parameter specifying how many neighbors each candidate rectangle should have
	 * to retain it.
	 *
	 * @param flags Parameter with the same meaning for an old cascade as in the function
	 * cvHaarDetectObjects. It is not used for a new cascade.
	 *
	 * @param minSize Minimum possible object size. Objects smaller than that are ignored.
	 *
	 * @param maxSize Maximum possible object size. Objects larger than that are ignored. If maxSize ==
	 * minSize model is evaluated on single scale.
	 */
	detectMultiScale(image: Mat, objects: any, numDetections: any, scaleFactor?: double, minNeighbors?: int, flags?: int, minSize?: Size, maxSize?: Size): Mat;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts. This function allows you to retrieve the final stage
	 * decision certainty of classification. For this, one needs to set `outputRejectLevels` on true and
	 * provide the `rejectLevels` and `levelWeights` parameter. For each resulting detection,
	 * `levelWeights` will then contain the certainty of classification at the final stage. This value can
	 * then be used to separate strong from weaker classifications.
	 *
	 *   A code sample on how to use it efficiently can be found below:
	 *
	 *   ```cpp
	 *   Mat img;
	 *   vector<double> weights;
	 *   vector<int> levels;
	 *   vector<Rect> detections;
	 *   CascadeClassifier model("/path/to/your/model.xml");
	 *   model.detectMultiScale(img, detections, levels, weights, 1.1, 3, 0, Size(), Size(), true);
	 *   cerr << "Detection " << detections[0] << " with weight " << weights[0] << endl;
	 *   ```
	 */
	detectMultiScale(image: Mat, objects: any, rejectLevels: any, levelWeights: any, scaleFactor?: double, minNeighbors?: int, flags?: int, minSize?: Size, maxSize?: Size, outputRejectLevels?: bool): Mat;
	empty(): bool;
	getFeatureType(): int;
	getMaskGenerator(): Ptr;
	getOldCascade(): any;
	getOriginalWindowSize(): Size;
	isOldFormatCascade(): bool;
	/**
	 * @param filename Name of the file from which the classifier is loaded. The file may contain an old
	 * HAAR classifier trained by the haartraining application or a new cascade classifier trained by the
	 * traincascade application.
	 */
	load(filename: String): String;
	/**
	 *   The file may contain a new cascade classifier (trained traincascade application) only.
	 */
	read(node: FileNode): FileNode;
	setMaskGenerator(maskGenerator: Ptr): Ptr;
	static convert(oldcascade: String, newcascade: String): String;
}
/**
 * The function [cv::absdiff] calculates: Absolute difference between two arrays when they have the
 * same size and type: `\\[\\texttt{dst}(I) = \\texttt{saturate} (| \\texttt{src1}(I) -
 * \\texttt{src2}(I)|)\\]` Absolute difference between an array and a scalar when the second array is
 * constructed from Scalar or has as many elements as the number of channels in `src1`:
 * `\\[\\texttt{dst}(I) = \\texttt{saturate} (| \\texttt{src1}(I) - \\texttt{src2} |)\\]` Absolute
 * difference between a scalar and an array when the first array is constructed from Scalar or has as
 * many elements as the number of channels in `src2`: `\\[\\texttt{dst}(I) = \\texttt{saturate} (|
 * \\texttt{src1} - \\texttt{src2}(I) |)\\]` where I is a multi-dimensional index of array elements. In
 * case of multi-channel arrays, each channel is processed independently.
 *
 * Saturation is not applied when the arrays have the depth CV_32S. You may even get a negative value
 * in the case of overflow.
 *
 * cv::abs(const Mat&)
 *
 * @param src1 first input array or a scalar.
 *
 * @param src2 second input array or a scalar.
 *
 * @param dst output array that has the same size and type as input arrays.
 */
export declare function absdiff(src1: Mat, src2: Mat, dst: Mat): void;
/**
 * The function add calculates:
 *
 * Sum of two arrays when both input arrays have the same size and the same number of channels:
 * `\\[\\texttt{dst}(I) = \\texttt{saturate} ( \\texttt{src1}(I) + \\texttt{src2}(I)) \\quad
 * \\texttt{if mask}(I) \\ne0\\]`
 * Sum of an array and a scalar when src2 is constructed from Scalar or has the same number of elements
 * as `src1.channels()`: `\\[\\texttt{dst}(I) = \\texttt{saturate} ( \\texttt{src1}(I) + \\texttt{src2}
 * ) \\quad \\texttt{if mask}(I) \\ne0\\]`
 * Sum of a scalar and an array when src1 is constructed from Scalar or has the same number of elements
 * as `src2.channels()`: `\\[\\texttt{dst}(I) = \\texttt{saturate} ( \\texttt{src1} + \\texttt{src2}(I)
 * ) \\quad \\texttt{if mask}(I) \\ne0\\]` where `I` is a multi-dimensional index of array elements. In
 * case of multi-channel arrays, each channel is processed independently.
 *
 * The first function in the list above can be replaced with matrix expressions:
 *
 * ```cpp
 * dst = src1 + src2;
 * dst += src1; // equivalent to add(dst, src1, dst);
 * ```
 *
 *  The input arrays and the output array can all have the same or different depths. For example, you
 * can add a 16-bit unsigned array to a 8-bit signed array and store the sum as a 32-bit floating-point
 * array. Depth of the output array is determined by the dtype parameter. In the second and third cases
 * above, as well as in the first case, when src1.depth() == src2.depth(), dtype can be set to the
 * default -1. In this case, the output array will have the same depth as the input array, be it src1,
 * src2 or both.
 *
 * Saturation is not applied when the output array has the depth CV_32S. You may even get result of an
 * incorrect sign in the case of overflow.
 *
 * [subtract], [addWeighted], [scaleAdd], [Mat::convertTo]
 *
 * @param src1 first input array or a scalar.
 *
 * @param src2 second input array or a scalar.
 *
 * @param dst output array that has the same size and number of channels as the input array(s); the
 * depth is defined by dtype or src1/src2.
 *
 * @param mask optional operation mask - 8-bit single channel array, that specifies elements of the
 * output array to be changed.
 *
 * @param dtype optional depth of the output array (see the discussion below).
 */
export declare function add(src1: Mat, src2: Mat, dst: Mat, mask?: Mat, dtype?: int): void;
/**
 * The function addWeighted calculates the weighted sum of two arrays as follows: `\\[\\texttt{dst}
 * (I)= \\texttt{saturate} ( \\texttt{src1} (I)* \\texttt{alpha} + \\texttt{src2} (I)* \\texttt{beta} +
 * \\texttt{gamma} )\\]` where I is a multi-dimensional index of array elements. In case of
 * multi-channel arrays, each channel is processed independently. The function can be replaced with a
 * matrix expression:
 *
 * ```cpp
 * dst = src1*alpha + src2*beta + gamma;
 * ```
 *
 * Saturation is not applied when the output array has the depth CV_32S. You may even get result of an
 * incorrect sign in the case of overflow.
 *
 * [add], [subtract], [scaleAdd], [Mat::convertTo]
 *
 * @param src1 first input array.
 *
 * @param alpha weight of the first array elements.
 *
 * @param src2 second input array of the same size and channel number as src1.
 *
 * @param beta weight of the second array elements.
 *
 * @param gamma scalar added to each sum.
 *
 * @param dst output array that has the same size and number of channels as the input arrays.
 *
 * @param dtype optional depth of the output array; when both input arrays have the same depth, dtype
 * can be set to -1, which will be equivalent to src1.depth().
 */
export declare function addWeighted(src1: Mat, alpha: double, src2: Mat, beta: double, gamma: double, dst: Mat, dtype?: int): void;
/**
 * see
 */
export declare function batchDistance(src1: Mat, src2: Mat, dist: Mat, dtype: int, nidx: Mat, normType?: int, K?: int, mask?: Mat, update?: int, crosscheck?: bool): void;
/**
 * The function [cv::bitwise_and] calculates the per-element bit-wise logical conjunction for: Two
 * arrays when src1 and src2 have the same size: `\\[\\texttt{dst} (I) = \\texttt{src1} (I) \\wedge
 * \\texttt{src2} (I) \\quad \\texttt{if mask} (I) \\ne0\\]` An array and a scalar when src2 is
 * constructed from Scalar or has the same number of elements as `src1.channels()`: `\\[\\texttt{dst}
 * (I) = \\texttt{src1} (I) \\wedge \\texttt{src2} \\quad \\texttt{if mask} (I) \\ne0\\]` A scalar and
 * an array when src1 is constructed from Scalar or has the same number of elements as
 * `src2.channels()`: `\\[\\texttt{dst} (I) = \\texttt{src1} \\wedge \\texttt{src2} (I) \\quad
 * \\texttt{if mask} (I) \\ne0\\]` In case of floating-point arrays, their machine-specific bit
 * representations (usually IEEE754-compliant) are used for the operation. In case of multi-channel
 * arrays, each channel is processed independently. In the second and third cases above, the scalar is
 * first converted to the array type.
 *
 * @param src1 first input array or a scalar.
 *
 * @param src2 second input array or a scalar.
 *
 * @param dst output array that has the same size and type as the input arrays.
 *
 * @param mask optional operation mask, 8-bit single channel array, that specifies elements of the
 * output array to be changed.
 */
export declare function bitwise_and(src1: Mat, src2: Mat, dst: Mat, mask?: Mat): void;
/**
 * The function [cv::bitwise_not] calculates per-element bit-wise inversion of the input array:
 * `\\[\\texttt{dst} (I) = \\neg \\texttt{src} (I)\\]` In case of a floating-point input array, its
 * machine-specific bit representation (usually IEEE754-compliant) is used for the operation. In case
 * of multi-channel arrays, each channel is processed independently.
 *
 * @param src input array.
 *
 * @param dst output array that has the same size and type as the input array.
 *
 * @param mask optional operation mask, 8-bit single channel array, that specifies elements of the
 * output array to be changed.
 */
export declare function bitwise_not(src: Mat, dst: Mat, mask?: Mat): void;
/**
 * The function [cv::bitwise_or] calculates the per-element bit-wise logical disjunction for: Two
 * arrays when src1 and src2 have the same size: `\\[\\texttt{dst} (I) = \\texttt{src1} (I) \\vee
 * \\texttt{src2} (I) \\quad \\texttt{if mask} (I) \\ne0\\]` An array and a scalar when src2 is
 * constructed from Scalar or has the same number of elements as `src1.channels()`: `\\[\\texttt{dst}
 * (I) = \\texttt{src1} (I) \\vee \\texttt{src2} \\quad \\texttt{if mask} (I) \\ne0\\]` A scalar and an
 * array when src1 is constructed from Scalar or has the same number of elements as `src2.channels()`:
 * `\\[\\texttt{dst} (I) = \\texttt{src1} \\vee \\texttt{src2} (I) \\quad \\texttt{if mask} (I)
 * \\ne0\\]` In case of floating-point arrays, their machine-specific bit representations (usually
 * IEEE754-compliant) are used for the operation. In case of multi-channel arrays, each channel is
 * processed independently. In the second and third cases above, the scalar is first converted to the
 * array type.
 *
 * @param src1 first input array or a scalar.
 *
 * @param src2 second input array or a scalar.
 *
 * @param dst output array that has the same size and type as the input arrays.
 *
 * @param mask optional operation mask, 8-bit single channel array, that specifies elements of the
 * output array to be changed.
 */
export declare function bitwise_or(src1: Mat, src2: Mat, dst: Mat, mask?: Mat): void;
/**
 * The function [cv::bitwise_xor] calculates the per-element bit-wise logical "exclusive-or" operation
 * for: Two arrays when src1 and src2 have the same size: `\\[\\texttt{dst} (I) = \\texttt{src1} (I)
 * \\oplus \\texttt{src2} (I) \\quad \\texttt{if mask} (I) \\ne0\\]` An array and a scalar when src2 is
 * constructed from Scalar or has the same number of elements as `src1.channels()`: `\\[\\texttt{dst}
 * (I) = \\texttt{src1} (I) \\oplus \\texttt{src2} \\quad \\texttt{if mask} (I) \\ne0\\]` A scalar and
 * an array when src1 is constructed from Scalar or has the same number of elements as
 * `src2.channels()`: `\\[\\texttt{dst} (I) = \\texttt{src1} \\oplus \\texttt{src2} (I) \\quad
 * \\texttt{if mask} (I) \\ne0\\]` In case of floating-point arrays, their machine-specific bit
 * representations (usually IEEE754-compliant) are used for the operation. In case of multi-channel
 * arrays, each channel is processed independently. In the 2nd and 3rd cases above, the scalar is first
 * converted to the array type.
 *
 * @param src1 first input array or a scalar.
 *
 * @param src2 second input array or a scalar.
 *
 * @param dst output array that has the same size and type as the input arrays.
 *
 * @param mask optional operation mask, 8-bit single channel array, that specifies elements of the
 * output array to be changed.
 */
export declare function bitwise_xor(src1: Mat, src2: Mat, dst: Mat, mask?: Mat): void;
/**
 * The function computes and returns the coordinate of a donor pixel corresponding to the specified
 * extrapolated pixel when using the specified extrapolation border mode. For example, if you use
 * [cv::BORDER_WRAP] mode in the horizontal direction, [cv::BORDER_REFLECT_101] in the vertical
 * direction and want to compute value of the "virtual" pixel Point(-5, 100) in a floating-point image
 * img , it looks like:
 *
 * ```cpp
 * float val = img.at<float>(borderInterpolate(100, img.rows, cv::BORDER_REFLECT_101),
 *                           borderInterpolate(-5, img.cols, cv::BORDER_WRAP));
 * ```
 *
 *  Normally, the function is not called directly. It is used inside filtering functions and also in
 * copyMakeBorder.
 *
 * [copyMakeBorder]
 *
 * @param p 0-based coordinate of the extrapolated pixel along one of the axes, likely <0 or >= len
 *
 * @param len Length of the array along the corresponding axis.
 *
 * @param borderType Border type, one of the BorderTypes, except for BORDER_TRANSPARENT and
 * BORDER_ISOLATED . When borderType==BORDER_CONSTANT , the function always returns -1, regardless of p
 * and len.
 */
export declare function borderInterpolate(p: int, len: int, borderType: int): int;
/**
 * The function [cv::calcCovarMatrix] calculates the covariance matrix and, optionally, the mean vector
 * of the set of input vectors.
 *
 * [PCA], [mulTransposed], [Mahalanobis]
 *
 * @param samples samples stored as separate matrices
 *
 * @param nsamples number of samples
 *
 * @param covar output covariance matrix of the type ctype and square size.
 *
 * @param mean input or output (depending on the flags) array as the average value of the input
 * vectors.
 *
 * @param flags operation flags as a combination of CovarFlags
 *
 * @param ctype type of the matrixl; it equals 'CV_64F' by default.
 */
export declare function calcCovarMatrix(samples: any, nsamples: int, covar: any, mean: any, flags: int, ctype?: int): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * use [COVAR_ROWS] or [COVAR_COLS] flag
 *
 * @param samples samples stored as rows/columns of a single matrix.
 *
 * @param covar output covariance matrix of the type ctype and square size.
 *
 * @param mean input or output (depending on the flags) array as the average value of the input
 * vectors.
 *
 * @param flags operation flags as a combination of CovarFlags
 *
 * @param ctype type of the matrixl; it equals 'CV_64F' by default.
 */
export declare function calcCovarMatrix(samples: Mat, covar: Mat, mean: Mat, flags: int, ctype?: int): void;
/**
 * The function [cv::cartToPolar] calculates either the magnitude, angle, or both for every 2D vector
 * (x(I),y(I)): `\\[\\begin{array}{l} \\texttt{magnitude} (I)=
 * \\sqrt{\\texttt{x}(I)^2+\\texttt{y}(I)^2} , \\\\ \\texttt{angle} (I)= \\texttt{atan2} ( \\texttt{y}
 * (I), \\texttt{x} (I))[ \\cdot180 / \\pi ] \\end{array}\\]`
 *
 * The angles are calculated with accuracy about 0.3 degrees. For the point (0,0), the angle is set to
 * 0.
 *
 * [Sobel], [Scharr]
 *
 * @param x array of x-coordinates; this must be a single-precision or double-precision floating-point
 * array.
 *
 * @param y array of y-coordinates, that must have the same size and same type as x.
 *
 * @param magnitude output array of magnitudes of the same size and type as x.
 *
 * @param angle output array of angles that has the same size and type as x; the angles are measured in
 * radians (from 0 to 2*Pi) or in degrees (0 to 360 degrees).
 *
 * @param angleInDegrees a flag, indicating whether the angles are measured in radians (which is by
 * default), or in degrees.
 */
export declare function cartToPolar(x: Mat, y: Mat, magnitude: Mat, angle: Mat, angleInDegrees?: bool): void;
/**
 * The function [cv::checkRange] checks that every array element is neither NaN nor infinite. When
 * minVal > -DBL_MAX and maxVal < DBL_MAX, the function also checks that each value is between minVal
 * and maxVal. In case of multi-channel arrays, each channel is processed independently. If some values
 * are out of range, position of the first outlier is stored in pos (when pos != NULL). Then, the
 * function either returns false (when quiet=true) or throws an exception.
 *
 * @param a input array.
 *
 * @param quiet a flag, indicating whether the functions quietly return false when the array elements
 * are out of range or they throw an exception.
 *
 * @param pos optional output parameter, when not NULL, must be a pointer to array of src.dims
 * elements.
 *
 * @param minVal inclusive lower boundary of valid values range.
 *
 * @param maxVal exclusive upper boundary of valid values range.
 */
export declare function checkRange(a: Mat, quiet?: bool, pos?: any, minVal?: double, maxVal?: double): bool;
/**
 * The function compares: Elements of two arrays when src1 and src2 have the same size:
 * `\\[\\texttt{dst} (I) = \\texttt{src1} (I) \\,\\texttt{cmpop}\\, \\texttt{src2} (I)\\]` Elements of
 * src1 with a scalar src2 when src2 is constructed from Scalar or has a single element:
 * `\\[\\texttt{dst} (I) = \\texttt{src1}(I) \\,\\texttt{cmpop}\\, \\texttt{src2}\\]` src1 with
 * elements of src2 when src1 is constructed from Scalar or has a single element: `\\[\\texttt{dst} (I)
 * = \\texttt{src1} \\,\\texttt{cmpop}\\, \\texttt{src2} (I)\\]` When the comparison result is true,
 * the corresponding element of output array is set to 255. The comparison operations can be replaced
 * with the equivalent matrix expressions:
 *
 * ```cpp
 * Mat dst1 = src1 >= src2;
 * Mat dst2 = src1 < 8;
 * ...
 * ```
 *
 * [checkRange], [min], [max], [threshold]
 *
 * @param src1 first input array or a scalar; when it is an array, it must have a single channel.
 *
 * @param src2 second input array or a scalar; when it is an array, it must have a single channel.
 *
 * @param dst output array of type ref CV_8U that has the same size and the same number of channels as
 * the input arrays.
 *
 * @param cmpop a flag, that specifies correspondence between the arrays (cv::CmpTypes)
 */
export declare function compare(src1: Mat, src2: Mat, dst: Mat, cmpop: int): void;
/**
 * The function [cv::completeSymm] copies the lower or the upper half of a square matrix to its another
 * half. The matrix diagonal remains unchanged:
 *
 * `$\\texttt{m}_{ij}=\\texttt{m}_{ji}$` for `$i > j$` if lowerToUpper=false
 * `$\\texttt{m}_{ij}=\\texttt{m}_{ji}$` for `$i < j$` if lowerToUpper=true
 *
 * [flip], [transpose]
 *
 * @param m input-output floating-point square matrix.
 *
 * @param lowerToUpper operation flag; if true, the lower half is copied to the upper half. Otherwise,
 * the upper half is copied to the lower half.
 */
export declare function completeSymm(m: Mat, lowerToUpper?: bool): void;
/**
 * This function converts FP32 (single precision floating point) from/to FP16 (half precision floating
 * point). CV_16S format is used to represent FP16 data. There are two use modes (src -> dst): CV_32F
 * -> CV_16S and CV_16S -> CV_32F. The input array has to have type of CV_32F or CV_16S to represent
 * the bit depth. If the input array is neither of them, the function will raise an error. The format
 * of half precision floating point is defined in IEEE 754-2008.
 *
 * @param src input array.
 *
 * @param dst output array.
 */
export declare function convertFp16(src: Mat, dst: Mat): void;
/**
 * On each element of the input array, the function convertScaleAbs performs three operations
 * sequentially: scaling, taking an absolute value, conversion to an unsigned 8-bit type:
 * `\\[\\texttt{dst} (I)= \\texttt{saturate\\_cast<uchar>} (| \\texttt{src} (I)* \\texttt{alpha} +
 * \\texttt{beta} |)\\]` In case of multi-channel arrays, the function processes each channel
 * independently. When the output is not 8-bit, the operation can be emulated by calling the
 * [Mat::convertTo] method (or by using matrix expressions) and then by calculating an absolute value
 * of the result. For example:
 *
 * ```cpp
 * Mat_<float> A(30,30);
 * randu(A, Scalar(-100), Scalar(100));
 * Mat_<float> B = A*5 + 3;
 * B = abs(B);
 * // Mat_<float> B = abs(A*5+3) will also do the job,
 * // but it will allocate a temporary matrix
 * ```
 *
 * [Mat::convertTo], cv::abs(const Mat&)
 *
 * @param src input array.
 *
 * @param dst output array.
 *
 * @param alpha optional scale factor.
 *
 * @param beta optional delta added to the scaled values.
 */
export declare function convertScaleAbs(src: Mat, dst: Mat, alpha?: double, beta?: double): void;
/**
 * The function copies the source image into the middle of the destination image. The areas to the
 * left, to the right, above and below the copied source image will be filled with extrapolated pixels.
 * This is not what filtering functions based on it do (they extrapolate pixels on-fly), but what other
 * more complex functions, including your own, may do to simplify image boundary handling.
 *
 * The function supports the mode when src is already in the middle of dst . In this case, the function
 * does not copy src itself but simply constructs the border, for example:
 *
 * ```cpp
 * // let border be the same in all directions
 * int border=2;
 * // constructs a larger image to fit both the image and the border
 * Mat gray_buf(rgb.rows + border*2, rgb.cols + border*2, rgb.depth());
 * // select the middle part of it w/o copying data
 * Mat gray(gray_canvas, Rect(border, border, rgb.cols, rgb.rows));
 * // convert image from RGB to grayscale
 * cvtColor(rgb, gray, COLOR_RGB2GRAY);
 * // form a border in-place
 * copyMakeBorder(gray, gray_buf, border, border,
 *                border, border, BORDER_REPLICATE);
 * // now do some custom filtering ...
 * ...
 * ```
 *
 * When the source image is a part (ROI) of a bigger image, the function will try to use the pixels
 * outside of the ROI to form a border. To disable this feature and always do extrapolation, as if src
 * was not a ROI, use borderType | [BORDER_ISOLATED].
 *
 * [borderInterpolate]
 *
 * @param src Source image.
 *
 * @param dst Destination image of the same type as src and the size Size(src.cols+left+right,
 * src.rows+top+bottom) .
 *
 * @param top the top pixels
 *
 * @param bottom the bottom pixels
 *
 * @param left the left pixels
 *
 * @param right Parameter specifying how many pixels in each direction from the source image rectangle
 * to extrapolate. For example, top=1, bottom=1, left=1, right=1 mean that 1 pixel-wide border needs to
 * be built.
 *
 * @param borderType Border type. See borderInterpolate for details.
 *
 * @param value Border value if borderType==BORDER_CONSTANT .
 */
export declare function copyMakeBorder(src: Mat, dst: Mat, top: int, bottom: int, left: int, right: int, borderType: int, value?: any): void;
/**
 * @param src source matrix.
 *
 * @param dst Destination matrix. If it does not have a proper size or type before the operation, it is
 * reallocated.
 *
 * @param mask Operation mask of the same size as *this. Its non-zero elements indicate which matrix
 * elements need to be copied. The mask has to be of type CV_8U and can have 1 or multiple channels.
 */
export declare function copyTo(src: Mat, dst: Mat, mask: Mat): void;
/**
 * The function returns the number of non-zero elements in src : `\\[\\sum _{I: \\; \\texttt{src} (I)
 * \\ne0 } 1\\]`
 *
 * [mean], [meanStdDev], [norm], [minMaxLoc], [calcCovarMatrix]
 *
 * @param src single-channel array.
 */
export declare function countNonZero(src: Mat): int;
/**
 * The function [cv::dct] performs a forward or inverse discrete Cosine transform (DCT) of a 1D or 2D
 * floating-point array:
 *
 * Forward Cosine transform of a 1D vector of N elements: `\\[Y = C^{(N)} \\cdot X\\]` where
 * `\\[C^{(N)}_{jk}= \\sqrt{\\alpha_j/N} \\cos \\left ( \\frac{\\pi(2k+1)j}{2N} \\right )\\]` and
 * `$\\alpha_0=1$`, `$\\alpha_j=2$` for *j > 0*.
 * Inverse Cosine transform of a 1D vector of N elements: `\\[X = \\left (C^{(N)} \\right )^{-1} \\cdot
 * Y = \\left (C^{(N)} \\right )^T \\cdot Y\\]` (since `$C^{(N)}$` is an orthogonal matrix, `$C^{(N)}
 * \\cdot \\left(C^{(N)}\\right)^T = I$` )
 * Forward 2D Cosine transform of M x N matrix: `\\[Y = C^{(N)} \\cdot X \\cdot \\left (C^{(N)} \\right
 * )^T\\]`
 * Inverse 2D Cosine transform of M x N matrix: `\\[X = \\left (C^{(N)} \\right )^T \\cdot X \\cdot
 * C^{(N)}\\]`
 *
 * The function chooses the mode of operation by looking at the flags and size of the input array:
 *
 * If (flags & [DCT_INVERSE]) == 0 , the function does a forward 1D or 2D transform. Otherwise, it is
 * an inverse 1D or 2D transform.
 * If (flags & [DCT_ROWS]) != 0 , the function performs a 1D transform of each row.
 * If the array is a single column or a single row, the function performs a 1D transform.
 * If none of the above is true, the function performs a 2D transform.
 *
 * Currently dct supports even-size arrays (2, 4, 6 ...). For data analysis and approximation, you can
 * pad the array when necessary. Also, the function performance depends very much, and not
 * monotonically, on the array size (see getOptimalDFTSize ). In the current implementation DCT of a
 * vector of size N is calculated via DFT of a vector of size N/2 . Thus, the optimal DCT size N1 >= N
 * can be calculated as:
 *
 * ```cpp
 * size_t getOptimalDCTSize(size_t N) { return 2*getOptimalDFTSize((N+1)/2); }
 * N1 = getOptimalDCTSize(N);
 * ```
 *
 * [dft] , [getOptimalDFTSize] , [idct]
 *
 * @param src input floating-point array.
 *
 * @param dst output array of the same size and type as src .
 *
 * @param flags transformation flags as a combination of cv::DftFlags (DCT_*)
 */
export declare function dct(src: Mat, dst: Mat, flags?: int): void;
/**
 * The function [cv::determinant] calculates and returns the determinant of the specified matrix. For
 * small matrices ( mtx.cols=mtx.rows<=3 ), the direct method is used. For larger matrices, the
 * function uses LU factorization with partial pivoting.
 *
 * For symmetric positively-determined matrices, it is also possible to use eigen decomposition to
 * calculate the determinant.
 *
 * [trace], [invert], [solve], [eigen], [MatrixExpressions]
 *
 * @param mtx input matrix that must have CV_32FC1 or CV_64FC1 type and square size.
 */
export declare function determinant(mtx: Mat): double;
/**
 * The function [cv::dft] performs one of the following:
 *
 * Forward the Fourier transform of a 1D vector of N elements: `\\[Y = F^{(N)} \\cdot X,\\]` where
 * `$F^{(N)}_{jk}=\\exp(-2\\pi i j k/N)$` and `$i=\\sqrt{-1}$`
 * Inverse the Fourier transform of a 1D vector of N elements: `\\[\\begin{array}{l} X'= \\left
 * (F^{(N)} \\right )^{-1} \\cdot Y = \\left (F^{(N)} \\right )^* \\cdot y \\\\ X = (1/N) \\cdot X,
 * \\end{array}\\]` where `$F^*=\\left(\\textrm{Re}(F^{(N)})-\\textrm{Im}(F^{(N)})\\right)^T$`
 * Forward the 2D Fourier transform of a M x N matrix: `\\[Y = F^{(M)} \\cdot X \\cdot F^{(N)}\\]`
 * Inverse the 2D Fourier transform of a M x N matrix: `\\[\\begin{array}{l} X'= \\left (F^{(M)}
 * \\right )^* \\cdot Y \\cdot \\left (F^{(N)} \\right )^* \\\\ X = \\frac{1}{M \\cdot N} \\cdot X'
 * \\end{array}\\]`
 *
 * In case of real (single-channel) data, the output spectrum of the forward Fourier transform or input
 * spectrum of the inverse Fourier transform can be represented in a packed format called *CCS*
 * (complex-conjugate-symmetrical). It was borrowed from IPL (Intel* Image Processing Library). Here is
 * how 2D *CCS* spectrum looks: `\\[\\begin{bmatrix} Re Y_{0,0} & Re Y_{0,1} & Im Y_{0,1} & Re Y_{0,2}
 * & Im Y_{0,2} & \\cdots & Re Y_{0,N/2-1} & Im Y_{0,N/2-1} & Re Y_{0,N/2} \\\\ Re Y_{1,0} & Re Y_{1,1}
 * & Im Y_{1,1} & Re Y_{1,2} & Im Y_{1,2} & \\cdots & Re Y_{1,N/2-1} & Im Y_{1,N/2-1} & Re Y_{1,N/2}
 * \\\\ Im Y_{1,0} & Re Y_{2,1} & Im Y_{2,1} & Re Y_{2,2} & Im Y_{2,2} & \\cdots & Re Y_{2,N/2-1} & Im
 * Y_{2,N/2-1} & Im Y_{1,N/2} \\\\ \\hdotsfor{9} \\\\ Re Y_{M/2-1,0} & Re Y_{M-3,1} & Im Y_{M-3,1} &
 * \\hdotsfor{3} & Re Y_{M-3,N/2-1} & Im Y_{M-3,N/2-1}& Re Y_{M/2-1,N/2} \\\\ Im Y_{M/2-1,0} & Re
 * Y_{M-2,1} & Im Y_{M-2,1} & \\hdotsfor{3} & Re Y_{M-2,N/2-1} & Im Y_{M-2,N/2-1}& Im Y_{M/2-1,N/2}
 * \\\\ Re Y_{M/2,0} & Re Y_{M-1,1} & Im Y_{M-1,1} & \\hdotsfor{3} & Re Y_{M-1,N/2-1} & Im
 * Y_{M-1,N/2-1}& Re Y_{M/2,N/2} \\end{bmatrix}\\]`
 *
 * In case of 1D transform of a real vector, the output looks like the first row of the matrix above.
 *
 * So, the function chooses an operation mode depending on the flags and size of the input array:
 *
 * If [DFT_ROWS] is set or the input array has a single row or single column, the function performs a
 * 1D forward or inverse transform of each row of a matrix when [DFT_ROWS] is set. Otherwise, it
 * performs a 2D transform.
 * If the input array is real and [DFT_INVERSE] is not set, the function performs a forward 1D or 2D
 * transform:
 *
 * When [DFT_COMPLEX_OUTPUT] is set, the output is a complex matrix of the same size as input.
 * When [DFT_COMPLEX_OUTPUT] is not set, the output is a real matrix of the same size as input. In case
 * of 2D transform, it uses the packed format as shown above. In case of a single 1D transform, it
 * looks like the first row of the matrix above. In case of multiple 1D transforms (when using the
 * [DFT_ROWS] flag), each row of the output matrix looks like the first row of the matrix above.
 *
 * If the input array is complex and either [DFT_INVERSE] or [DFT_REAL_OUTPUT] are not set, the output
 * is a complex array of the same size as input. The function performs a forward or inverse 1D or 2D
 * transform of the whole input array or each row of the input array independently, depending on the
 * flags DFT_INVERSE and DFT_ROWS.
 * When [DFT_INVERSE] is set and the input array is real, or it is complex but [DFT_REAL_OUTPUT] is
 * set, the output is a real array of the same size as input. The function performs a 1D or 2D inverse
 * transformation of the whole input array or each individual row, depending on the flags [DFT_INVERSE]
 * and [DFT_ROWS].
 *
 * If [DFT_SCALE] is set, the scaling is done after the transformation.
 *
 * Unlike dct , the function supports arrays of arbitrary size. But only those arrays are processed
 * efficiently, whose sizes can be factorized in a product of small prime numbers (2, 3, and 5 in the
 * current implementation). Such an efficient DFT size can be calculated using the getOptimalDFTSize
 * method.
 *
 * The sample below illustrates how to calculate a DFT-based convolution of two 2D real arrays:
 *
 * ```cpp
 * void convolveDFT(InputArray A, InputArray B, OutputArray C)
 * {
 *     // reallocate the output array if needed
 *     C.create(abs(A.rows - B.rows)+1, abs(A.cols - B.cols)+1, A.type());
 *     Size dftSize;
 *     // calculate the size of DFT transform
 *     dftSize.width = getOptimalDFTSize(A.cols + B.cols - 1);
 *     dftSize.height = getOptimalDFTSize(A.rows + B.rows - 1);
 *
 *     // allocate temporary buffers and initialize them with 0's
 *     Mat tempA(dftSize, A.type(), Scalar::all(0));
 *     Mat tempB(dftSize, B.type(), Scalar::all(0));
 *
 *     // copy A and B to the top-left corners of tempA and tempB, respectively
 *     Mat roiA(tempA, Rect(0,0,A.cols,A.rows));
 *     A.copyTo(roiA);
 *     Mat roiB(tempB, Rect(0,0,B.cols,B.rows));
 *     B.copyTo(roiB);
 *
 *     // now transform the padded A & B in-place;
 *     // use "nonzeroRows" hint for faster processing
 *     dft(tempA, tempA, 0, A.rows);
 *     dft(tempB, tempB, 0, B.rows);
 *
 *     // multiply the spectrums;
 *     // the function handles packed spectrum representations well
 *     mulSpectrums(tempA, tempB, tempA);
 *
 *     // transform the product back from the frequency domain.
 *     // Even though all the result rows will be non-zero,
 *     // you need only the first C.rows of them, and thus you
 *     // pass nonzeroRows == C.rows
 *     dft(tempA, tempA, DFT_INVERSE + DFT_SCALE, C.rows);
 *
 *     // now copy the result back to C.
 *     tempA(Rect(0, 0, C.cols, C.rows)).copyTo(C);
 *
 *     // all the temporary buffers will be deallocated automatically
 * }
 * ```
 *
 *  To optimize this sample, consider the following approaches:
 *
 * Since nonzeroRows != 0 is passed to the forward transform calls and since A and B are copied to the
 * top-left corners of tempA and tempB, respectively, it is not necessary to clear the whole tempA and
 * tempB. It is only necessary to clear the tempA.cols - A.cols ( tempB.cols - B.cols) rightmost
 * columns of the matrices.
 * This DFT-based convolution does not have to be applied to the whole big arrays, especially if B is
 * significantly smaller than A or vice versa. Instead, you can calculate convolution by parts. To do
 * this, you need to split the output array C into multiple tiles. For each tile, estimate which parts
 * of A and B are required to calculate convolution in this tile. If the tiles in C are too small, the
 * speed will decrease a lot because of repeated work. In the ultimate case, when each tile in C is a
 * single pixel, the algorithm becomes equivalent to the naive convolution algorithm. If the tiles are
 * too big, the temporary arrays tempA and tempB become too big and there is also a slowdown because of
 * bad cache locality. So, there is an optimal tile size somewhere in the middle.
 * If different tiles in C can be calculated in parallel and, thus, the convolution is done by parts,
 * the loop can be threaded.
 *
 * All of the above improvements have been implemented in [matchTemplate] and [filter2D] . Therefore,
 * by using them, you can get the performance even better than with the above theoretically optimal
 * implementation. Though, those two functions actually calculate cross-correlation, not convolution,
 * so you need to "flip" the second convolution operand B vertically and horizontally using flip .
 *
 * An example using the discrete fourier transform can be found at
 * opencv_source_code/samples/cpp/dft.cpp
 * (Python) An example using the dft functionality to perform Wiener deconvolution can be found at
 * opencv_source/samples/python/deconvolution.py
 * (Python) An example rearranging the quadrants of a Fourier image can be found at
 * opencv_source/samples/python/dft.py
 *
 * [dct] , [getOptimalDFTSize] , [mulSpectrums], [filter2D] , [matchTemplate] , [flip] , [cartToPolar]
 * , [magnitude] , [phase]
 *
 * @param src input array that could be real or complex.
 *
 * @param dst output array whose size and type depends on the flags .
 *
 * @param flags transformation flags, representing a combination of the DftFlags
 *
 * @param nonzeroRows when the parameter is not zero, the function assumes that only the first
 * nonzeroRows rows of the input array (DFT_INVERSE is not set) or only the first nonzeroRows of the
 * output array (DFT_INVERSE is set) contain non-zeros, thus, the function can handle the rest of the
 * rows more efficiently and save some time; this technique is very useful for calculating array
 * cross-correlation or convolution using DFT.
 */
export declare function dft(src: Mat, dst: Mat, flags?: int, nonzeroRows?: int): void;
/**
 * The function [cv::divide] divides one array by another: `\\[\\texttt{dst(I) =
 * saturate(src1(I)*scale/src2(I))}\\]` or a scalar by an array when there is no src1 :
 * `\\[\\texttt{dst(I) = saturate(scale/src2(I))}\\]`
 *
 * Different channels of multi-channel arrays are processed independently.
 *
 * For integer types when src2(I) is zero, dst(I) will also be zero.
 *
 * In case of floating point data there is no special defined behavior for zero src2(I) values. Regular
 * floating-point division is used. Expect correct IEEE-754 behaviour for floating-point data (with
 * NaN, Inf result values).
 *
 * Saturation is not applied when the output array has the depth CV_32S. You may even get result of an
 * incorrect sign in the case of overflow.
 *
 * [multiply], [add], [subtract]
 *
 * @param src1 first input array.
 *
 * @param src2 second input array of the same size and type as src1.
 *
 * @param dst output array of the same size and type as src2.
 *
 * @param scale scalar factor.
 *
 * @param dtype optional depth of the output array; if -1, dst will have depth src2.depth(), but in
 * case of an array-by-array division, you can only pass -1 when src1.depth()==src2.depth().
 */
export declare function divide(src1: Mat, src2: Mat, dst: Mat, scale?: double, dtype?: int): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function divide(scale: double, src2: Mat, dst: Mat, dtype?: int): void;
/**
 * The function [cv::eigen] calculates just eigenvalues, or eigenvalues and eigenvectors of the
 * symmetric matrix src:
 *
 * ```cpp
 * src*eigenvectors.row(i).t() = eigenvalues.at<srcType>(i)*eigenvectors.row(i).t()
 * ```
 *
 * Use [cv::eigenNonSymmetric] for calculation of real eigenvalues and eigenvectors of non-symmetric
 * matrix.
 *
 * [eigenNonSymmetric], [completeSymm] , [PCA]
 *
 * @param src input matrix that must have CV_32FC1 or CV_64FC1 type, square size and be symmetrical
 * (src ^T^ == src).
 *
 * @param eigenvalues output vector of eigenvalues of the same type as src; the eigenvalues are stored
 * in the descending order.
 *
 * @param eigenvectors output matrix of eigenvectors; it has the same size and type as src; the
 * eigenvectors are stored as subsequent matrix rows, in the same order as the corresponding
 * eigenvalues.
 */
export declare function eigen(src: Mat, eigenvalues: Mat, eigenvectors?: Mat): bool;
/**
 * Assumes real eigenvalues.
 * The function calculates eigenvalues and eigenvectors (optional) of the square matrix src:
 *
 * ```cpp
 * src*eigenvectors.row(i).t() = eigenvalues.at<srcType>(i)*eigenvectors.row(i).t()
 * ```
 *
 * [eigen]
 *
 * @param src input matrix (CV_32FC1 or CV_64FC1 type).
 *
 * @param eigenvalues output vector of eigenvalues (type is the same type as src).
 *
 * @param eigenvectors output matrix of eigenvectors (type is the same type as src). The eigenvectors
 * are stored as subsequent matrix rows, in the same order as the corresponding eigenvalues.
 */
export declare function eigenNonSymmetric(src: Mat, eigenvalues: Mat, eigenvectors: Mat): void;
/**
 * The function [cv::exp] calculates the exponent of every element of the input array:
 * `\\[\\texttt{dst} [I] = e^{ src(I) }\\]`
 *
 * The maximum relative error is about 7e-6 for single-precision input and less than 1e-10 for
 * double-precision input. Currently, the function converts denormalized values to zeros on output.
 * Special values (NaN, Inf) are not handled.
 *
 * [log] , [cartToPolar] , [polarToCart] , [phase] , [pow] , [sqrt] , [magnitude]
 *
 * @param src input array.
 *
 * @param dst output array of the same size and type as src.
 */
export declare function exp(src: Mat, dst: Mat): void;
/**
 * [mixChannels], [split]
 *
 * @param src input array
 *
 * @param dst output array
 *
 * @param coi index of channel to extract
 */
export declare function extractChannel(src: Mat, dst: Mat, coi: int): void;
/**
 * Given a binary matrix (likely returned from an operation such as [threshold()], [compare()], >, ==,
 * etc, return all of the non-zero indices as a [cv::Mat] or std::vector<cv::Point> (x,y) For example:
 *
 * ```cpp
 * cv::Mat binaryImage; // input, binary image
 * cv::Mat locations;   // output, locations of non-zero pixels
 * cv::findNonZero(binaryImage, locations);
 *
 * // access pixel coordinates
 * Point pnt = locations.at<Point>(i);
 * ```
 *
 *  or
 *
 * ```cpp
 * cv::Mat binaryImage; // input, binary image
 * vector<Point> locations;   // output, locations of non-zero pixels
 * cv::findNonZero(binaryImage, locations);
 *
 * // access pixel coordinates
 * Point pnt = locations[i];
 * ```
 *
 * @param src single-channel array
 *
 * @param idx the output array, type of cv::Mat or std::vector<Point>, corresponding to non-zero
 * indices in the input
 */
export declare function findNonZero(src: Mat, idx: Mat): void;
/**
 * The function [cv::flip] flips the array in one of three different ways (row and column indices are
 * 0-based): `\\[\\texttt{dst} _{ij} = \\left\\{ \\begin{array}{l l} \\texttt{src}
 * _{\\texttt{src.rows}-i-1,j} & if\\; \\texttt{flipCode} = 0 \\\\ \\texttt{src} _{i,
 * \\texttt{src.cols} -j-1} & if\\; \\texttt{flipCode} > 0 \\\\ \\texttt{src} _{ \\texttt{src.rows}
 * -i-1, \\texttt{src.cols} -j-1} & if\\; \\texttt{flipCode} < 0 \\\\ \\end{array} \\right.\\]` The
 * example scenarios of using the function are the following: Vertical flipping of the image (flipCode
 * == 0) to switch between top-left and bottom-left image origin. This is a typical operation in video
 * processing on Microsoft Windows* OS. Horizontal flipping of the image with the subsequent horizontal
 * shift and absolute difference calculation to check for a vertical-axis symmetry (flipCode > 0).
 * Simultaneous horizontal and vertical flipping of the image with the subsequent shift and absolute
 * difference calculation to check for a central symmetry (flipCode < 0). Reversing the order of point
 * arrays (flipCode > 0 or flipCode == 0).
 *
 * [transpose] , [repeat] , [completeSymm]
 *
 * @param src input array.
 *
 * @param dst output array of the same size and type as src.
 *
 * @param flipCode a flag to specify how to flip the array; 0 means flipping around the x-axis and
 * positive value (for example, 1) means flipping around y-axis. Negative value (for example, -1) means
 * flipping around both axes.
 */
export declare function flip(src: Mat, dst: Mat, flipCode: int): void;
/**
 * The function [cv::gemm] performs generalized matrix multiplication similar to the gemm functions in
 * BLAS level 3. For example, `gemm(src1, src2, alpha, src3, beta, dst, GEMM_1_T + GEMM_3_T)`
 * corresponds to `\\[\\texttt{dst} = \\texttt{alpha} \\cdot \\texttt{src1} ^T \\cdot \\texttt{src2} +
 * \\texttt{beta} \\cdot \\texttt{src3} ^T\\]`
 *
 * In case of complex (two-channel) data, performed a complex matrix multiplication.
 *
 * The function can be replaced with a matrix expression. For example, the above call can be replaced
 * with:
 *
 * ```cpp
 * dst = alpha*src1.t()*src2 + beta*src3.t();
 * ```
 *
 * [mulTransposed] , [transform]
 *
 * @param src1 first multiplied input matrix that could be real(CV_32FC1, CV_64FC1) or
 * complex(CV_32FC2, CV_64FC2).
 *
 * @param src2 second multiplied input matrix of the same type as src1.
 *
 * @param alpha weight of the matrix product.
 *
 * @param src3 third optional delta matrix added to the matrix product; it should have the same type as
 * src1 and src2.
 *
 * @param beta weight of src3.
 *
 * @param dst output matrix; it has the proper size and the same type as input matrices.
 *
 * @param flags operation flags (cv::GemmFlags)
 */
export declare function gemm(src1: Mat, src2: Mat, alpha: double, src3: Mat, beta: double, dst: Mat, flags?: int): void;
/**
 * DFT performance is not a monotonic function of a vector size. Therefore, when you calculate
 * convolution of two arrays or perform the spectral analysis of an array, it usually makes sense to
 * pad the input data with zeros to get a bit larger array that can be transformed much faster than the
 * original one. Arrays whose size is a power-of-two (2, 4, 8, 16, 32, ...) are the fastest to process.
 * Though, the arrays whose size is a product of 2's, 3's, and 5's (for example, 300 = 5*5*3*2*2) are
 * also processed quite efficiently.
 *
 * The function [cv::getOptimalDFTSize] returns the minimum number N that is greater than or equal to
 * vecsize so that the DFT of a vector of size N can be processed efficiently. In the current
 * implementation N = 2 ^p^ * 3 ^q^ * 5 ^r^ for some integer p, q, r.
 *
 * The function returns a negative number if vecsize is too large (very close to INT_MAX ).
 *
 * While the function cannot be used directly to estimate the optimal vector size for DCT transform
 * (since the current DCT implementation supports only even-size vectors), it can be easily processed
 * as getOptimalDFTSize((vecsize+1)/2)*2.
 *
 * [dft] , [dct] , [idft] , [idct] , [mulSpectrums]
 *
 * @param vecsize vector size.
 */
export declare function getOptimalDFTSize(vecsize: int): int;
/**
 * The function horizontally concatenates two or more [cv::Mat] matrices (with the same number of
 * rows).
 *
 * ```cpp
 * cv::Mat matArray[] = { cv::Mat(4, 1, CV_8UC1, cv::Scalar(1)),
 *                        cv::Mat(4, 1, CV_8UC1, cv::Scalar(2)),
 *                        cv::Mat(4, 1, CV_8UC1, cv::Scalar(3)),};
 *
 * cv::Mat out;
 * cv::hconcat( matArray, 3, out );
 * //out:
 * //[1, 2, 3;
 * // 1, 2, 3;
 * // 1, 2, 3;
 * // 1, 2, 3]
 * ```
 *
 * [cv::vconcat(const Mat*, size_t, OutputArray)],
 *
 * [cv::vconcat(InputArrayOfArrays, OutputArray)] and
 *
 * [cv::vconcat(InputArray, InputArray, OutputArray)]
 *
 * @param src input array or vector of matrices. all of the matrices must have the same number of rows
 * and the same depth.
 *
 * @param nsrc number of matrices in src.
 *
 * @param dst output array. It has the same number of rows and depth as the src, and the sum of cols of
 * the src.
 */
export declare function hconcat(src: any, nsrc: size_t, dst: Mat): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * ```cpp
 * cv::Mat_<float> A = (cv::Mat_<float>(3, 2) << 1, 4,
 *                                               2, 5,
 *                                               3, 6);
 * cv::Mat_<float> B = (cv::Mat_<float>(3, 2) << 7, 10,
 *                                               8, 11,
 *                                               9, 12);
 *
 * cv::Mat C;
 * cv::hconcat(A, B, C);
 * //C:
 * //[1, 4, 7, 10;
 * // 2, 5, 8, 11;
 * // 3, 6, 9, 12]
 * ```
 *
 * @param src1 first input array to be considered for horizontal concatenation.
 *
 * @param src2 second input array to be considered for horizontal concatenation.
 *
 * @param dst output array. It has the same number of rows and depth as the src1 and src2, and the sum
 * of cols of the src1 and src2.
 */
export declare function hconcat(src1: Mat, src2: Mat, dst: Mat): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * ```cpp
 * std::vector<cv::Mat> matrices = { cv::Mat(4, 1, CV_8UC1, cv::Scalar(1)),
 *                                   cv::Mat(4, 1, CV_8UC1, cv::Scalar(2)),
 *                                   cv::Mat(4, 1, CV_8UC1, cv::Scalar(3)),};
 *
 * cv::Mat out;
 * cv::hconcat( matrices, out );
 * //out:
 * //[1, 2, 3;
 * // 1, 2, 3;
 * // 1, 2, 3;
 * // 1, 2, 3]
 * ```
 *
 * @param src input array or vector of matrices. all of the matrices must have the same number of rows
 * and the same depth.
 *
 * @param dst output array. It has the same number of rows and depth as the src, and the sum of cols of
 * the src. same depth.
 */
export declare function hconcat(src: MatVector, dst: Mat): void;
/**
 * idct(src, dst, flags) is equivalent to dct(src, dst, flags | DCT_INVERSE).
 *
 * [dct], [dft], [idft], [getOptimalDFTSize]
 *
 * @param src input floating-point single-channel array.
 *
 * @param dst output array of the same size and type as src.
 *
 * @param flags operation flags.
 */
export declare function idct(src: Mat, dst: Mat, flags?: int): void;
/**
 * idft(src, dst, flags) is equivalent to dft(src, dst, flags | [DFT_INVERSE]) .
 *
 * None of dft and idft scales the result by default. So, you should pass [DFT_SCALE] to one of dft or
 * idft explicitly to make these transforms mutually inverse.
 *
 * [dft], [dct], [idct], [mulSpectrums], [getOptimalDFTSize]
 *
 * @param src input floating-point real or complex array.
 *
 * @param dst output array whose size and type depend on the flags.
 *
 * @param flags operation flags (see dft and DftFlags).
 *
 * @param nonzeroRows number of dst rows to process; the rest of the rows have undefined content (see
 * the convolution sample in dft description.
 */
export declare function idft(src: Mat, dst: Mat, flags?: int, nonzeroRows?: int): void;
/**
 * The function checks the range as follows:
 *
 * For every element of a single-channel input array: `\\[\\texttt{dst} (I)= \\texttt{lowerb} (I)_0
 * \\leq \\texttt{src} (I)_0 \\leq \\texttt{upperb} (I)_0\\]`
 * For two-channel arrays: `\\[\\texttt{dst} (I)= \\texttt{lowerb} (I)_0 \\leq \\texttt{src} (I)_0
 * \\leq \\texttt{upperb} (I)_0 \\land \\texttt{lowerb} (I)_1 \\leq \\texttt{src} (I)_1 \\leq
 * \\texttt{upperb} (I)_1\\]`
 * and so forth.
 *
 * That is, dst (I) is set to 255 (all 1 -bits) if src (I) is within the specified 1D, 2D, 3D, ... box
 * and 0 otherwise.
 *
 * When the lower and/or upper boundary parameters are scalars, the indexes (I) at lowerb and upperb in
 * the above formulas should be omitted.
 *
 * @param src first input array.
 *
 * @param lowerb inclusive lower boundary array or a scalar.
 *
 * @param upperb inclusive upper boundary array or a scalar.
 *
 * @param dst output array of the same size as src and CV_8U type.
 */
export declare function inRange(src: Mat, lowerb: Mat, upperb: Mat, dst: Mat): void;
/**
 * [mixChannels], [merge]
 *
 * @param src input array
 *
 * @param dst output array
 *
 * @param coi index of channel for insertion
 */
export declare function insertChannel(src: Mat, dst: Mat, coi: int): void;
/**
 * The function [cv::invert] inverts the matrix src and stores the result in dst . When the matrix src
 * is singular or non-square, the function calculates the pseudo-inverse matrix (the dst matrix) so
 * that norm(src*dst - I) is minimal, where I is an identity matrix.
 *
 * In case of the [DECOMP_LU] method, the function returns non-zero value if the inverse has been
 * successfully calculated and 0 if src is singular.
 *
 * In case of the [DECOMP_SVD] method, the function returns the inverse condition number of src (the
 * ratio of the smallest singular value to the largest singular value) and 0 if src is singular. The
 * [SVD] method calculates a pseudo-inverse matrix if src is singular.
 *
 * Similarly to [DECOMP_LU], the method [DECOMP_CHOLESKY] works only with non-singular square matrices
 * that should also be symmetrical and positively defined. In this case, the function stores the
 * inverted matrix in dst and returns non-zero. Otherwise, it returns 0.
 *
 * [solve], [SVD]
 *
 * @param src input floating-point M x N matrix.
 *
 * @param dst output matrix of N x M size and the same type as src.
 *
 * @param flags inversion method (cv::DecompTypes)
 */
export declare function invert(src: Mat, dst: Mat, flags?: int): double;
/**
 * The function [cv::log] calculates the natural logarithm of every element of the input array:
 * `\\[\\texttt{dst} (I) = \\log (\\texttt{src}(I)) \\]`
 *
 * Output on zero, negative and special (NaN, Inf) values is undefined.
 *
 * [exp], [cartToPolar], [polarToCart], [phase], [pow], [sqrt], [magnitude]
 *
 * @param src input array.
 *
 * @param dst output array of the same size and type as src .
 */
export declare function log(src: Mat, dst: Mat): void;
/**
 * The function LUT fills the output array with values from the look-up table. Indices of the entries
 * are taken from the input array. That is, the function processes each element of src as follows:
 * `\\[\\texttt{dst} (I) \\leftarrow \\texttt{lut(src(I) + d)}\\]` where `\\[d = \\fork{0}{if
 * \\(\\texttt{src}\\) has depth \\(\\texttt{CV_8U}\\)}{128}{if \\(\\texttt{src}\\) has depth
 * \\(\\texttt{CV_8S}\\)}\\]`
 *
 * [convertScaleAbs], [Mat::convertTo]
 *
 * @param src input array of 8-bit elements.
 *
 * @param lut look-up table of 256 elements; in case of multi-channel input array, the table should
 * either have a single channel (in this case the same table is used for all channels) or the same
 * number of channels as in the input array.
 *
 * @param dst output array of the same size and number of channels as src, and the same depth as lut.
 */
export declare function LUT(src: Mat, lut: Mat, dst: Mat): void;
/**
 * The function [cv::magnitude] calculates the magnitude of 2D vectors formed from the corresponding
 * elements of x and y arrays: `\\[\\texttt{dst} (I) = \\sqrt{\\texttt{x}(I)^2 + \\texttt{y}(I)^2}\\]`
 *
 * [cartToPolar], [polarToCart], [phase], [sqrt]
 *
 * @param x floating-point array of x-coordinates of the vectors.
 *
 * @param y floating-point array of y-coordinates of the vectors; it must have the same size as x.
 *
 * @param magnitude output array of the same size and type as x.
 */
export declare function magnitude(x: Mat, y: Mat, magnitude: Mat): void;
/**
 * The function [cv::Mahalanobis] calculates and returns the weighted distance between two vectors:
 * `\\[d( \\texttt{vec1} , \\texttt{vec2} )=
 * \\sqrt{\\sum_{i,j}{\\texttt{icovar(i,j)}\\cdot(\\texttt{vec1}(I)-\\texttt{vec2}(I))\\cdot(\\texttt{vec1(j)}-\\texttt{vec2(j)})}
 * }\\]` The covariance matrix may be calculated using the [calcCovarMatrix] function and then inverted
 * using the invert function (preferably using the [DECOMP_SVD] method, as the most accurate).
 *
 * @param v1 first 1D input vector.
 *
 * @param v2 second 1D input vector.
 *
 * @param icovar inverse covariance matrix.
 */
export declare function Mahalanobis(v1: Mat, v2: Mat, icovar: Mat): double;
/**
 * The function [cv::max] calculates the per-element maximum of two arrays: `\\[\\texttt{dst} (I)=
 * \\max ( \\texttt{src1} (I), \\texttt{src2} (I))\\]` or array and a scalar: `\\[\\texttt{dst} (I)=
 * \\max ( \\texttt{src1} (I), \\texttt{value} )\\]`
 *
 * [min], [compare], [inRange], [minMaxLoc], [MatrixExpressions]
 *
 * @param src1 first input array.
 *
 * @param src2 second input array of the same size and type as src1 .
 *
 * @param dst output array of the same size and type as src1.
 */
export declare function max(src1: Mat, src2: Mat, dst: Mat): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts. needed to avoid conflicts with const _Tp& std::min(const _Tp&,
 * const _Tp&, _Compare)
 */
export declare function max(src1: any, src2: any, dst: any): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts. needed to avoid conflicts with const _Tp& std::min(const _Tp&,
 * const _Tp&, _Compare)
 */
export declare function max(src1: any, src2: any, dst: any): void;
/**
 * The function [cv::mean] calculates the mean value M of array elements, independently for each
 * channel, and return it: `\\[\\begin{array}{l} N = \\sum _{I: \\; \\texttt{mask} (I) \\ne 0} 1 \\\\
 * M_c = \\left ( \\sum _{I: \\; \\texttt{mask} (I) \\ne 0}{ \\texttt{mtx} (I)_c} \\right )/N
 * \\end{array}\\]` When all the mask elements are 0's, the function returns Scalar::all(0)
 *
 * [countNonZero], [meanStdDev], [norm], [minMaxLoc]
 *
 * @param src input array that should have from 1 to 4 channels so that the result can be stored in
 * Scalar_ .
 *
 * @param mask optional operation mask.
 */
export declare function mean(src: Mat, mask?: Mat): Scalar;
/**
 * Calculates a mean and standard deviation of array elements.
 *
 * The function [cv::meanStdDev] calculates the mean and the standard deviation M of array elements
 * independently for each channel and returns it via the output parameters: `\\[\\begin{array}{l} N =
 * \\sum _{I, \\texttt{mask} (I) \\ne 0} 1 \\\\ \\texttt{mean} _c = \\frac{\\sum_{ I: \\;
 * \\texttt{mask}(I) \\ne 0} \\texttt{src} (I)_c}{N} \\\\ \\texttt{stddev} _c = \\sqrt{\\frac{\\sum_{
 * I: \\; \\texttt{mask}(I) \\ne 0} \\left ( \\texttt{src} (I)_c - \\texttt{mean} _c \\right )^2}{N}}
 * \\end{array}\\]` When all the mask elements are 0's, the function returns
 * mean=stddev=Scalar::all(0).
 *
 * The calculated standard deviation is only the diagonal of the complete normalized covariance matrix.
 * If the full matrix is needed, you can reshape the multi-channel array M x N to the single-channel
 * array M*N x mtx.channels() (only possible when the matrix is continuous) and then pass the matrix to
 * calcCovarMatrix .
 *
 * [countNonZero], [mean], [norm], [minMaxLoc], [calcCovarMatrix]
 *
 * @param src input array that should have from 1 to 4 channels so that the results can be stored in
 * Scalar_ 's.
 *
 * @param mean output parameter: calculated mean value.
 *
 * @param stddev output parameter: calculated standard deviation.
 *
 * @param mask optional operation mask.
 */
export declare function meanStdDev(src: Mat, mean: Mat, stddev: Mat, mask?: Mat): void;
/**
 * The function [cv::merge] merges several arrays to make a single multi-channel array. That is, each
 * element of the output array will be a concatenation of the elements of the input arrays, where
 * elements of i-th input array are treated as mv[i].channels()-element vectors.
 *
 * The function [cv::split] does the reverse operation. If you need to shuffle channels in some other
 * advanced way, use [cv::mixChannels].
 *
 * The following example shows how to merge 3 single channel matrices into a single 3-channel matrix.
 *
 * ```cpp
 *     Mat m1 = (Mat_<uchar>(2,2) << 1,4,7,10);
 *     Mat m2 = (Mat_<uchar>(2,2) << 2,5,8,11);
 *     Mat m3 = (Mat_<uchar>(2,2) << 3,6,9,12);
 *
 *     Mat channels[3] = {m1, m2, m3};
 *     Mat m;
 *     merge(channels, 3, m);
 *     /*
 *     m =
 *     [  1,   2,   3,   4,   5,   6;
 *        7,   8,   9,  10,  11,  12]
 *     m.channels() = 3
 * \/
 * ```
 *
 * [mixChannels], [split], [Mat::reshape]
 *
 * @param mv input array of matrices to be merged; all the matrices in mv must have the same size and
 * the same depth.
 *
 * @param count number of input matrices when mv is a plain C array; it must be greater than zero.
 *
 * @param dst output array of the same size and the same depth as mv[0]; The number of channels will be
 * equal to the parameter count.
 */
export declare function merge(mv: any, count: size_t, dst: Mat): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param mv input vector of matrices to be merged; all the matrices in mv must have the same size and
 * the same depth.
 *
 * @param dst output array of the same size and the same depth as mv[0]; The number of channels will be
 * the total number of channels in the matrix array.
 */
export declare function merge(mv: MatVector, dst: Mat): void;
/**
 * The function [cv::min] calculates the per-element minimum of two arrays: `\\[\\texttt{dst} (I)=
 * \\min ( \\texttt{src1} (I), \\texttt{src2} (I))\\]` or array and a scalar: `\\[\\texttt{dst} (I)=
 * \\min ( \\texttt{src1} (I), \\texttt{value} )\\]`
 *
 * [max], [compare], [inRange], [minMaxLoc]
 *
 * @param src1 first input array.
 *
 * @param src2 second input array of the same size and type as src1.
 *
 * @param dst output array of the same size and type as src1.
 */
export declare function min(src1: Mat, src2: Mat, dst: Mat): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts. needed to avoid conflicts with const _Tp& std::min(const _Tp&,
 * const _Tp&, _Compare)
 */
export declare function min(src1: any, src2: any, dst: any): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts. needed to avoid conflicts with const _Tp& std::min(const _Tp&,
 * const _Tp&, _Compare)
 */
export declare function min(src1: any, src2: any, dst: any): void;
/**
 * The function [cv::minMaxIdx] finds the minimum and maximum element values and their positions. The
 * extremums are searched across the whole array or, if mask is not an empty array, in the specified
 * array region. The function does not work with multi-channel arrays. If you need to find minimum or
 * maximum elements across all the channels, use [Mat::reshape] first to reinterpret the array as
 * single-channel. Or you may extract the particular channel using either extractImageCOI , or
 * mixChannels , or split . In case of a sparse matrix, the minimum is found among non-zero elements
 * only.
 *
 * When minIdx is not NULL, it must have at least 2 elements (as well as maxIdx), even if src is a
 * single-row or single-column matrix. In OpenCV (following MATLAB) each array has at least 2
 * dimensions, i.e. single-column matrix is Mx1 matrix (and therefore minIdx/maxIdx will be
 * (i1,0)/(i2,0)) and single-row matrix is 1xN matrix (and therefore minIdx/maxIdx will be
 * (0,j1)/(0,j2)).
 *
 * @param src input single-channel array.
 *
 * @param minVal pointer to the returned minimum value; NULL is used if not required.
 *
 * @param maxVal pointer to the returned maximum value; NULL is used if not required.
 *
 * @param minIdx pointer to the returned minimum location (in nD case); NULL is used if not required;
 * Otherwise, it must point to an array of src.dims elements, the coordinates of the minimum element in
 * each dimension are stored there sequentially.
 *
 * @param maxIdx pointer to the returned maximum location (in nD case). NULL is used if not required.
 *
 * @param mask specified array region
 */
export declare function minMaxIdx(src: Mat, minVal: any, maxVal?: any, minIdx?: any, maxIdx?: any, mask?: Mat): void;
/**
 * The function [cv::minMaxLoc] finds the minimum and maximum element values and their positions. The
 * extremums are searched across the whole array or, if mask is not an empty array, in the specified
 * array region.
 *
 * The function do not work with multi-channel arrays. If you need to find minimum or maximum elements
 * across all the channels, use [Mat::reshape] first to reinterpret the array as single-channel. Or you
 * may extract the particular channel using either extractImageCOI , or mixChannels , or split .
 *
 * [max], [min], [compare], [inRange], extractImageCOI, [mixChannels], [split], [Mat::reshape]
 *
 * @param src input single-channel array.
 *
 * @param minVal pointer to the returned minimum value; NULL is used if not required.
 *
 * @param maxVal pointer to the returned maximum value; NULL is used if not required.
 *
 * @param minLoc pointer to the returned minimum location (in 2D case); NULL is used if not required.
 *
 * @param maxLoc pointer to the returned maximum location (in 2D case); NULL is used if not required.
 *
 * @param mask optional mask used to select a sub-array.
 */
export declare function minMaxLoc(src: Mat, minVal: any, maxVal?: any, minLoc?: any, maxLoc?: any, mask?: Mat): MinMaxLoc;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param a input single-channel array.
 *
 * @param minVal pointer to the returned minimum value; NULL is used if not required.
 *
 * @param maxVal pointer to the returned maximum value; NULL is used if not required.
 *
 * @param minIdx pointer to the returned minimum location (in nD case); NULL is used if not required;
 * Otherwise, it must point to an array of src.dims elements, the coordinates of the minimum element in
 * each dimension are stored there sequentially.
 *
 * @param maxIdx pointer to the returned maximum location (in nD case). NULL is used if not required.
 */
export declare function minMaxLoc(a: any, minVal: any, maxVal: any, minIdx?: any, maxIdx?: any): MinMaxLoc;
/**
 * The function [cv::mixChannels] provides an advanced mechanism for shuffling image channels.
 *
 * [cv::split],[cv::merge],[cv::extractChannel],[cv::insertChannel] and some forms of [cv::cvtColor]
 * are partial cases of [cv::mixChannels].
 *
 * In the example below, the code splits a 4-channel BGRA image into a 3-channel BGR (with B and R
 * channels swapped) and a separate alpha-channel image:
 *
 * ```cpp
 * Mat bgra( 100, 100, CV_8UC4, Scalar(255,0,0,255) );
 * Mat bgr( bgra.rows, bgra.cols, CV_8UC3 );
 * Mat alpha( bgra.rows, bgra.cols, CV_8UC1 );
 *
 * // forming an array of matrices is a quite efficient operation,
 * // because the matrix data is not copied, only the headers
 * Mat out[] = { bgr, alpha };
 * // bgra[0] -> bgr[2], bgra[1] -> bgr[1],
 * // bgra[2] -> bgr[0], bgra[3] -> alpha[0]
 * int from_to[] = { 0,2, 1,1, 2,0, 3,3 };
 * mixChannels( &bgra, 1, out, 2, from_to, 4 );
 * ```
 *
 * Unlike many other new-style C++ functions in OpenCV (see the introduction section and [Mat::create]
 * ), [cv::mixChannels] requires the output arrays to be pre-allocated before calling the function.
 *
 * [split], [merge], [extractChannel], [insertChannel], [cvtColor]
 *
 * @param src input array or vector of matrices; all of the matrices must have the same size and the
 * same depth.
 *
 * @param nsrcs number of matrices in src.
 *
 * @param dst output array or vector of matrices; all the matrices must be allocated; their size and
 * depth must be the same as in src[0].
 *
 * @param ndsts number of matrices in dst.
 *
 * @param fromTo array of index pairs specifying which channels are copied and where; fromTo[k*2] is a
 * 0-based index of the input channel in src, fromTo[k*2+1] is an index of the output channel in dst;
 * the continuous channel numbering is used: the first input image channels are indexed from 0 to
 * src[0].channels()-1, the second input image channels are indexed from src[0].channels() to
 * src[0].channels() + src[1].channels()-1, and so on, the same scheme is used for the output image
 * channels; as a special case, when fromTo[k*2] is negative, the corresponding output channel is
 * filled with zero .
 *
 * @param npairs number of index pairs in fromTo.
 */
export declare function mixChannels(src: any, nsrcs: size_t, dst: any, ndsts: size_t, fromTo: any, npairs: size_t): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param src input array or vector of matrices; all of the matrices must have the same size and the
 * same depth.
 *
 * @param dst output array or vector of matrices; all the matrices must be allocated; their size and
 * depth must be the same as in src[0].
 *
 * @param fromTo array of index pairs specifying which channels are copied and where; fromTo[k*2] is a
 * 0-based index of the input channel in src, fromTo[k*2+1] is an index of the output channel in dst;
 * the continuous channel numbering is used: the first input image channels are indexed from 0 to
 * src[0].channels()-1, the second input image channels are indexed from src[0].channels() to
 * src[0].channels() + src[1].channels()-1, and so on, the same scheme is used for the output image
 * channels; as a special case, when fromTo[k*2] is negative, the corresponding output channel is
 * filled with zero .
 *
 * @param npairs number of index pairs in fromTo.
 */
export declare function mixChannels(src: MatVector, dst: MatVector, fromTo: any, npairs: size_t): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param src input array or vector of matrices; all of the matrices must have the same size and the
 * same depth.
 *
 * @param dst output array or vector of matrices; all the matrices must be allocated; their size and
 * depth must be the same as in src[0].
 *
 * @param fromTo array of index pairs specifying which channels are copied and where; fromTo[k*2] is a
 * 0-based index of the input channel in src, fromTo[k*2+1] is an index of the output channel in dst;
 * the continuous channel numbering is used: the first input image channels are indexed from 0 to
 * src[0].channels()-1, the second input image channels are indexed from src[0].channels() to
 * src[0].channels() + src[1].channels()-1, and so on, the same scheme is used for the output image
 * channels; as a special case, when fromTo[k*2] is negative, the corresponding output channel is
 * filled with zero .
 */
export declare function mixChannels(src: MatVector, dst: MatVector, fromTo: any): void;
/**
 * The function [cv::mulSpectrums] performs the per-element multiplication of the two CCS-packed or
 * complex matrices that are results of a real or complex Fourier transform.
 *
 * The function, together with dft and idft , may be used to calculate convolution (pass conjB=false )
 * or correlation (pass conjB=true ) of two arrays rapidly. When the arrays are complex, they are
 * simply multiplied (per element) with an optional conjugation of the second-array elements. When the
 * arrays are real, they are assumed to be CCS-packed (see dft for details).
 *
 * @param a first input array.
 *
 * @param b second input array of the same size and type as src1 .
 *
 * @param c output array of the same size and type as src1 .
 *
 * @param flags operation flags; currently, the only supported flag is cv::DFT_ROWS, which indicates
 * that each row of src1 and src2 is an independent 1D Fourier spectrum. If you do not want to use this
 * flag, then simply add a 0 as value.
 *
 * @param conjB optional flag that conjugates the second input array before the multiplication (true)
 * or not (false).
 */
export declare function mulSpectrums(a: Mat, b: Mat, c: Mat, flags: int, conjB?: bool): void;
/**
 * The function multiply calculates the per-element product of two arrays:
 *
 * `\\[\\texttt{dst} (I)= \\texttt{saturate} ( \\texttt{scale} \\cdot \\texttt{src1} (I) \\cdot
 * \\texttt{src2} (I))\\]`
 *
 * There is also a [MatrixExpressions] -friendly variant of the first function. See [Mat::mul] .
 *
 * For a not-per-element matrix product, see gemm .
 *
 * Saturation is not applied when the output array has the depth CV_32S. You may even get result of an
 * incorrect sign in the case of overflow.
 *
 * [add], [subtract], [divide], [scaleAdd], [addWeighted], [accumulate], [accumulateProduct],
 * [accumulateSquare], [Mat::convertTo]
 *
 * @param src1 first input array.
 *
 * @param src2 second input array of the same size and the same type as src1.
 *
 * @param dst output array of the same size and type as src1.
 *
 * @param scale optional scale factor.
 *
 * @param dtype optional depth of the output array
 */
export declare function multiply(src1: Mat, src2: Mat, dst: Mat, scale?: double, dtype?: int): void;
/**
 * The function [cv::mulTransposed] calculates the product of src and its transposition:
 * `\\[\\texttt{dst} = \\texttt{scale} ( \\texttt{src} - \\texttt{delta} )^T ( \\texttt{src} -
 * \\texttt{delta} )\\]` if aTa=true , and `\\[\\texttt{dst} = \\texttt{scale} ( \\texttt{src} -
 * \\texttt{delta} ) ( \\texttt{src} - \\texttt{delta} )^T\\]` otherwise. The function is used to
 * calculate the covariance matrix. With zero delta, it can be used as a faster substitute for general
 * matrix product A*B when B=A'
 *
 * [calcCovarMatrix], [gemm], [repeat], [reduce]
 *
 * @param src input single-channel matrix. Note that unlike gemm, the function can multiply not only
 * floating-point matrices.
 *
 * @param dst output square matrix.
 *
 * @param aTa Flag specifying the multiplication ordering. See the description below.
 *
 * @param delta Optional delta matrix subtracted from src before the multiplication. When the matrix is
 * empty ( delta=noArray() ), it is assumed to be zero, that is, nothing is subtracted. If it has the
 * same size as src , it is simply subtracted. Otherwise, it is "repeated" (see repeat ) to cover the
 * full src and then subtracted. Type of the delta matrix, when it is not empty, must be the same as
 * the type of created output matrix. See the dtype parameter description below.
 *
 * @param scale Optional scale factor for the matrix product.
 *
 * @param dtype Optional type of the output matrix. When it is negative, the output matrix will have
 * the same type as src . Otherwise, it will be type=CV_MAT_DEPTH(dtype) that should be either CV_32F
 * or CV_64F .
 */
export declare function mulTransposed(src: Mat, dst: Mat, aTa: bool, delta?: Mat, scale?: double, dtype?: int): void;
/**
 * This version of [norm] calculates the absolute norm of src1. The type of norm to calculate is
 * specified using [NormTypes].
 *
 * As example for one array consider the function `$r(x)= \\begin{pmatrix} x \\\\ 1-x \\end{pmatrix}, x
 * \\in [-1;1]$`. The `$ L_{1}, L_{2} $` and `$ L_{\\infty} $` norm for the sample value `$r(-1) =
 * \\begin{pmatrix} -1 \\\\ 2 \\end{pmatrix}$` is calculated as follows `\\begin{align*} \\| r(-1)
 * \\|_{L_1} &= |-1| + |2| = 3 \\\\ \\| r(-1) \\|_{L_2} &= \\sqrt{(-1)^{2} + (2)^{2}} = \\sqrt{5} \\\\
 * \\| r(-1) \\|_{L_\\infty} &= \\max(|-1|,|2|) = 2 \\end{align*}` and for `$r(0.5) = \\begin{pmatrix}
 * 0.5 \\\\ 0.5 \\end{pmatrix}$` the calculation is `\\begin{align*} \\| r(0.5) \\|_{L_1} &= |0.5| +
 * |0.5| = 1 \\\\ \\| r(0.5) \\|_{L_2} &= \\sqrt{(0.5)^{2} + (0.5)^{2}} = \\sqrt{0.5} \\\\ \\| r(0.5)
 * \\|_{L_\\infty} &= \\max(|0.5|,|0.5|) = 0.5. \\end{align*}` The following graphic shows all values
 * for the three norm functions `$\\| r(x) \\|_{L_1}, \\| r(x) \\|_{L_2}$` and `$\\| r(x)
 * \\|_{L_\\infty}$`. It is notable that the `$ L_{1} $` norm forms the upper and the `$ L_{\\infty} $`
 * norm forms the lower border for the example function `$ r(x) $`.
 *  When the mask parameter is specified and it is not empty, the norm is
 *
 * If normType is not specified, [NORM_L2] is used. calculated only over the region specified by the
 * mask.
 *
 * Multi-channel input arrays are treated as single-channel arrays, that is, the results for all
 * channels are combined.
 *
 * [Hamming] norms can only be calculated with CV_8U depth arrays.
 *
 * @param src1 first input array.
 *
 * @param normType type of the norm (see NormTypes).
 *
 * @param mask optional operation mask; it must have the same size as src1 and CV_8UC1 type.
 */
export declare function norm(src1: Mat, normType?: int, mask?: Mat): double;
/**
 * This version of [cv::norm] calculates the absolute difference norm or the relative difference norm
 * of arrays src1 and src2. The type of norm to calculate is specified using [NormTypes].
 *
 * @param src1 first input array.
 *
 * @param src2 second input array of the same size and the same type as src1.
 *
 * @param normType type of the norm (see NormTypes).
 *
 * @param mask optional operation mask; it must have the same size as src1 and CV_8UC1 type.
 */
export declare function norm(src1: Mat, src2: Mat, normType?: int, mask?: Mat): double;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param src first input array.
 *
 * @param normType type of the norm (see NormTypes).
 */
export declare function norm(src: any, normType: int): double;
/**
 * The function [cv::normalize] normalizes scale and shift the input array elements so that `\\[\\|
 * \\texttt{dst} \\| _{L_p}= \\texttt{alpha}\\]` (where p=Inf, 1 or 2) when normType=NORM_INF, NORM_L1,
 * or NORM_L2, respectively; or so that `\\[\\min _I \\texttt{dst} (I)= \\texttt{alpha} , \\, \\, \\max
 * _I \\texttt{dst} (I)= \\texttt{beta}\\]`
 *
 * when normType=NORM_MINMAX (for dense arrays only). The optional mask specifies a sub-array to be
 * normalized. This means that the norm or min-n-max are calculated over the sub-array, and then this
 * sub-array is modified to be normalized. If you want to only use the mask to calculate the norm or
 * min-max but modify the whole array, you can use norm and [Mat::convertTo].
 *
 * In case of sparse matrices, only the non-zero values are analyzed and transformed. Because of this,
 * the range transformation for sparse matrices is not allowed since it can shift the zero level.
 *
 * Possible usage with some positive example data:
 *
 * ```cpp
 * vector<double> positiveData = { 2.0, 8.0, 10.0 };
 * vector<double> normalizedData_l1, normalizedData_l2, normalizedData_inf, normalizedData_minmax;
 *
 * // Norm to probability (total count)
 * // sum(numbers) = 20.0
 * // 2.0      0.1     (2.0/20.0)
 * // 8.0      0.4     (8.0/20.0)
 * // 10.0     0.5     (10.0/20.0)
 * normalize(positiveData, normalizedData_l1, 1.0, 0.0, NORM_L1);
 *
 * // Norm to unit vector: ||positiveData|| = 1.0
 * // 2.0      0.15
 * // 8.0      0.62
 * // 10.0     0.77
 * normalize(positiveData, normalizedData_l2, 1.0, 0.0, NORM_L2);
 *
 * // Norm to max element
 * // 2.0      0.2     (2.0/10.0)
 * // 8.0      0.8     (8.0/10.0)
 * // 10.0     1.0     (10.0/10.0)
 * normalize(positiveData, normalizedData_inf, 1.0, 0.0, NORM_INF);
 *
 * // Norm to range [0.0;1.0]
 * // 2.0      0.0     (shift to left border)
 * // 8.0      0.75    (6.0/8.0)
 * // 10.0     1.0     (shift to right border)
 * normalize(positiveData, normalizedData_minmax, 1.0, 0.0, NORM_MINMAX);
 * ```
 *
 * [norm], [Mat::convertTo], [SparseMat::convertTo]
 *
 * @param src input array.
 *
 * @param dst output array of the same size as src .
 *
 * @param alpha norm value to normalize to or the lower range boundary in case of the range
 * normalization.
 *
 * @param beta upper range boundary in case of the range normalization; it is not used for the norm
 * normalization.
 *
 * @param norm_type normalization type (see cv::NormTypes).
 *
 * @param dtype when negative, the output array has the same type as src; otherwise, it has the same
 * number of channels as src and the depth =CV_MAT_DEPTH(dtype).
 *
 * @param mask optional operation mask.
 */
export declare function normalize(src: Mat, dst: Mat, alpha?: double, beta?: double, norm_type?: int, dtype?: int, mask?: Mat): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param src input array.
 *
 * @param dst output array of the same size as src .
 *
 * @param alpha norm value to normalize to or the lower range boundary in case of the range
 * normalization.
 *
 * @param normType normalization type (see cv::NormTypes).
 */
export declare function normalize(src: any, dst: any, alpha: double, normType: int): void;
export declare function patchNaNs(a: Mat, val?: double): void;
/**
 * wrap [PCA::backProject]
 */
export declare function PCABackProject(data: Mat, mean: Mat, eigenvectors: Mat, result: Mat): void;
/**
 * wrap PCA::operator()
 */
export declare function PCACompute(data: Mat, mean: Mat, eigenvectors: Mat, maxComponents?: int): void;
/**
 * wrap PCA::operator() and add eigenvalues output parameter
 */
export declare function PCACompute(data: Mat, mean: Mat, eigenvectors: Mat, eigenvalues: Mat, maxComponents?: int): void;
/**
 * wrap PCA::operator()
 */
export declare function PCACompute(data: Mat, mean: Mat, eigenvectors: Mat, retainedVariance: double): void;
/**
 * wrap PCA::operator() and add eigenvalues output parameter
 */
export declare function PCACompute(data: Mat, mean: Mat, eigenvectors: Mat, eigenvalues: Mat, retainedVariance: double): void;
/**
 * wrap [PCA::project]
 */
export declare function PCAProject(data: Mat, mean: Mat, eigenvectors: Mat, result: Mat): void;
/**
 * The function [cv::perspectiveTransform] transforms every element of src by treating it as a 2D or 3D
 * vector, in the following way: `\\[(x, y, z) \\rightarrow (x'/w, y'/w, z'/w)\\]` where `\\[(x', y',
 * z', w') = \\texttt{mat} \\cdot \\begin{bmatrix} x & y & z & 1 \\end{bmatrix}\\]` and `\\[w =
 * \\fork{w'}{if \\(w' \\ne 0\\)}{\\infty}{otherwise}\\]`
 *
 * Here a 3D vector transformation is shown. In case of a 2D vector transformation, the z component is
 * omitted.
 *
 * The function transforms a sparse set of 2D or 3D vectors. If you want to transform an image using
 * perspective transformation, use warpPerspective . If you have an inverse problem, that is, you want
 * to compute the most probable perspective transformation out of several pairs of corresponding
 * points, you can use getPerspectiveTransform or findHomography .
 *
 * [transform], [warpPerspective], [getPerspectiveTransform], [findHomography]
 *
 * @param src input two-channel or three-channel floating-point array; each element is a 2D/3D vector
 * to be transformed.
 *
 * @param dst output array of the same size and type as src.
 *
 * @param m 3x3 or 4x4 floating-point transformation matrix.
 */
export declare function perspectiveTransform(src: Mat, dst: Mat, m: Mat): void;
/**
 * The function [cv::phase] calculates the rotation angle of each 2D vector that is formed from the
 * corresponding elements of x and y : `\\[\\texttt{angle} (I) = \\texttt{atan2} ( \\texttt{y} (I),
 * \\texttt{x} (I))\\]`
 *
 * The angle estimation accuracy is about 0.3 degrees. When x(I)=y(I)=0 , the corresponding angle(I) is
 * set to 0.
 *
 * @param x input floating-point array of x-coordinates of 2D vectors.
 *
 * @param y input array of y-coordinates of 2D vectors; it must have the same size and the same type as
 * x.
 *
 * @param angle output array of vector angles; it has the same size and same type as x .
 *
 * @param angleInDegrees when true, the function calculates the angle in degrees, otherwise, they are
 * measured in radians.
 */
export declare function phase(x: Mat, y: Mat, angle: Mat, angleInDegrees?: bool): void;
/**
 * The function [cv::polarToCart] calculates the Cartesian coordinates of each 2D vector represented by
 * the corresponding elements of magnitude and angle: `\\[\\begin{array}{l} \\texttt{x} (I) =
 * \\texttt{magnitude} (I) \\cos ( \\texttt{angle} (I)) \\\\ \\texttt{y} (I) = \\texttt{magnitude} (I)
 * \\sin ( \\texttt{angle} (I)) \\\\ \\end{array}\\]`
 *
 * The relative accuracy of the estimated coordinates is about 1e-6.
 *
 * [cartToPolar], [magnitude], [phase], [exp], [log], [pow], [sqrt]
 *
 * @param magnitude input floating-point array of magnitudes of 2D vectors; it can be an empty matrix
 * (=Mat()), in this case, the function assumes that all the magnitudes are =1; if it is not empty, it
 * must have the same size and type as angle.
 *
 * @param angle input floating-point array of angles of 2D vectors.
 *
 * @param x output array of x-coordinates of 2D vectors; it has the same size and type as angle.
 *
 * @param y output array of y-coordinates of 2D vectors; it has the same size and type as angle.
 *
 * @param angleInDegrees when true, the input angles are measured in degrees, otherwise, they are
 * measured in radians.
 */
export declare function polarToCart(magnitude: Mat, angle: Mat, x: Mat, y: Mat, angleInDegrees?: bool): void;
/**
 * The function [cv::pow] raises every element of the input array to power : `\\[\\texttt{dst} (I) =
 * \\fork{\\texttt{src}(I)^{power}}{if \\(\\texttt{power}\\) is
 * integer}{|\\texttt{src}(I)|^{power}}{otherwise}\\]`
 *
 * So, for a non-integer power exponent, the absolute values of input array elements are used. However,
 * it is possible to get true values for negative values using some extra operations. In the example
 * below, computing the 5th root of array src shows:
 *
 * ```cpp
 * Mat mask = src < 0;
 * pow(src, 1./5, dst);
 * subtract(Scalar::all(0), dst, dst, mask);
 * ```
 *
 *  For some values of power, such as integer values, 0.5 and -0.5, specialized faster algorithms are
 * used.
 *
 * Special values (NaN, Inf) are not handled.
 *
 * [sqrt], [exp], [log], [cartToPolar], [polarToCart]
 *
 * @param src input array.
 *
 * @param power exponent of power.
 *
 * @param dst output array of the same size and type as src.
 */
export declare function pow(src: Mat, power: double, dst: Mat): void;
/**
 * This function calculates the Peak Signal-to-Noise Ratio (PSNR) image quality metric in decibels
 * (dB), between two input arrays src1 and src2. The arrays must have the same type.
 *
 * The PSNR is calculated as follows:
 *
 * `\\[ \\texttt{PSNR} = 10 \\cdot \\log_{10}{\\left( \\frac{R^2}{MSE} \\right) } \\]`
 *
 * where R is the maximum integer value of depth (e.g. 255 in the case of CV_8U data) and MSE is the
 * mean squared error between the two arrays.
 *
 * @param src1 first input array.
 *
 * @param src2 second input array of the same size as src1.
 *
 * @param R the maximum pixel value (255 by default)
 */
export declare function PSNR(src1: Mat, src2: Mat, R?: double): double;
/**
 * The function [cv::randn] fills the matrix dst with normally distributed random numbers with the
 * specified mean vector and the standard deviation matrix. The generated random numbers are clipped to
 * fit the value range of the output array data type.
 *
 * [RNG], [randu]
 *
 * @param dst output array of random numbers; the array must be pre-allocated and have 1 to 4 channels.
 *
 * @param mean mean value (expectation) of the generated random numbers.
 *
 * @param stddev standard deviation of the generated random numbers; it can be either a vector (in
 * which case a diagonal standard deviation matrix is assumed) or a square matrix.
 */
export declare function randn(dst: Mat, mean: Mat, stddev: Mat): void;
/**
 * The function [cv::randShuffle] shuffles the specified 1D array by randomly choosing pairs of
 * elements and swapping them. The number of such swap operations will be dst.rows*dst.cols*iterFactor
 * .
 *
 * [RNG], [sort]
 *
 * @param dst input/output numerical 1D array.
 *
 * @param iterFactor scale factor that determines the number of random swap operations (see the details
 * below).
 *
 * @param rng optional random number generator used for shuffling; if it is zero, theRNG () is used
 * instead.
 */
export declare function randShuffle(dst: Mat, iterFactor?: double, rng?: any): void;
/**
 * Non-template variant of the function fills the matrix dst with uniformly-distributed random numbers
 * from the specified range: `\\[\\texttt{low} _c \\leq \\texttt{dst} (I)_c < \\texttt{high} _c\\]`
 *
 * [RNG], [randn], [theRNG]
 *
 * @param dst output array of random numbers; the array must be pre-allocated.
 *
 * @param low inclusive lower boundary of the generated random numbers.
 *
 * @param high exclusive upper boundary of the generated random numbers.
 */
export declare function randu(dst: Mat, low: Mat, high: Mat): void;
/**
 * The function [reduce] reduces the matrix to a vector by treating the matrix rows/columns as a set of
 * 1D vectors and performing the specified operation on the vectors until a single row/column is
 * obtained. For example, the function can be used to compute horizontal and vertical projections of a
 * raster image. In case of [REDUCE_MAX] and [REDUCE_MIN] , the output image should have the same type
 * as the source one. In case of [REDUCE_SUM] and [REDUCE_AVG] , the output may have a larger element
 * bit-depth to preserve accuracy. And multi-channel arrays are also supported in these two reduction
 * modes.
 *
 * The following code demonstrates its usage for a single channel matrix.
 *
 * ```cpp
 *         Mat m = (Mat_<uchar>(3,2) << 1,2,3,4,5,6);
 *         Mat col_sum, row_sum;
 *
 *         reduce(m, col_sum, 0, REDUCE_SUM, CV_32F);
 *         reduce(m, row_sum, 1, REDUCE_SUM, CV_32F);
 *         /*
 *         m =
 *         [  1,   2;
 *            3,   4;
 *            5,   6]
 *         col_sum =
 *         [9, 12]
 *         row_sum =
 *         [3;
 *          7;
 *          11]
 * \/
 * ```
 *
 *  And the following code demonstrates its usage for a two-channel matrix.
 *
 * ```cpp
 *         // two channels
 *         char d[] = {1,2,3,4,5,6};
 *         Mat m(3, 1, CV_8UC2, d);
 *         Mat col_sum_per_channel;
 *         reduce(m, col_sum_per_channel, 0, REDUCE_SUM, CV_32F);
 *         /*
 *         col_sum_per_channel =
 *         [9, 12]
 * \/
 * ```
 *
 * [repeat]
 *
 * @param src input 2D matrix.
 *
 * @param dst output vector. Its size and type is defined by dim and dtype parameters.
 *
 * @param dim dimension index along which the matrix is reduced. 0 means that the matrix is reduced to
 * a single row. 1 means that the matrix is reduced to a single column.
 *
 * @param rtype reduction operation that could be one of ReduceTypes
 *
 * @param dtype when negative, the output vector will have the same type as the input matrix,
 * otherwise, its type will be CV_MAKE_TYPE(CV_MAT_DEPTH(dtype), src.channels()).
 */
export declare function reduce(src: Mat, dst: Mat, dim: int, rtype: int, dtype?: int): void;
/**
 * The function [cv::repeat] duplicates the input array one or more times along each of the two axes:
 * `\\[\\texttt{dst} _{ij}= \\texttt{src} _{i\\mod src.rows, \\; j\\mod src.cols }\\]` The second
 * variant of the function is more convenient to use with [MatrixExpressions].
 *
 * [cv::reduce]
 *
 * @param src input array to replicate.
 *
 * @param ny Flag to specify how many times the src is repeated along the vertical axis.
 *
 * @param nx Flag to specify how many times the src is repeated along the horizontal axis.
 *
 * @param dst output array of the same type as src.
 */
export declare function repeat(src: Mat, ny: int, nx: int, dst: Mat): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param src input array to replicate.
 *
 * @param ny Flag to specify how many times the src is repeated along the vertical axis.
 *
 * @param nx Flag to specify how many times the src is repeated along the horizontal axis.
 */
export declare function repeat(src: any, ny: int, nx: int): Mat;
/**
 * [transpose] , [repeat] , [completeSymm], [flip], [RotateFlags]
 *
 * @param src input array.
 *
 * @param dst output array of the same type as src. The size is the same with ROTATE_180, and the rows
 * and cols are switched for ROTATE_90_CLOCKWISE and ROTATE_90_COUNTERCLOCKWISE.
 *
 * @param rotateCode an enum to specify how to rotate the array; see the enum RotateFlags
 */
export declare function rotate(src: Mat, dst: Mat, rotateCode: int): void;
/**
 * The function scaleAdd is one of the classical primitive linear algebra operations, known as DAXPY or
 * SAXPY in . It calculates the sum of a scaled array and another array: `\\[\\texttt{dst} (I)=
 * \\texttt{scale} \\cdot \\texttt{src1} (I) + \\texttt{src2} (I)\\]` The function can also be emulated
 * with a matrix expression, for example:
 *
 * ```cpp
 * Mat A(3, 3, CV_64F);
 * ...
 * A.row(0) = A.row(1)*2 + A.row(2);
 * ```
 *
 * [add], [addWeighted], [subtract], [Mat::dot], [Mat::convertTo]
 *
 * @param src1 first input array.
 *
 * @param alpha scale factor for the first array.
 *
 * @param src2 second input array of the same size and type as src1.
 *
 * @param dst output array of the same size and type as src1.
 */
export declare function scaleAdd(src1: Mat, alpha: double, src2: Mat, dst: Mat): void;
/**
 * The function [cv::setIdentity] initializes a scaled identity matrix: `\\[\\texttt{mtx} (i,j)=
 * \\fork{\\texttt{value}}{ if \\(i=j\\)}{0}{otherwise}\\]`
 *
 * The function can also be emulated using the matrix initializers and the matrix expressions:
 *
 * ```cpp
 * Mat A = Mat::eye(4, 3, CV_32F)*5;
 * // A will be set to [[5, 0, 0], [0, 5, 0], [0, 0, 5], [0, 0, 0]]
 * ```
 *
 * [Mat::zeros], [Mat::ones], [Mat::setTo], [Mat::operator=]
 *
 * @param mtx matrix to initialize (not necessarily square).
 *
 * @param s value to assign to diagonal elements.
 */
export declare function setIdentity(mtx: Mat, s?: any): void;
/**
 * The function [cv::setRNGSeed] sets state of default random number generator to custom value.
 *
 * [RNG], [randu], [randn]
 *
 * @param seed new state for default random number generator
 */
export declare function setRNGSeed(seed: int): void;
/**
 * The function [cv::solve] solves a linear system or least-squares problem (the latter is possible
 * with [SVD] or QR methods, or by specifying the flag [DECOMP_NORMAL] ): `\\[\\texttt{dst} = \\arg
 * \\min _X \\| \\texttt{src1} \\cdot \\texttt{X} - \\texttt{src2} \\|\\]`
 *
 * If [DECOMP_LU] or [DECOMP_CHOLESKY] method is used, the function returns 1 if src1 (or
 * `$\\texttt{src1}^T\\texttt{src1}$` ) is non-singular. Otherwise, it returns 0. In the latter case,
 * dst is not valid. Other methods find a pseudo-solution in case of a singular left-hand side part.
 *
 * If you want to find a unity-norm solution of an under-defined singular system
 * `$\\texttt{src1}\\cdot\\texttt{dst}=0$` , the function solve will not do the work. Use [SVD::solveZ]
 * instead.
 *
 * [invert], [SVD], [eigen]
 *
 * @param src1 input matrix on the left-hand side of the system.
 *
 * @param src2 input matrix on the right-hand side of the system.
 *
 * @param dst output solution.
 *
 * @param flags solution (matrix inversion) method (DecompTypes)
 */
export declare function solve(src1: Mat, src2: Mat, dst: Mat, flags?: int): bool;
/**
 * The function solveCubic finds the real roots of a cubic equation:
 *
 * if coeffs is a 4-element vector: `\\[\\texttt{coeffs} [0] x^3 + \\texttt{coeffs} [1] x^2 +
 * \\texttt{coeffs} [2] x + \\texttt{coeffs} [3] = 0\\]`
 * if coeffs is a 3-element vector: `\\[x^3 + \\texttt{coeffs} [0] x^2 + \\texttt{coeffs} [1] x +
 * \\texttt{coeffs} [2] = 0\\]`
 *
 * The roots are stored in the roots array.
 *
 * number of real roots. It can be 0, 1 or 2.
 *
 * @param coeffs equation coefficients, an array of 3 or 4 elements.
 *
 * @param roots output array of real roots that has 1 or 3 elements.
 */
export declare function solveCubic(coeffs: Mat, roots: Mat): int;
/**
 * The function [cv::solvePoly] finds real and complex roots of a polynomial equation:
 * `\\[\\texttt{coeffs} [n] x^{n} + \\texttt{coeffs} [n-1] x^{n-1} + ... + \\texttt{coeffs} [1] x +
 * \\texttt{coeffs} [0] = 0\\]`
 *
 * @param coeffs array of polynomial coefficients.
 *
 * @param roots output (complex) array of roots.
 *
 * @param maxIters maximum number of iterations the algorithm does.
 */
export declare function solvePoly(coeffs: Mat, roots: Mat, maxIters?: int): double;
/**
 * The function [cv::sort] sorts each matrix row or each matrix column in ascending or descending
 * order. So you should pass two operation flags to get desired behaviour. If you want to sort matrix
 * rows or columns lexicographically, you can use STL std::sort generic function with the proper
 * comparison predicate.
 *
 * [sortIdx], [randShuffle]
 *
 * @param src input single-channel array.
 *
 * @param dst output array of the same size and type as src.
 *
 * @param flags operation flags, a combination of SortFlags
 */
export declare function sort(src: Mat, dst: Mat, flags: int): void;
/**
 * The function [cv::sortIdx] sorts each matrix row or each matrix column in the ascending or
 * descending order. So you should pass two operation flags to get desired behaviour. Instead of
 * reordering the elements themselves, it stores the indices of sorted elements in the output array.
 * For example:
 *
 * ```cpp
 * Mat A = Mat::eye(3,3,CV_32F), B;
 * sortIdx(A, B, SORT_EVERY_ROW + SORT_ASCENDING);
 * // B will probably contain
 * // (because of equal elements in A some permutations are possible):
 * // [[1, 2, 0], [0, 2, 1], [0, 1, 2]]
 * ```
 *
 * [sort], [randShuffle]
 *
 * @param src input single-channel array.
 *
 * @param dst output integer array of the same size as src.
 *
 * @param flags operation flags that could be a combination of cv::SortFlags
 */
export declare function sortIdx(src: Mat, dst: Mat, flags: int): void;
/**
 * The function [cv::split] splits a multi-channel array into separate single-channel arrays:
 * `\\[\\texttt{mv} [c](I) = \\texttt{src} (I)_c\\]` If you need to extract a single channel or do some
 * other sophisticated channel permutation, use mixChannels .
 *
 * The following example demonstrates how to split a 3-channel matrix into 3 single channel matrices.
 *
 * ```cpp
 *     char d[] = {1,2,3,4,5,6,7,8,9,10,11,12};
 *     Mat m(2, 2, CV_8UC3, d);
 *     Mat channels[3];
 *     split(m, channels);
 *
 *     /*
 *     channels[0] =
 *     [  1,   4;
 *        7,  10]
 *     channels[1] =
 *     [  2,   5;
 *        8,  11]
 *     channels[2] =
 *     [  3,   6;
 *        9,  12]
 * \/
 * ```
 *
 * [merge], [mixChannels], [cvtColor]
 *
 * @param src input multi-channel array.
 *
 * @param mvbegin output array; the number of arrays must match src.channels(); the arrays themselves
 * are reallocated, if needed.
 */
export declare function split(src: any, mvbegin: any): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param m input multi-channel array.
 *
 * @param mv output vector of arrays; the arrays themselves are reallocated, if needed.
 */
export declare function split(m: Mat, mv: MatVector): void;
/**
 * The function [cv::sqrt] calculates a square root of each input array element. In case of
 * multi-channel arrays, each channel is processed independently. The accuracy is approximately the
 * same as of the built-in std::sqrt .
 *
 * @param src input floating-point array.
 *
 * @param dst output array of the same size and type as src.
 */
export declare function sqrt(src: Mat, dst: Mat): void;
/**
 * The function subtract calculates:
 *
 * Difference between two arrays, when both input arrays have the same size and the same number of
 * channels: `\\[\\texttt{dst}(I) = \\texttt{saturate} ( \\texttt{src1}(I) - \\texttt{src2}(I)) \\quad
 * \\texttt{if mask}(I) \\ne0\\]`
 * Difference between an array and a scalar, when src2 is constructed from Scalar or has the same
 * number of elements as `src1.channels()`: `\\[\\texttt{dst}(I) = \\texttt{saturate} (
 * \\texttt{src1}(I) - \\texttt{src2} ) \\quad \\texttt{if mask}(I) \\ne0\\]`
 * Difference between a scalar and an array, when src1 is constructed from Scalar or has the same
 * number of elements as `src2.channels()`: `\\[\\texttt{dst}(I) = \\texttt{saturate} ( \\texttt{src1}
 * - \\texttt{src2}(I) ) \\quad \\texttt{if mask}(I) \\ne0\\]`
 * The reverse difference between a scalar and an array in the case of `SubRS`: `\\[\\texttt{dst}(I) =
 * \\texttt{saturate} ( \\texttt{src2} - \\texttt{src1}(I) ) \\quad \\texttt{if mask}(I) \\ne0\\]`
 * where I is a multi-dimensional index of array elements. In case of multi-channel arrays, each
 * channel is processed independently.
 *
 * The first function in the list above can be replaced with matrix expressions:
 *
 * ```cpp
 * dst = src1 - src2;
 * dst -= src1; // equivalent to subtract(dst, src1, dst);
 * ```
 *
 *  The input arrays and the output array can all have the same or different depths. For example, you
 * can subtract to 8-bit unsigned arrays and store the difference in a 16-bit signed array. Depth of
 * the output array is determined by dtype parameter. In the second and third cases above, as well as
 * in the first case, when src1.depth() == src2.depth(), dtype can be set to the default -1. In this
 * case the output array will have the same depth as the input array, be it src1, src2 or both.
 *
 * Saturation is not applied when the output array has the depth CV_32S. You may even get result of an
 * incorrect sign in the case of overflow.
 *
 * [add], [addWeighted], [scaleAdd], [Mat::convertTo]
 *
 * @param src1 first input array or a scalar.
 *
 * @param src2 second input array or a scalar.
 *
 * @param dst output array of the same size and the same number of channels as the input array.
 *
 * @param mask optional operation mask; this is an 8-bit single channel array that specifies elements
 * of the output array to be changed.
 *
 * @param dtype optional depth of the output array
 */
export declare function subtract(src1: Mat, src2: Mat, dst: Mat, mask?: Mat, dtype?: int): void;
/**
 * The function [cv::sum] calculates and returns the sum of array elements, independently for each
 * channel.
 *
 * [countNonZero], [mean], [meanStdDev], [norm], [minMaxLoc], [reduce]
 *
 * @param src input array that must have from 1 to 4 channels.
 */
export declare function sum(src: Mat): Scalar;
/**
 * wrap [SVD::backSubst]
 */
export declare function SVBackSubst(w: Mat, u: Mat, vt: Mat, rhs: Mat, dst: Mat): void;
/**
 * wrap [SVD::compute]
 */
export declare function SVDecomp(src: Mat, w: Mat, u: Mat, vt: Mat, flags?: int): void;
/**
 * The function [cv::theRNG] returns the default random number generator. For each thread, there is a
 * separate random number generator, so you can use the function safely in multi-thread environments.
 * If you just need to get a single random number using this generator or initialize an array, you can
 * use randu or randn instead. But if you are going to generate many random numbers inside a loop, it
 * is much faster to use this function to retrieve the generator and then use RNG::operator _Tp() .
 *
 * [RNG], [randu], [randn]
 */
export declare function theRNG(): any;
/**
 * The function [cv::trace] returns the sum of the diagonal elements of the matrix mtx .
 * `\\[\\mathrm{tr} ( \\texttt{mtx} ) = \\sum _i \\texttt{mtx} (i,i)\\]`
 *
 * @param mtx input matrix.
 */
export declare function trace(mtx: Mat): Scalar;
/**
 * The function [cv::transform] performs the matrix transformation of every element of the array src
 * and stores the results in dst : `\\[\\texttt{dst} (I) = \\texttt{m} \\cdot \\texttt{src} (I)\\]`
 * (when m.cols=src.channels() ), or `\\[\\texttt{dst} (I) = \\texttt{m} \\cdot [ \\texttt{src} (I);
 * 1]\\]` (when m.cols=src.channels()+1 )
 *
 * Every element of the N -channel array src is interpreted as N -element vector that is transformed
 * using the M x N or M x (N+1) matrix m to M-element vector - the corresponding element of the output
 * array dst .
 *
 * The function may be used for geometrical transformation of N -dimensional points, arbitrary linear
 * color space transformation (such as various kinds of RGB to YUV transforms), shuffling the image
 * channels, and so forth.
 *
 * [perspectiveTransform], [getAffineTransform], [estimateAffine2D], [warpAffine], [warpPerspective]
 *
 * @param src input array that must have as many channels (1 to 4) as m.cols or m.cols-1.
 *
 * @param dst output array of the same size and depth as src; it has as many channels as m.rows.
 *
 * @param m transformation 2x2 or 2x3 floating-point matrix.
 */
export declare function transform(src: Mat, dst: Mat, m: Mat): void;
/**
 * The function [cv::transpose] transposes the matrix src : `\\[\\texttt{dst} (i,j) = \\texttt{src}
 * (j,i)\\]`
 *
 * No complex conjugation is done in case of a complex matrix. It should be done separately if needed.
 *
 * @param src input array.
 *
 * @param dst output array of the same type as src.
 */
export declare function transpose(src: Mat, dst: Mat): void;
/**
 * The function vertically concatenates two or more [cv::Mat] matrices (with the same number of cols).
 *
 * ```cpp
 * cv::Mat matArray[] = { cv::Mat(1, 4, CV_8UC1, cv::Scalar(1)),
 *                        cv::Mat(1, 4, CV_8UC1, cv::Scalar(2)),
 *                        cv::Mat(1, 4, CV_8UC1, cv::Scalar(3)),};
 *
 * cv::Mat out;
 * cv::vconcat( matArray, 3, out );
 * //out:
 * //[1,   1,   1,   1;
 * // 2,   2,   2,   2;
 * // 3,   3,   3,   3]
 * ```
 *
 * [cv::hconcat(const Mat*, size_t, OutputArray)],
 *
 * [cv::hconcat(InputArrayOfArrays, OutputArray)] and
 *
 * [cv::hconcat(InputArray, InputArray, OutputArray)]
 *
 * @param src input array or vector of matrices. all of the matrices must have the same number of cols
 * and the same depth.
 *
 * @param nsrc number of matrices in src.
 *
 * @param dst output array. It has the same number of cols and depth as the src, and the sum of rows of
 * the src.
 */
export declare function vconcat(src: any, nsrc: size_t, dst: Mat): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * ```cpp
 * cv::Mat_<float> A = (cv::Mat_<float>(3, 2) << 1, 7,
 *                                               2, 8,
 *                                               3, 9);
 * cv::Mat_<float> B = (cv::Mat_<float>(3, 2) << 4, 10,
 *                                               5, 11,
 *                                               6, 12);
 *
 * cv::Mat C;
 * cv::vconcat(A, B, C);
 * //C:
 * //[1, 7;
 * // 2, 8;
 * // 3, 9;
 * // 4, 10;
 * // 5, 11;
 * // 6, 12]
 * ```
 *
 * @param src1 first input array to be considered for vertical concatenation.
 *
 * @param src2 second input array to be considered for vertical concatenation.
 *
 * @param dst output array. It has the same number of cols and depth as the src1 and src2, and the sum
 * of rows of the src1 and src2.
 */
export declare function vconcat(src1: Mat, src2: Mat, dst: Mat): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * ```cpp
 * std::vector<cv::Mat> matrices = { cv::Mat(1, 4, CV_8UC1, cv::Scalar(1)),
 *                                   cv::Mat(1, 4, CV_8UC1, cv::Scalar(2)),
 *                                   cv::Mat(1, 4, CV_8UC1, cv::Scalar(3)),};
 *
 * cv::Mat out;
 * cv::vconcat( matrices, out );
 * //out:
 * //[1,   1,   1,   1;
 * // 2,   2,   2,   2;
 * // 3,   3,   3,   3]
 * ```
 *
 * @param src input array or vector of matrices. all of the matrices must have the same number of cols
 * and the same depth
 *
 * @param dst output array. It has the same number of cols and depth as the src, and the sum of rows of
 * the src. same depth.
 */
export declare function vconcat(src: MatVector, dst: Mat): void;
export declare const BORDER_CONSTANT: BorderTypes;
export declare const BORDER_REPLICATE: BorderTypes;
export declare const BORDER_REFLECT: BorderTypes;
export declare const BORDER_WRAP: BorderTypes;
export declare const BORDER_REFLECT_101: BorderTypes;
export declare const BORDER_TRANSPARENT: BorderTypes;
export declare const BORDER_REFLECT101: BorderTypes;
export declare const BORDER_DEFAULT: BorderTypes;
export declare const BORDER_ISOLATED: BorderTypes;
export declare const CMP_EQ: CmpTypes;
export declare const CMP_GT: CmpTypes;
export declare const CMP_GE: CmpTypes;
export declare const CMP_LT: CmpTypes;
export declare const CMP_LE: CmpTypes;
export declare const CMP_NE: CmpTypes;
/**
 * Gaussian elimination with the optimal pivot element chosen.
 *
 */
export declare const DECOMP_LU: DecompTypes;
/**
 * singular value decomposition ([SVD]) method; the system can be over-defined and/or the matrix src1
 * can be singular
 *
 */
export declare const DECOMP_SVD: DecompTypes;
/**
 * eigenvalue decomposition; the matrix src1 must be symmetrical
 *
 */
export declare const DECOMP_EIG: DecompTypes;
/**
 * Cholesky `$LL^T$` factorization; the matrix src1 must be symmetrical and positively defined
 *
 */
export declare const DECOMP_CHOLESKY: DecompTypes;
/**
 * QR factorization; the system can be over-defined and/or the matrix src1 can be singular
 *
 */
export declare const DECOMP_QR: DecompTypes;
/**
 * while all the previous flags are mutually exclusive, this flag can be used together with any of the
 * previous; it means that the normal equations
 * `$\\texttt{src1}^T\\cdot\\texttt{src1}\\cdot\\texttt{dst}=\\texttt{src1}^T\\texttt{src2}$` are
 * solved instead of the original system `$\\texttt{src1}\\cdot\\texttt{dst}=\\texttt{src2}$`
 *
 */
export declare const DECOMP_NORMAL: DecompTypes;
/**
 * performs an inverse 1D or 2D transform instead of the default forward transform.
 *
 */
export declare const DFT_INVERSE: DftFlags;
/**
 * scales the result: divide it by the number of array elements. Normally, it is combined with
 * DFT_INVERSE.
 *
 */
export declare const DFT_SCALE: DftFlags;
/**
 * performs a forward or inverse transform of every individual row of the input matrix; this flag
 * enables you to transform multiple vectors simultaneously and can be used to decrease the overhead
 * (which is sometimes several times larger than the processing itself) to perform 3D and
 * higher-dimensional transformations and so forth.
 *
 */
export declare const DFT_ROWS: DftFlags;
/**
 * performs a forward transformation of 1D or 2D real array; the result, though being a complex array,
 * has complex-conjugate symmetry (*CCS*, see the function description below for details), and such an
 * array can be packed into a real array of the same size as input, which is the fastest option and
 * which is what the function does by default; however, you may wish to get a full complex array (for
 * simpler spectrum analysis, and so on) - pass the flag to enable the function to produce a full-size
 * complex output array.
 *
 */
export declare const DFT_COMPLEX_OUTPUT: DftFlags;
/**
 * performs an inverse transformation of a 1D or 2D complex array; the result is normally a complex
 * array of the same size, however, if the input array has conjugate-complex symmetry (for example, it
 * is a result of forward transformation with DFT_COMPLEX_OUTPUT flag), the output is a real array;
 * while the function itself does not check whether the input is symmetrical or not, you can pass the
 * flag and then the function will assume the symmetry and produce the real output array (note that
 * when the input is packed into a real array and inverse transformation is executed, the function
 * treats the input as a packed complex-conjugate symmetrical array, and the output will also be a real
 * array).
 *
 */
export declare const DFT_REAL_OUTPUT: DftFlags;
/**
 * specifies that input is complex input. If this flag is set, the input must have 2 channels. On the
 * other hand, for backwards compatibility reason, if input has 2 channels, input is already considered
 * complex.
 *
 */
export declare const DFT_COMPLEX_INPUT: DftFlags;
/**
 * performs an inverse 1D or 2D transform instead of the default forward transform.
 *
 */
export declare const DCT_INVERSE: DftFlags;
/**
 * performs a forward or inverse transform of every individual row of the input matrix. This flag
 * enables you to transform multiple vectors simultaneously and can be used to decrease the overhead
 * (which is sometimes several times larger than the processing itself) to perform 3D and
 * higher-dimensional transforms and so forth.
 *
 */
export declare const DCT_ROWS: DftFlags;
export declare const GEMM_1_T: GemmFlags;
export declare const GEMM_2_T: GemmFlags;
export declare const GEMM_3_T: GemmFlags;
/**
 * `\\[ norm = \\forkthree {\\|\\texttt{src1}\\|_{L_{\\infty}} = \\max _I | \\texttt{src1} (I)|}{if
 * \\(\\texttt{normType} = \\texttt{NORM_INF}\\) } {\\|\\texttt{src1}-\\texttt{src2}\\|_{L_{\\infty}} =
 * \\max _I | \\texttt{src1} (I) - \\texttt{src2} (I)|}{if \\(\\texttt{normType} =
 * \\texttt{NORM_INF}\\) } {\\frac{\\|\\texttt{src1}-\\texttt{src2}\\|_{L_{\\infty}}
 * }{\\|\\texttt{src2}\\|_{L_{\\infty}} }}{if \\(\\texttt{normType} = \\texttt{NORM_RELATIVE |
 * NORM_INF}\\) } \\]`
 *
 */
export declare const NORM_INF: NormTypes;
/**
 * `\\[ norm = \\forkthree {\\| \\texttt{src1} \\| _{L_1} = \\sum _I | \\texttt{src1} (I)|}{if
 * \\(\\texttt{normType} = \\texttt{NORM_L1}\\)} { \\| \\texttt{src1} - \\texttt{src2} \\| _{L_1} =
 * \\sum _I | \\texttt{src1} (I) - \\texttt{src2} (I)|}{if \\(\\texttt{normType} = \\texttt{NORM_L1}\\)
 * } { \\frac{\\|\\texttt{src1}-\\texttt{src2}\\|_{L_1} }{\\|\\texttt{src2}\\|_{L_1}} }{if
 * \\(\\texttt{normType} = \\texttt{NORM_RELATIVE | NORM_L1}\\) } \\]`
 *
 */
export declare const NORM_L1: NormTypes;
/**
 * `\\[ norm = \\forkthree { \\| \\texttt{src1} \\| _{L_2} = \\sqrt{\\sum_I \\texttt{src1}(I)^2} }{if
 * \\(\\texttt{normType} = \\texttt{NORM_L2}\\) } { \\| \\texttt{src1} - \\texttt{src2} \\| _{L_2} =
 * \\sqrt{\\sum_I (\\texttt{src1}(I) - \\texttt{src2}(I))^2} }{if \\(\\texttt{normType} =
 * \\texttt{NORM_L2}\\) } { \\frac{\\|\\texttt{src1}-\\texttt{src2}\\|_{L_2}
 * }{\\|\\texttt{src2}\\|_{L_2}} }{if \\(\\texttt{normType} = \\texttt{NORM_RELATIVE | NORM_L2}\\) }
 * \\]`
 *
 */
export declare const NORM_L2: NormTypes;
/**
 * `\\[ norm = \\forkthree { \\| \\texttt{src1} \\| _{L_2} ^{2} = \\sum_I \\texttt{src1}(I)^2} {if
 * \\(\\texttt{normType} = \\texttt{NORM_L2SQR}\\)} { \\| \\texttt{src1} - \\texttt{src2} \\| _{L_2}
 * ^{2} = \\sum_I (\\texttt{src1}(I) - \\texttt{src2}(I))^2 }{if \\(\\texttt{normType} =
 * \\texttt{NORM_L2SQR}\\) } { \\left(\\frac{\\|\\texttt{src1}-\\texttt{src2}\\|_{L_2}
 * }{\\|\\texttt{src2}\\|_{L_2}}\\right)^2 }{if \\(\\texttt{normType} = \\texttt{NORM_RELATIVE |
 * NORM_L2SQR}\\) } \\]`
 *
 */
export declare const NORM_L2SQR: NormTypes;
/**
 * In the case of one input array, calculates the [Hamming] distance of the array from zero, In the
 * case of two input arrays, calculates the [Hamming] distance between the arrays.
 *
 */
export declare const NORM_HAMMING: NormTypes;
/**
 * Similar to NORM_HAMMING, but in the calculation, each two bits of the input sequence will be added
 * and treated as a single bit to be used in the same calculation as NORM_HAMMING.
 *
 */
export declare const NORM_HAMMING2: NormTypes;
export declare const NORM_TYPE_MASK: NormTypes;
export declare const NORM_RELATIVE: NormTypes;
export declare const NORM_MINMAX: NormTypes;
export declare const ROTATE_90_CLOCKWISE: RotateFlags;
export declare const ROTATE_180: RotateFlags;
export declare const ROTATE_90_COUNTERCLOCKWISE: RotateFlags;
/**
 * Various border types, image boundaries are denoted with `|`
 *
 * [borderInterpolate], [copyMakeBorder]
 *
 */
export type BorderTypes = any;
/**
 * Various border types, image boundaries are denoted with `|`
 *
 * [borderInterpolate], [copyMakeBorder]
 *
 */
export type CmpTypes = any;
/**
 * Various border types, image boundaries are denoted with `|`
 *
 * [borderInterpolate], [copyMakeBorder]
 *
 */
export type DecompTypes = any;
/**
 * Various border types, image boundaries are denoted with `|`
 *
 * [borderInterpolate], [copyMakeBorder]
 *
 */
export type DftFlags = any;
/**
 * Various border types, image boundaries are denoted with `|`
 *
 * [borderInterpolate], [copyMakeBorder]
 *
 */
export type GemmFlags = any;
/**
 * Various border types, image boundaries are denoted with `|`
 *
 * [borderInterpolate], [copyMakeBorder]
 *
 */
export type NormTypes = any;
/**
 * Various border types, image boundaries are denoted with `|`
 *
 * [borderInterpolate], [copyMakeBorder]
 *
 */
export type RotateFlags = any;
/**
 * The function kmeans implements a k-means algorithm that finds the centers of cluster_count clusters
 * and groups the input samples around the clusters. As an output, `$\\texttt{bestLabels}_i$` contains
 * a 0-based cluster index for the sample stored in the `$i^{th}$` row of the samples matrix.
 *
 * (Python) An example on K-means clustering can be found at
 * opencv_source_code/samples/python/kmeans.py
 *
 * The function returns the compactness measure that is computed as `\\[\\sum _i \\| \\texttt{samples}
 * _i - \\texttt{centers} _{ \\texttt{labels} _i} \\| ^2\\]` after every attempt. The best (minimum)
 * value is chosen and the corresponding labels and the compactness value are returned by the function.
 * Basically, you can use only the core of the function, set the number of attempts to 1, initialize
 * labels each time using a custom algorithm, pass them with the ( flags = [KMEANS_USE_INITIAL_LABELS]
 * ) flag, and then choose the best (most-compact) clustering.
 *
 * @param data Data for clustering. An array of N-Dimensional points with float coordinates is needed.
 * Examples of this array can be:
 * Mat points(count, 2, CV_32F);Mat points(count, 1, CV_32FC2);Mat points(1, count,
 * CV_32FC2);std::vector<cv::Point2f> points(sampleCount);
 *
 * @param K Number of clusters to split the set by.
 *
 * @param bestLabels Input/output integer array that stores the cluster indices for every sample.
 *
 * @param criteria The algorithm termination criteria, that is, the maximum number of iterations and/or
 * the desired accuracy. The accuracy is specified as criteria.epsilon. As soon as each of the cluster
 * centers moves by less than criteria.epsilon on some iteration, the algorithm stops.
 *
 * @param attempts Flag to specify the number of times the algorithm is executed using different
 * initial labellings. The algorithm returns the labels that yield the best compactness (see the last
 * function parameter).
 *
 * @param flags Flag that can take values of cv::KmeansFlags
 *
 * @param centers Output matrix of the cluster centers, one row per each cluster center.
 */
export declare function kmeans(data: Mat, K: int, bestLabels: Mat, criteria: TermCriteria, attempts: int, flags: int, centers?: Mat): double;
/**
 * The generic function partition implements an `$O(N^2)$` algorithm for splitting a set of `$N$`
 * elements into one or more equivalency classes, as described in  . The function returns the number of
 * equivalency classes.
 *
 * @param _vec Set of elements stored as a vector.
 *
 * @param labels Output vector of labels. It contains as many elements as vec. Each label labels[i] is
 * a 0-based cluster index of vec[i].
 *
 * @param predicate Equivalence predicate (pointer to a boolean function of two arguments or an
 * instance of the class that has the method bool operator()(const _Tp& a, const _Tp& b) ). The
 * predicate returns true when the elements are certainly in the same class, and returns false if they
 * may or may not be in the same class.
 */
export declare function partition(arg119: any, arg120: any, _vec: any, labels: any, predicate?: _EqPredicate): any;
/**
 * @param context pointer to context storing all necessary data
 *
 * @param src_data source image data and step
 *
 * @param dst_data destination image data and step
 */
export declare function hal_ni_dct2D(context: cvhalDFT, src_data: uchar, src_step: size_t, dst_data: uchar, dst_step: size_t): cvhalDFT;
/**
 * @param context pointer to context storing all necessary data
 */
export declare function hal_ni_dctFree2D(context: cvhalDFT): cvhalDFT;
/**
 * @param context double pointer to context storing all necessary data
 *
 * @param width image dimensions
 *
 * @param depth image type (CV_32F or CV64F)
 *
 * @param flags algorithm options (combination of CV_HAL_DFT_INVERSE, ...)
 */
export declare function hal_ni_dctInit2D(context: cvhalDFT, width: int, height: int, depth: int, flags: int): cvhalDFT;
/**
 * @param context pointer to context storing all necessary data
 *
 * @param src source data
 *
 * @param dst destination data
 */
export declare function hal_ni_dft1D(context: cvhalDFT, src: uchar, dst: uchar): cvhalDFT;
/**
 * @param context pointer to context storing all necessary data
 *
 * @param src_data source image data and step
 *
 * @param dst_data destination image data and step
 */
export declare function hal_ni_dft2D(context: cvhalDFT, src_data: uchar, src_step: size_t, dst_data: uchar, dst_step: size_t): cvhalDFT;
/**
 * @param context pointer to context storing all necessary data
 */
export declare function hal_ni_dftFree1D(context: cvhalDFT): cvhalDFT;
/**
 * @param context pointer to context storing all necessary data
 */
export declare function hal_ni_dftFree2D(context: cvhalDFT): cvhalDFT;
/**
 * @param context double pointer to context storing all necessary data
 *
 * @param len transformed array length
 *
 * @param count estimated transformation count
 *
 * @param depth array type (CV_32F or CV_64F)
 *
 * @param flags algorithm options (combination of CV_HAL_DFT_INVERSE, CV_HAL_DFT_SCALE, ...)
 *
 * @param needBuffer pointer to boolean variable, if valid pointer provided, then variable value should
 * be set to true to signal that additional memory buffer is needed for operations
 */
export declare function hal_ni_dftInit1D(context: cvhalDFT, len: int, count: int, depth: int, flags: int, needBuffer: any): cvhalDFT;
/**
 * @param context double pointer to context storing all necessary data
 *
 * @param width image dimensions
 *
 * @param depth image type (CV_32F or CV64F)
 *
 * @param src_channels number of channels in input image
 *
 * @param dst_channels number of channels in output image
 *
 * @param flags algorithm options (combination of CV_HAL_DFT_INVERSE, ...)
 *
 * @param nonzero_rows number of nonzero rows in image, can be used for optimization
 */
export declare function hal_ni_dftInit2D(context: cvhalDFT, width: int, height: int, depth: int, src_channels: int, dst_channels: int, flags: int, nonzero_rows: int): cvhalDFT;
/**
 * @param src_data Source image
 *
 * @param width Source image dimensions
 *
 * @param depth Depth of source image
 *
 * @param minVal Pointer to the returned global minimum and maximum in an array.
 *
 * @param minIdx Pointer to the returned minimum and maximum location.
 *
 * @param mask Specified array region.
 */
export declare function hal_ni_minMaxIdx(src_data: uchar, src_step: size_t, width: int, height: int, depth: int, minVal: any, maxVal: any, minIdx: any, maxIdx: any, mask: uchar): uchar;
/**
 * The function returns the aligned pointer of the same type as the input pointer:
 * `\\[\\texttt{(_Tp*)(((size_t)ptr + n-1) & -n)}\\]`
 *
 * @param ptr Aligned pointer.
 *
 * @param n Alignment size that must be a power of two.
 */
export declare function alignPtr(arg92: any, ptr: any, n?: int): any;
/**
 * The function returns the minimum number that is greater than or equal to sz and is divisible by n :
 * `\\[\\texttt{(sz + n-1) & -n}\\]`
 *
 * @param sz Buffer size to align.
 *
 * @param n Alignment size that must be a power of two.
 */
export declare function alignSize(sz: size_t, n: int): size_t;
/**
 * The function returns true if the host hardware supports the specified feature. When user calls
 * setUseOptimized(false), the subsequent calls to [checkHardwareSupport()] will return false until
 * setUseOptimized(true) is called. This way user can dynamically switch on and off the optimized code
 * in OpenCV.
 *
 * @param feature The feature of interest, one of cv::CpuFeatures
 */
export declare function checkHardwareSupport(feature: int): bool;
/**
 * proxy for hal::Cholesky
 */
export declare function Cholesky(A: any, astep: size_t, m: int, b: any, bstep: size_t, n: int): bool;
/**
 * proxy for hal::Cholesky
 */
export declare function Cholesky(A: any, astep: size_t, m: int, b: any, bstep: size_t, n: int): bool;
/**
 * The function cubeRoot computes `$\\sqrt[3]{\\texttt{val}}$`. Negative arguments are handled
 * correctly. NaN and Inf are not handled. The accuracy approaches the maximum possible accuracy for
 * single-precision data.
 *
 * @param val A function argument.
 */
export declare function cubeRoot(val: float): float;
export declare function cv_abs(arg93: any, x: _Tp): any;
export declare function cv_abs(x: uchar): uchar;
export declare function cv_abs(x: schar): schar;
export declare function cv_abs(x: ushort): ushort;
export declare function cv_abs(x: short): int;
export declare function CV_XADD(addr: any, delta: int): any;
/**
 * The function computes an integer i such that: `\\[i \\le \\texttt{value} < i+1\\]`
 *
 * @param value floating-point number. If the value is outside of INT_MIN ... INT_MAX range, the result
 * is not defined.
 */
export declare function cvCeil(value: double): int;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function cvCeil(value: float): int;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function cvCeil(value: int): int;
/**
 * The function computes an integer i such that: `\\[i \\le \\texttt{value} < i+1\\]`
 *
 * @param value floating-point number. If the value is outside of INT_MIN ... INT_MAX range, the result
 * is not defined.
 */
export declare function cvFloor(value: double): int;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function cvFloor(value: float): int;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function cvFloor(value: int): int;
/**
 * The function returns 1 if the argument is a plus or minus infinity (as defined by IEEE754 standard)
 * and 0 otherwise.
 *
 * @param value The input floating-point value
 */
export declare function cvIsInf(value: double): int;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function cvIsInf(value: float): int;
/**
 * The function returns 1 if the argument is Not A Number (as defined by IEEE754 standard), 0
 * otherwise.
 *
 * @param value The input floating-point value
 */
export declare function cvIsNaN(value: double): int;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function cvIsNaN(value: float): int;
/**
 * @param value floating-point number. If the value is outside of INT_MIN ... INT_MAX range, the result
 * is not defined.
 */
export declare function cvRound(value: double): int;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function cvRound(value: float): int;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function cvRound(value: int): int;
/**
 * Use this function instead of `ceil((float)a / b)` expressions.
 *
 * [alignSize]
 */
export declare function divUp(a: int, b: any): int;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function divUp(a: size_t, b: any): size_t;
export declare function dumpInputArray(argument: Mat): String;
export declare function dumpInputArrayOfArrays(argument: MatVector): String;
export declare function dumpInputOutputArray(argument: Mat): String;
export declare function dumpInputOutputArrayOfArrays(argument: MatVector): String;
/**
 * By default the function prints information about the error to stderr, then it either stops if
 * [cv::setBreakOnError()] had been called before or raises the exception. It is possible to alternate
 * error processing by using [redirectError()].
 *
 * @param exc the exception raisen.
 */
export declare function error(exc: any): void;
/**
 * By default the function prints information about the error to stderr, then it either stops if
 * [setBreakOnError()] had been called before or raises the exception. It is possible to alternate
 * error processing by using [redirectError()].
 *
 * [CV_Error], [CV_Error_], [CV_Assert], [CV_DbgAssert]
 *
 * @param _code - error code (Error::Code)
 *
 * @param _err - error description
 *
 * @param _func - function name. Available only when the compiler supports getting it
 *
 * @param _file - source file name where the error has occurred
 *
 * @param _line - line number in the source file where the error has occurred
 */
export declare function error(_code: int, _err: any, _func: any, _file: any, _line: int): void;
/**
 * The function fastAtan2 calculates the full-range angle of an input 2D vector. The angle is measured
 * in degrees and varies from 0 to 360 degrees. The accuracy is about 0.3 degrees.
 *
 * @param y y-coordinate of the vector.
 *
 * @param x x-coordinate of the vector.
 */
export declare function fastAtan2(y: float, x: float): float;
/**
 * The function deallocates the buffer allocated with fastMalloc . If NULL pointer is passed, the
 * function does nothing. C version of the function clears the pointer *pptr* to avoid problems with
 * double memory deallocation.
 *
 * @param ptr Pointer to the allocated buffer.
 */
export declare function fastFree(ptr: any): void;
/**
 * The function allocates the buffer of the specified size and returns it. When the buffer size is 16
 * bytes or more, the returned buffer is aligned to 16 bytes.
 *
 * @param bufSize Allocated buffer size.
 */
export declare function fastMalloc(bufSize: size_t): any;
export declare function forEach_impl(arg94: any, arg95: any, operation: any): any;
/**
 * Returned value is raw cmake output including version control system revision, compiler version,
 * compiler flags, enabled modules and third party libraries, etc. Output format depends on target
 * architecture.
 */
export declare function getBuildInformation(): any;
/**
 * Returned value is a string containing space separated list of CPU features with following markers:
 *
 * no markers - baseline features
 * prefix `*` - features enabled in dispatcher
 * suffix `?` - features enabled but not available in HW
 *
 * Example: `SSE SSE2 SSE3 *SSE4.1 *SSE4.2 *FP16 *AVX *AVX2 *AVX512-SKX?`
 */
export declare function getCPUFeaturesLine(): any;
/**
 * The function returns the current number of CPU ticks on some architectures (such as x86, x64,
 * PowerPC). On other platforms the function is equivalent to getTickCount. It can also be used for
 * very accurate time measurements, as well as for [RNG] initialization. Note that in case of multi-CPU
 * systems a thread, from which getCPUTickCount is called, can be suspended and resumed at another CPU
 * with its own counter. So, theoretically (and practically) the subsequent calls to the function do
 * not necessary return the monotonously increasing values. Also, since a modern CPU varies the CPU
 * frequency depending on the load, the number of CPU clocks spent in some code cannot be directly
 * converted to time units. Therefore, getTickCount is generally a preferable solution for measuring
 * execution time.
 */
export declare function getCPUTickCount(): int64;
export declare function getElemSize(type: int): size_t;
/**
 * Returns empty string if feature is not defined
 */
export declare function getHardwareFeatureName(feature: int): String;
export declare function getNumberOfCPUs(): int;
/**
 * Always returns 1 if OpenCV is built without threading support.
 *
 * The exact meaning of return value depends on the threading framework used by OpenCV library:
 *
 * `TBB` - The number of threads, that OpenCV will try to use for parallel regions. If there is any
 * tbb::thread_scheduler_init in user code conflicting with OpenCV, then function returns default
 * number of threads used by TBB library.
 * `OpenMP` - An upper bound on the number of threads that could be used to form a new team.
 * `Concurrency` - The number of threads, that OpenCV will try to use for parallel regions.
 * `GCD` - Unsupported; returns the GCD thread pool limit (512) for compatibility.
 * `C=` - The number of threads, that OpenCV will try to use for parallel regions, if before called
 * setNumThreads with threads > 0, otherwise returns the number of logical CPUs, available for the
 * process.
 *
 * [setNumThreads], [getThreadNum]
 */
export declare function getNumThreads(): int;
/**
 * The exact meaning of the return value depends on the threading framework used by OpenCV library:
 *
 * `TBB` - Unsupported with current 4.1 TBB release. Maybe will be supported in future.
 * `OpenMP` - The thread number, within the current team, of the calling thread.
 * `Concurrency` - An ID for the virtual processor that the current context is executing on (0 for
 * master thread and unique number for others, but not necessary 1,2,3,...).
 * `GCD` - System calling thread's ID. Never returns 0 inside parallel region.
 * `C=` - The index of the current parallel task.
 *
 * [setNumThreads], [getNumThreads]
 */
export declare function getThreadNum(): int;
/**
 * The function returns the number of ticks after the certain event (for example, when the machine was
 * turned on). It can be used to initialize [RNG] or to measure a function execution time by reading
 * the tick count before and after the function call.
 *
 * [getTickFrequency], [TickMeter]
 */
export declare function getTickCount(): int64;
/**
 * The function returns the number of ticks per second. That is, the following code computes the
 * execution time in seconds:
 *
 * ```cpp
 * double t = (double)getTickCount();
 * // do something ...
 * t = ((double)getTickCount() - t)/getTickFrequency();
 * ```
 *
 * [getTickCount], [TickMeter]
 */
export declare function getTickFrequency(): double;
export declare function getVersionMajor(): int;
export declare function getVersionMinor(): int;
export declare function getVersionRevision(): int;
/**
 * For example "3.4.1-dev".
 *
 * getMajorVersion, getMinorVersion, getRevisionVersion
 */
export declare function getVersionString(): String;
export declare function glob(pattern: String, result: any, recursive?: bool): void;
/**
 * proxy for hal::LU
 */
export declare function LU(A: any, astep: size_t, m: int, b: any, bstep: size_t, n: int): int;
/**
 * proxy for hal::LU
 */
export declare function LU(A: any, astep: size_t, m: int, b: any, bstep: size_t, n: int): int;
export declare function normInf(arg96: any, arg97: any, a: any, n: int): any;
export declare function normInf(arg98: any, arg99: any, a: any, b: any, n: int): any;
export declare function normL1(arg100: any, arg101: any, a: any, n: int): any;
export declare function normL1(arg102: any, arg103: any, a: any, b: any, n: int): any;
export declare function normL1(a: any, b: any, n: int): float;
export declare function normL1(a: uchar, b: uchar, n: int): uchar;
export declare function normL2Sqr(arg104: any, arg105: any, a: any, n: int): any;
export declare function normL2Sqr(arg106: any, arg107: any, a: any, b: any, n: int): any;
export declare function normL2Sqr(a: any, b: any, n: int): float;
export declare function parallel_for_(range: any, body: any, nstripes?: double): void;
export declare function parallel_for_(range: any, functor: any, nstripes?: double): void;
/**
 * The function sets the new error handler, called from [cv::error()].
 *
 * the previous error handler
 *
 * @param errCallback the new error handler. If NULL, the default error handler is used.
 *
 * @param userdata the optional user data pointer, passed to the callback.
 *
 * @param prevUserdata the optional output parameter where the previous user data pointer is stored
 */
export declare function redirectError(errCallback: ErrorCallback$1, userdata?: any, prevUserdata?: any): ErrorCallback$1;
/**
 * Use this function instead of `ceil((float)a / b) * b` expressions.
 *
 * [divUp]
 */
export declare function roundUp(a: int, b: any): int;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function roundUp(a: size_t, b: any): size_t;
/**
 * The function saturate_cast resembles the standard C++ cast operations, such as static_cast<T>() and
 * others. It perform an efficient and accurate conversion from one primitive type to another (see the
 * introduction chapter). saturate in the name means that when the input value v is out of the range of
 * the target type, the result is not formed just by taking low bits of the input, but instead the
 * value is clipped. For example:
 *
 * ```cpp
 * uchar a = saturate_cast<uchar>(-100); // a = 0 (UCHAR_MIN)
 * short b = saturate_cast<short>(33333.33333); // b = 32767 (SHRT_MAX)
 * ```
 *
 *  Such clipping is done when the target type is unsigned char , signed char , unsigned short or
 * signed short . For 32-bit integers, no clipping is done.
 *
 * When the parameter is a floating-point value and the target type is an integer (8-, 16- or 32-bit),
 * the floating-point value is first rounded to the nearest integer and then clipped if needed (when
 * the target type is 8- or 16-bit).
 *
 * This operation is used in the simplest or most complex image processing functions in OpenCV.
 *
 * [add], [subtract], [multiply], [divide], [Mat::convertTo]
 *
 * @param v Function parameter.
 */
export declare function saturate_cast(arg108: any, v: uchar): uchar;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function saturate_cast(arg109: any, v: schar): schar;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function saturate_cast(arg110: any, v: ushort): ushort;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function saturate_cast(arg111: any, v: short): any;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function saturate_cast(arg112: any, v: unsigned): any;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function saturate_cast(arg113: any, v: int): any;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function saturate_cast(arg114: any, v: float): any;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function saturate_cast(arg115: any, v: double): any;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function saturate_cast(arg116: any, v: int64): int64;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function saturate_cast(arg117: any, v: uint64): uint64;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function saturate_cast(arg118: any, v: float16_t): any;
/**
 * When the break-on-error mode is set, the default error handler issues a hardware exception, which
 * can make debugging more convenient.
 *
 * the previous state
 */
export declare function setBreakOnError(flag: bool): bool;
/**
 * If threads == 0, OpenCV will disable threading optimizations and run all it's functions
 * sequentially. Passing threads < 0 will reset threads number to system default. This function must be
 * called outside of parallel region.
 *
 * OpenCV will try to run its functions with specified threads number, but some behaviour differs from
 * framework:
 *
 * `TBB` - User-defined parallel constructions will run with the same threads number, if another is not
 * specified. If later on user creates his own scheduler, OpenCV will use it.
 * `OpenMP` - No special defined behaviour.
 * `Concurrency` - If threads == 1, OpenCV will disable threading optimizations and run its functions
 * sequentially.
 * `GCD` - Supports only values <= 0.
 * `C=` - No special defined behaviour.
 *
 * [getNumThreads], [getThreadNum]
 *
 * @param nthreads Number of threads used by OpenCV.
 */
export declare function setNumThreads(nthreads: int): void;
/**
 * The function can be used to dynamically turn on and off optimized dispatched code (code that uses
 * SSE4.2, AVX/AVX2, and other instructions on the platforms that support it). It sets a global flag
 * that is further checked by OpenCV functions. Since the flag is not checked in the inner OpenCV
 * loops, it is only safe to call the function on the very top level in your application where you can
 * be sure that no other OpenCV function is currently executed.
 *
 * By default, the optimized code is enabled unless you disable it in CMake. The current status can be
 * retrieved using useOptimized.
 *
 * @param onoff The boolean flag specifying whether the optimized code should be used (onoff=true) or
 * not (onoff=false).
 */
export declare function setUseOptimized(onoff: bool): void;
export declare function tempfile(suffix?: any): String;
export declare function testAsyncArray(argument: Mat): AsyncArray;
export declare function testAsyncException(): AsyncArray;
/**
 * The function returns true if the optimized code is enabled. Otherwise, it returns false.
 */
export declare function useOptimized(): bool;
export declare const CPU_MMX: CpuFeatures;
export declare const CPU_SSE: CpuFeatures;
export declare const CPU_SSE2: CpuFeatures;
export declare const CPU_SSE3: CpuFeatures;
export declare const CPU_SSSE3: CpuFeatures;
export declare const CPU_SSE4_1: CpuFeatures;
export declare const CPU_SSE4_2: CpuFeatures;
export declare const CPU_POPCNT: CpuFeatures;
export declare const CPU_FP16: CpuFeatures;
export declare const CPU_AVX: CpuFeatures;
export declare const CPU_AVX2: CpuFeatures;
export declare const CPU_FMA3: CpuFeatures;
export declare const CPU_AVX_512F: CpuFeatures;
export declare const CPU_AVX_512BW: CpuFeatures;
export declare const CPU_AVX_512CD: CpuFeatures;
export declare const CPU_AVX_512DQ: CpuFeatures;
export declare const CPU_AVX_512ER: CpuFeatures;
export declare const CPU_AVX_512IFMA512: CpuFeatures;
export declare const CPU_AVX_512IFMA: CpuFeatures;
export declare const CPU_AVX_512PF: CpuFeatures;
export declare const CPU_AVX_512VBMI: CpuFeatures;
export declare const CPU_AVX_512VL: CpuFeatures;
export declare const CPU_AVX_512VBMI2: CpuFeatures;
export declare const CPU_AVX_512VNNI: CpuFeatures;
export declare const CPU_AVX_512BITALG: CpuFeatures;
export declare const CPU_AVX_512VPOPCNTDQ: CpuFeatures;
export declare const CPU_AVX_5124VNNIW: CpuFeatures;
export declare const CPU_AVX_5124FMAPS: CpuFeatures;
export declare const CPU_NEON: CpuFeatures;
export declare const CPU_VSX: CpuFeatures;
export declare const CPU_VSX3: CpuFeatures;
export declare const CPU_AVX512_SKX: CpuFeatures;
export declare const CPU_AVX512_COMMON: CpuFeatures;
export declare const CPU_AVX512_KNL: CpuFeatures;
export declare const CPU_AVX512_KNM: CpuFeatures;
export declare const CPU_AVX512_CNL: CpuFeatures;
export declare const CPU_AVX512_CEL: CpuFeatures;
export declare const CPU_AVX512_ICL: CpuFeatures;
export declare const CPU_MAX_FEATURE: CpuFeatures;
export declare const SORT_EVERY_ROW: SortFlags;
/**
 * each matrix column is sorted independently; this flag and the previous one are mutually exclusive.
 *
 */
export declare const SORT_EVERY_COLUMN: SortFlags;
/**
 * each matrix row is sorted in the ascending order.
 *
 */
export declare const SORT_ASCENDING: SortFlags;
/**
 * each matrix row is sorted in the descending order; this flag and the previous one are also mutually
 * exclusive.
 *
 */
export declare const SORT_DESCENDING: SortFlags;
export type CpuFeatures = any;
export type SortFlags = any;
/**
 * It has two groups of match methods: for matching descriptors of an image with another image or with
 * an image set.
 *
 * Source:
 * [opencv2/features2d.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/features2d.hpp#L860).
 *
 */
export declare class DescriptorMatcher extends Algorithm$1 {
	/**
	 *   If the collection is not empty, the new descriptors are added to existing train descriptors.
	 *
	 * @param descriptors Descriptors to add. Each descriptors[i] is a set of descriptors from the same
	 * train image.
	 */
	add(descriptors: MatVector): MatVector;
	clear(): void;
	/**
	 * @param emptyTrainData If emptyTrainData is false, the method creates a deep copy of the object,
	 * that is, copies both parameters and train data. If emptyTrainData is true, the method creates an
	 * object copy with the current parameters but with empty train data.
	 */
	clone(emptyTrainData?: bool): Ptr;
	empty(): bool;
	getTrainDescriptors(): Mat;
	isMaskSupported(): bool;
	/**
	 *   These extended variants of [DescriptorMatcher::match] methods find several best matches for each
	 * query descriptor. The matches are returned in the distance increasing order. See
	 * [DescriptorMatcher::match] for the details about query and train descriptors.
	 *
	 * @param queryDescriptors Query set of descriptors.
	 *
	 * @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors
	 * collection stored in the class object.
	 *
	 * @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.
	 *
	 * @param k Count of best matches found per each query descriptor or less if a query descriptor has
	 * less than k possible matches in total.
	 *
	 * @param mask Mask specifying permissible matches between an input query and train matrices of
	 * descriptors.
	 *
	 * @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is
	 * false, the matches vector has the same size as queryDescriptors rows. If compactResult is true, the
	 * matches vector does not contain matches for fully masked-out query descriptors.
	 */
	knnMatch(queryDescriptors: Mat, trainDescriptors: Mat, matches: any, k: int, mask?: Mat, compactResult?: bool): Mat;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param queryDescriptors Query set of descriptors.
	 *
	 * @param matches Matches. Each matches[i] is k or less matches for the same query descriptor.
	 *
	 * @param k Count of best matches found per each query descriptor or less if a query descriptor has
	 * less than k possible matches in total.
	 *
	 * @param masks Set of masks. Each masks[i] specifies permissible matches between the input query
	 * descriptors and stored train descriptors from the i-th image trainDescCollection[i].
	 *
	 * @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is
	 * false, the matches vector has the same size as queryDescriptors rows. If compactResult is true, the
	 * matches vector does not contain matches for fully masked-out query descriptors.
	 */
	knnMatch(queryDescriptors: Mat, matches: any, k: int, masks?: MatVector, compactResult?: bool): Mat;
	/**
	 *   In the first variant of this method, the train descriptors are passed as an input argument. In the
	 * second variant of the method, train descriptors collection that was set by [DescriptorMatcher::add]
	 * is used. Optional mask (or masks) can be passed to specify which query and training descriptors can
	 * be matched. Namely, queryDescriptors[i] can be matched with trainDescriptors[j] only if
	 * mask.at<uchar>(i,j) is non-zero.
	 *
	 * @param queryDescriptors Query set of descriptors.
	 *
	 * @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors
	 * collection stored in the class object.
	 *
	 * @param matches Matches. If a query descriptor is masked out in mask , no match is added for this
	 * descriptor. So, matches size may be smaller than the query descriptors count.
	 *
	 * @param mask Mask specifying permissible matches between an input query and train matrices of
	 * descriptors.
	 */
	match(queryDescriptors: Mat, trainDescriptors: Mat, matches: any, mask?: Mat): Mat;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param queryDescriptors Query set of descriptors.
	 *
	 * @param matches Matches. If a query descriptor is masked out in mask , no match is added for this
	 * descriptor. So, matches size may be smaller than the query descriptors count.
	 *
	 * @param masks Set of masks. Each masks[i] specifies permissible matches between the input query
	 * descriptors and stored train descriptors from the i-th image trainDescCollection[i].
	 */
	match(queryDescriptors: Mat, matches: any, masks?: MatVector): Mat;
	/**
	 *   For each query descriptor, the methods find such training descriptors that the distance between
	 * the query descriptor and the training descriptor is equal or smaller than maxDistance. Found matches
	 * are returned in the distance increasing order.
	 *
	 * @param queryDescriptors Query set of descriptors.
	 *
	 * @param trainDescriptors Train set of descriptors. This set is not added to the train descriptors
	 * collection stored in the class object.
	 *
	 * @param matches Found matches.
	 *
	 * @param maxDistance Threshold for the distance between matched descriptors. Distance means here
	 * metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured in
	 * Pixels)!
	 *
	 * @param mask Mask specifying permissible matches between an input query and train matrices of
	 * descriptors.
	 *
	 * @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is
	 * false, the matches vector has the same size as queryDescriptors rows. If compactResult is true, the
	 * matches vector does not contain matches for fully masked-out query descriptors.
	 */
	radiusMatch(queryDescriptors: Mat, trainDescriptors: Mat, matches: any, maxDistance: float, mask?: Mat, compactResult?: bool): Mat;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param queryDescriptors Query set of descriptors.
	 *
	 * @param matches Found matches.
	 *
	 * @param maxDistance Threshold for the distance between matched descriptors. Distance means here
	 * metric distance (e.g. Hamming distance), not the distance between coordinates (which is measured in
	 * Pixels)!
	 *
	 * @param masks Set of masks. Each masks[i] specifies permissible matches between the input query
	 * descriptors and stored train descriptors from the i-th image trainDescCollection[i].
	 *
	 * @param compactResult Parameter used when the mask (or masks) is not empty. If compactResult is
	 * false, the matches vector has the same size as queryDescriptors rows. If compactResult is true, the
	 * matches vector does not contain matches for fully masked-out query descriptors.
	 */
	radiusMatch(queryDescriptors: Mat, matches: any, maxDistance: float, masks?: MatVector, compactResult?: bool): Mat;
	read(fileName: String): String;
	read(fn: FileNode): FileNode;
	/**
	 *   Trains a descriptor matcher (for example, the flann index). In all methods to match, the method
	 * [train()] is run every time before matching. Some descriptor matchers (for example,
	 * BruteForceMatcher) have an empty implementation of this method. Other matchers really train their
	 * inner structures (for example, [FlannBasedMatcher] trains [flann::Index] ).
	 */
	train(): void;
	write(fileName: String): String;
	write(fs: FileStorage): FileStorage;
	write(fs: Ptr, name?: String): Ptr;
}
export declare const FLANNBASED: MatcherType;
export declare const BRUTEFORCE: MatcherType;
export declare const BRUTEFORCE_L1: MatcherType;
export declare const BRUTEFORCE_HAMMING: MatcherType;
export declare const BRUTEFORCE_HAMMINGLUT: MatcherType;
export declare const BRUTEFORCE_SL2: MatcherType;
export type MatcherType = any;
/**
 * if `crop` is true, input image is resized so one side after resize is equal to corresponding
 * dimension in `size` and another one is equal or larger. Then, crop from the center is performed. If
 * `crop` is false, direct resize without cropping and preserving aspect ratio is performed.
 *
 * 4-dimensional [Mat] with NCHW dimensions order.
 *
 * @param image input image (with 1-, 3- or 4-channels).
 *
 * @param scalefactor multiplier for image values.
 *
 * @param size spatial size for output image
 *
 * @param mean scalar with mean values which are subtracted from channels. Values are intended to be in
 * (mean-R, mean-G, mean-B) order if image has BGR ordering and swapRB is true.
 *
 * @param swapRB flag which indicates that swap first and last channels in 3-channel image is
 * necessary.
 *
 * @param crop flag which indicates whether image will be cropped after resize or not
 *
 * @param ddepth Depth of output blob. Choose CV_32F or CV_8U.
 */
export declare function blobFromImage(image: Mat, scalefactor?: double, size?: any, mean?: any, swapRB?: bool, crop?: bool, ddepth?: int): Mat;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function blobFromImage(image: Mat, blob: Mat, scalefactor?: double, size?: any, mean?: any, swapRB?: bool, crop?: bool, ddepth?: int): void;
/**
 * if `crop` is true, input image is resized so one side after resize is equal to corresponding
 * dimension in `size` and another one is equal or larger. Then, crop from the center is performed. If
 * `crop` is false, direct resize without cropping and preserving aspect ratio is performed.
 *
 * 4-dimensional [Mat] with NCHW dimensions order.
 *
 * @param images input images (all with 1-, 3- or 4-channels).
 *
 * @param scalefactor multiplier for images values.
 *
 * @param size spatial size for output image
 *
 * @param mean scalar with mean values which are subtracted from channels. Values are intended to be in
 * (mean-R, mean-G, mean-B) order if image has BGR ordering and swapRB is true.
 *
 * @param swapRB flag which indicates that swap first and last channels in 3-channel image is
 * necessary.
 *
 * @param crop flag which indicates whether image will be cropped after resize or not
 *
 * @param ddepth Depth of output blob. Choose CV_32F or CV_8U.
 */
export declare function blobFromImages(images: MatVector, scalefactor?: double, size?: Size, mean?: any, swapRB?: bool, crop?: bool, ddepth?: int): Mat;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function blobFromImages(images: MatVector, blob: Mat, scalefactor?: double, size?: Size, mean?: any, swapRB?: bool, crop?: bool, ddepth?: int): void;
export declare function getAvailableBackends(): any;
export declare function getAvailableTargets(be: Backend): any;
/**
 * @param blob_ 4 dimensional array (images, channels, height, width) in floating point precision
 * (CV_32F) from which you would like to extract the images.
 *
 * @param images_ array of 2D Mat containing the images extracted from the blob in floating point
 * precision (CV_32F). They are non normalized neither mean added. The number of returned images equals
 * the first dimension of the blob (batch size). Every image has a number of channels equals to the
 * second dimension of the blob (depth).
 */
export declare function imagesFromBlob(blob_: any, images_: MatVector): any;
/**
 * @param bboxes a set of bounding boxes to apply NMS.
 *
 * @param scores a set of corresponding confidences.
 *
 * @param score_threshold a threshold used to filter boxes by score.
 *
 * @param nms_threshold a threshold used in non maximum suppression.
 *
 * @param indices the kept indices of bboxes after NMS.
 *
 * @param eta a coefficient in adaptive threshold formula: $nms\_threshold_{i+1}=eta\cdot
 * nms\_threshold_i$.
 *
 * @param top_k if >0, keep at most top_k picked indices.
 */
export declare function NMSBoxes(bboxes: any, scores: any, score_threshold: any, nms_threshold: any, indices: any, eta?: any, top_k?: any): void;
export declare function NMSBoxes(bboxes: any, scores: any, score_threshold: any, nms_threshold: any, indices: any, eta?: any, top_k?: any): void;
export declare function NMSBoxes(bboxes: any, scores: any, score_threshold: any, nms_threshold: any, indices: any, eta?: any, top_k?: any): void;
/**
 * [Net] object.
 * This function automatically detects an origin framework of trained model and calls an appropriate
 * function such [readNetFromCaffe], [readNetFromTensorflow], [readNetFromTorch] or
 * [readNetFromDarknet]. An order of `model` and `config` arguments does not matter.
 *
 * @param model Binary file contains trained weights. The following file extensions are expected for
 * models from different frameworks:
 * .caffemodel (Caffe, http://caffe.berkeleyvision.org/)*.pb (TensorFlow,
 * https://www.tensorflow.org/)*.t7 | *.net (Torch, http://torch.ch/)*.weights (Darknet,
 * https://pjreddie.com/darknet/)*.bin (DLDT, https://software.intel.com/openvino-toolkit)*.onnx (ONNX,
 * https://onnx.ai/)
 *
 * @param config Text file contains network configuration. It could be a file with the following
 * extensions:
 * .prototxt (Caffe, http://caffe.berkeleyvision.org/)*.pbtxt (TensorFlow,
 * https://www.tensorflow.org/)*.cfg (Darknet, https://pjreddie.com/darknet/)*.xml (DLDT,
 * https://software.intel.com/openvino-toolkit)
 *
 * @param framework Explicit framework name tag to determine a format.
 */
export declare function readNet(model: any, config?: any, framework?: any): Net;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * [Net] object.
 *
 * @param framework Name of origin framework.
 *
 * @param bufferModel A buffer with a content of binary file with weights
 *
 * @param bufferConfig A buffer with a content of text file contains network configuration.
 */
export declare function readNet(framework: any, bufferModel: uchar, bufferConfig?: uchar): uchar;
/**
 * [Net] object.
 *
 * @param prototxt path to the .prototxt file with text description of the network architecture.
 *
 * @param caffeModel path to the .caffemodel file with learned network.
 */
export declare function readNetFromCaffe(prototxt: any, caffeModel?: any): Net;
/**
 * [Net] object.
 *
 * @param bufferProto buffer containing the content of the .prototxt file
 *
 * @param bufferModel buffer containing the content of the .caffemodel file
 */
export declare function readNetFromCaffe(bufferProto: uchar, bufferModel?: uchar): uchar;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * [Net] object.
 *
 * @param bufferProto buffer containing the content of the .prototxt file
 *
 * @param lenProto length of bufferProto
 *
 * @param bufferModel buffer containing the content of the .caffemodel file
 *
 * @param lenModel length of bufferModel
 */
export declare function readNetFromCaffe(bufferProto: any, lenProto: size_t, bufferModel?: any, lenModel?: size_t): Net;
/**
 * Network object that ready to do forward, throw an exception in failure cases.
 *
 * [Net] object.
 *
 * @param cfgFile path to the .cfg file with text description of the network architecture.
 *
 * @param darknetModel path to the .weights file with learned network.
 */
export declare function readNetFromDarknet(cfgFile: any, darknetModel?: any): Net;
/**
 * [Net] object.
 *
 * @param bufferCfg A buffer contains a content of .cfg file with text description of the network
 * architecture.
 *
 * @param bufferModel A buffer contains a content of .weights file with learned network.
 */
export declare function readNetFromDarknet(bufferCfg: uchar, bufferModel?: uchar): uchar;
/**
 * [Net] object.
 *
 * @param bufferCfg A buffer contains a content of .cfg file with text description of the network
 * architecture.
 *
 * @param lenCfg Number of bytes to read from bufferCfg
 *
 * @param bufferModel A buffer contains a content of .weights file with learned network.
 *
 * @param lenModel Number of bytes to read from bufferModel
 */
export declare function readNetFromDarknet(bufferCfg: any, lenCfg: size_t, bufferModel?: any, lenModel?: size_t): Net;
/**
 * [Net] object. Networks imported from Intel's [Model] Optimizer are launched in Intel's Inference
 * Engine backend.
 *
 * @param xml XML configuration file with network's topology.
 *
 * @param bin Binary file with trained weights.
 */
export declare function readNetFromModelOptimizer(xml: any, bin: any): Net;
/**
 * Network object that ready to do forward, throw an exception in failure cases.
 *
 * @param onnxFile path to the .onnx file with text description of the network architecture.
 */
export declare function readNetFromONNX(onnxFile: any): Net;
/**
 * Network object that ready to do forward, throw an exception in failure cases.
 *
 * @param buffer memory address of the first byte of the buffer.
 *
 * @param sizeBuffer size of the buffer.
 */
export declare function readNetFromONNX(buffer: any, sizeBuffer: size_t): Net;
/**
 * Network object that ready to do forward, throw an exception in failure cases.
 *
 * @param buffer in-memory buffer that stores the ONNX model bytes.
 */
export declare function readNetFromONNX(buffer: uchar): uchar;
/**
 * [Net] object.
 *
 * @param model path to the .pb file with binary protobuf description of the network architecture
 *
 * @param config path to the .pbtxt file that contains text graph definition in protobuf format.
 * Resulting Net object is built by text graph using weights from a binary one that let us make it more
 * flexible.
 */
export declare function readNetFromTensorflow(model: any, config?: any): Net;
/**
 * [Net] object.
 *
 * @param bufferModel buffer containing the content of the pb file
 *
 * @param bufferConfig buffer containing the content of the pbtxt file
 */
export declare function readNetFromTensorflow(bufferModel: uchar, bufferConfig?: uchar): uchar;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param bufferModel buffer containing the content of the pb file
 *
 * @param lenModel length of bufferModel
 *
 * @param bufferConfig buffer containing the content of the pbtxt file
 *
 * @param lenConfig length of bufferConfig
 */
export declare function readNetFromTensorflow(bufferModel: any, lenModel: size_t, bufferConfig?: any, lenConfig?: size_t): Net;
/**
 * [Net] object.
 *
 * Ascii mode of Torch serializer is more preferable, because binary mode extensively use `long` type
 * of C language, which has various bit-length on different systems.
 * The loading file must contain serialized  object with importing network. Try to eliminate a custom
 * objects from serialazing data to avoid importing errors.
 *
 * List of supported layers (i.e. object instances derived from Torch nn.Module class):
 *
 * nn.Sequential
 * nn.Parallel
 * nn.Concat
 * nn.Linear
 * nn.SpatialConvolution
 * nn.SpatialMaxPooling, nn.SpatialAveragePooling
 * nn.ReLU, nn.TanH, nn.Sigmoid
 * nn.Reshape
 * nn.SoftMax, nn.LogSoftMax
 *
 * Also some equivalents of these classes from cunn, cudnn, and fbcunn may be successfully imported.
 *
 * @param model path to the file, dumped from Torch by using torch.save() function.
 *
 * @param isBinary specifies whether the network was serialized in ascii mode or binary.
 *
 * @param evaluate specifies testing phase of network. If true, it's similar to evaluate() method in
 * Torch.
 */
export declare function readNetFromTorch(model: any, isBinary?: bool, evaluate?: bool): Net;
/**
 * [Mat].
 *
 * @param path to the .pb file with input tensor.
 */
export declare function readTensorFromONNX(path: any): Mat;
/**
 * This function has the same limitations as [readNetFromTorch()].
 */
export declare function readTorchBlob(filename: any, isBinary?: bool): Mat;
/**
 * Shrinked model has no origin float32 weights so it can't be used in origin Caffe framework anymore.
 * However the structure of data is taken from NVidia's Caffe fork: . So the resulting model may be
 * used there.
 *
 * @param src Path to origin model from Caffe framework contains single precision floating point
 * weights (usually has .caffemodel extension).
 *
 * @param dst Path to destination model with updated weights.
 *
 * @param layersTypes Set of layers types which parameters will be converted. By default, converts only
 * Convolutional and Fully-Connected layers' weights.
 */
export declare function shrinkCaffeModel(src: any, dst: any, layersTypes?: any): void;
/**
 * To reduce output file size, trained weights are not included.
 *
 * @param model A path to binary network.
 *
 * @param output A path to output text file to be created.
 */
export declare function writeTextGraph(model: any, output: any): void;
/**
 * DNN_BACKEND_DEFAULT equals to DNN_BACKEND_INFERENCE_ENGINE if OpenCV is built with Intel's Inference
 * Engine library or DNN_BACKEND_OPENCV otherwise.
 *
 */
export declare const DNN_BACKEND_DEFAULT: Backend;
export declare const DNN_BACKEND_HALIDE: Backend;
export declare const DNN_BACKEND_INFERENCE_ENGINE: Backend;
export declare const DNN_BACKEND_OPENCV: Backend;
export declare const DNN_BACKEND_VKCOM: Backend;
export declare const DNN_TARGET_CPU: Target;
export declare const DNN_TARGET_OPENCL: Target;
export declare const DNN_TARGET_OPENCL_FP16: Target;
export declare const DNN_TARGET_MYRIAD: Target;
export declare const DNN_TARGET_VULKAN: Target;
export declare const DNN_TARGET_FPGA: Target;
/**
 * [Net::setPreferableBackend]
 *
 */
export type Backend = any;
/**
 * [Net::setPreferableBackend]
 *
 */
export type Target = any;
/**
 * Class re-implementing the boost version of it This helps not depending on boost, it also does not do
 * the bound checks and has a way to reset a block for speed
 *
 * Source:
 * [opencv2/flann/dynamic_bitset.h](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/flann/dynamic_bitset.h#L150).
 *
 */
export declare class DynamicBitset {
	/**
	 *   default constructor
	 */
	constructor();
	/**
	 *   only constructor we use in our code
	 *
	 * @param sz the size of the bitset (in bits)
	 */
	constructor(sz: size_t);
	/**
	 *   Sets all the bits to 0
	 */
	clear(): void;
	/**
	 *   true if the bitset is empty
	 */
	empty(): bool;
	/**
	 *   set all the bits to 0
	 */
	reset(): void;
	reset(index: size_t): void;
	reset_block(index: size_t): void;
	/**
	 *   resize the bitset so that it contains at least sz bits
	 */
	resize(sz: size_t): void;
	/**
	 *   set a bit to true
	 *
	 * @param index the index of the bit to set to 1
	 */
	set(index: size_t): void;
	/**
	 *   gives the number of contained bits
	 */
	size(): size_t;
	/**
	 *   check if a bit is set
	 *
	 *   true if the bit is set
	 *
	 * @param index the index of the bit to check
	 */
	test(index: size_t): bool;
}
/**
 * This class encapsulates all or almost all necessary information about the error happened in the
 * program. The exception is usually constructed and thrown implicitly via CV_Error and CV_Error_
 * macros.
 *
 * [error](#db/de0/group__core__utils_1gacbd081fdb20423a63cf731569ba70b2b})
 *
 * Source:
 * [opencv2/core.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core.hpp#L135).
 *
 */
export declare class Exception {
	/**
	 *   CVStatus
	 *
	 */
	code: int;
	err: String;
	file: String;
	func: String;
	line: int;
	msg: String;
	/**
	 *   Default constructor
	 */
	constructor();
	/**
	 *   Full constructor. Normally the constructor is not called explicitly. Instead, the macros
	 * [CV_Error()], [CV_Error_()] and [CV_Assert()] are used.
	 */
	constructor(_code: int, _err: String, _func: String, _file: String, _line: int);
	formatMessage(): void;
	/**
	 *   the error description and the context as a text string.
	 */
	what(): any;
}
/**
 * https://docs.opencv.org/4.10.0/d0/d13/classcv_1_1Feature2D.html
 */
export declare class Feature2D extends Algorithm$1 {
	/**
	 * Detects keypoints and computes the descriptors
	 * @param img
	 * @param mask
	 * @param keypoints
	 * @param descriptors
	 */
	detectAndCompute(img: Mat, mask: Mat, keypoints: KeyPointVector, descriptors: Mat): void;
}
/**
 * For Python API, flags are modified as cv.DRAW_MATCHES_FLAGS_DEFAULT,
 * cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS, cv.DRAW_MATCHES_FLAGS_DRAW_OVER_OUTIMG,
 * cv.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS
 *
 * @param image Source image.
 *
 * @param keypoints Keypoints from the source image.
 *
 * @param outImage Output image. Its content depends on the flags value defining what is drawn in the
 * output image. See possible flags bit values below.
 *
 * @param color Color of keypoints.
 *
 * @param flags Flags setting drawing features. Possible flags bit values are defined by
 * DrawMatchesFlags. See details above in drawMatches .
 */
export declare function drawKeypoints(image: Mat, keypoints: any, outImage: Mat, color?: any, flags?: DrawMatchesFlags): void;
/**
 * This function draws matches of keypoints from two images in the output image. Match is a line
 * connecting two keypoints (circles). See [cv::DrawMatchesFlags].
 *
 * @param img1 First source image.
 *
 * @param keypoints1 Keypoints from the first source image.
 *
 * @param img2 Second source image.
 *
 * @param keypoints2 Keypoints from the second source image.
 *
 * @param matches1to2 Matches from the first image to the second one, which means that keypoints1[i]
 * has a corresponding point in keypoints2[matches[i]] .
 *
 * @param outImg Output image. Its content depends on the flags value defining what is drawn in the
 * output image. See possible flags bit values below.
 *
 * @param matchColor Color of matches (lines and connected keypoints). If matchColor==Scalar::all(-1) ,
 * the color is generated randomly.
 *
 * @param singlePointColor Color of single keypoints (circles), which means that keypoints do not have
 * the matches. If singlePointColor==Scalar::all(-1) , the color is generated randomly.
 *
 * @param matchesMask Mask determining which matches are drawn. If the mask is empty, all matches are
 * drawn.
 *
 * @param flags Flags setting drawing features. Possible flags bit values are defined by
 * DrawMatchesFlags.
 */
export declare function drawMatches(img1: Mat, keypoints1: any, img2: Mat, keypoints2: any, matches1to2: any, outImg: Mat, matchColor?: any, singlePointColor?: any, matchesMask?: any, flags?: DrawMatchesFlags): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function drawMatches(img1: Mat, keypoints1: any, img2: Mat, keypoints2: any, matches1to2: any, outImg: Mat, matchColor?: any, singlePointColor?: any, matchesMask?: any, flags?: DrawMatchesFlags): void;
/**
 * Output image matrix will be created ([Mat::create]), i.e. existing memory of output image may be
 * reused. Two source image, matches and single keypoints will be drawn. For each keypoint only the
 * center point will be drawn (without the circle around keypoint with keypoint size and orientation).
 *
 */
export declare const DEFAULT: DrawMatchesFlags;
/**
 * Output image matrix will not be created ([Mat::create]). Matches will be drawn on existing content
 * of output image.
 *
 */
export declare const DRAW_OVER_OUTIMG: DrawMatchesFlags;
export declare const NOT_DRAW_SINGLE_POINTS: DrawMatchesFlags;
/**
 * For each keypoint the circle around keypoint with keypoint size and orientation will be drawn.
 *
 */
export declare const DRAW_RICH_KEYPOINTS: DrawMatchesFlags;
export type DrawMatchesFlags = any;
/**
 * Computes the undistortion and rectification maps for the image transform using remap.
 * If D is empty, zero distortion is used. If R or P is empty, identity matrices are used.
 *
 * @param {InputArray} K - Camera intrinsic matrix.
 * @param {InputArray} D - Input vector of distortion coefficients (k1, k2, k3, k4).
 * @param {InputArray} R - Rectification transformation in the object space: 3x3 1-channel, or vector: 3x1/1x3 1-channel or 1x1 3-channel.
 * @param {InputArray} P - New camera intrinsic matrix (3x3) or new projection matrix (3x4).
 * @param {Size} size - Undistorted image size.
 * @param {int} m1type - Type of the first output map that can be CV_32FC1 or CV_16SC2. See convertMaps for details.
 * @param {OutputArray} map1 - The first output map.
 * @param {OutputArray} map2 - The second output map.
 * @return {void}
 */
export declare function fisheye_initUndistortRectifyMap(K: Mat, D: Mat, R: Mat, P: Mat, size: Size, m1type: int, map1: Mat, map2: Mat): void;
/**
 * This matcher trains [cv::flann::Index](#d1/db2/classcv_1_1flann_1_1Index}) on a train descriptor
 * collection and calls its nearest search methods to find the best matches. So, this matcher may be
 * faster when matching a large train collection than the brute force matcher.
 * [FlannBasedMatcher](#dc/de2/classcv_1_1FlannBasedMatcher}) does not support masking permissible
 * matches of descriptor sets because [flann::Index](#d1/db2/classcv_1_1flann_1_1Index}) does not
 * support this. :
 *
 * Source:
 * [opencv2/features2d.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/features2d.hpp#L1187).
 *
 */
export declare class FlannBasedMatcher extends DescriptorMatcher {
	constructor(indexParams?: Ptr, searchParams?: Ptr);
	/**
	 *   If the collection is not empty, the new descriptors are added to existing train descriptors.
	 *
	 * @param descriptors Descriptors to add. Each descriptors[i] is a set of descriptors from the same
	 * train image.
	 */
	add(descriptors: MatVector): MatVector;
	clear(): void;
	/**
	 * @param emptyTrainData If emptyTrainData is false, the method creates a deep copy of the object,
	 * that is, copies both parameters and train data. If emptyTrainData is true, the method creates an
	 * object copy with the current parameters but with empty train data.
	 */
	clone(emptyTrainData?: bool): Ptr;
	isMaskSupported(): bool;
	read(fn: FileNode): FileNode;
	/**
	 *   Trains a descriptor matcher (for example, the flann index). In all methods to match, the method
	 * [train()] is run every time before matching. Some descriptor matchers (for example,
	 * BruteForceMatcher) have an empty implementation of this method. Other matchers really train their
	 * inner structures (for example, [FlannBasedMatcher] trains [flann::Index] ).
	 */
	train(): void;
	write(fs: FileStorage): FileStorage;
	static create(): Ptr;
}
/**
 * the HOG descriptor algorithm introduced by Navneet Dalal and Bill Triggs Dalal2005 .
 *
 * useful links:
 *
 * Source:
 * [opencv2/objdetect.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/objdetect.hpp#L377).
 *
 */
export declare class HOGDescriptor {
	blockSize: Size;
	blockStride: Size;
	cellSize: Size;
	derivAperture: int;
	free_coef: float;
	gammaCorrection: bool;
	histogramNormType: any;
	L2HysThreshold: double;
	nbins: int;
	nlevels: int;
	oclSvmDetector: UMat;
	signedGradient: bool;
	svmDetector: any;
	winSigma: double;
	winSize: Size;
	/**
	 *   aqual to [HOGDescriptor](Size(64,128), Size(16,16), Size(8,8), Size(8,8), 9 )
	 */
	constructor();
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param _winSize sets winSize with given value.
	 *
	 * @param _blockSize sets blockSize with given value.
	 *
	 * @param _blockStride sets blockStride with given value.
	 *
	 * @param _cellSize sets cellSize with given value.
	 *
	 * @param _nbins sets nbins with given value.
	 *
	 * @param _derivAperture sets derivAperture with given value.
	 *
	 * @param _winSigma sets winSigma with given value.
	 *
	 * @param _histogramNormType sets histogramNormType with given value.
	 *
	 * @param _L2HysThreshold sets L2HysThreshold with given value.
	 *
	 * @param _gammaCorrection sets gammaCorrection with given value.
	 *
	 * @param _nlevels sets nlevels with given value.
	 *
	 * @param _signedGradient sets signedGradient with given value.
	 */
	constructor(_winSize: Size, _blockSize: Size, _blockStride: Size, _cellSize: Size, _nbins: int, _derivAperture?: int, _winSigma?: double, _histogramNormType?: any, _L2HysThreshold?: double, _gammaCorrection?: bool, _nlevels?: int, _signedGradient?: bool);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param filename The file name containing HOGDescriptor properties and coefficients for the linear
	 * SVM classifier.
	 */
	constructor(filename: String);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param d the HOGDescriptor which cloned to create a new one.
	 */
	constructor(d: HOGDescriptor);
	checkDetectorSize(): bool;
	/**
	 * @param img Matrix of the type CV_8U containing an image where HOG features will be calculated.
	 *
	 * @param descriptors Matrix of the type CV_32F
	 *
	 * @param winStride Window stride. It must be a multiple of block stride.
	 *
	 * @param padding Padding
	 *
	 * @param locations Vector of Point
	 */
	compute(img: Mat, descriptors: any, winStride?: Size, padding?: Size, locations?: Point): Mat;
	/**
	 * @param img Matrix contains the image to be computed
	 *
	 * @param grad Matrix of type CV_32FC2 contains computed gradients
	 *
	 * @param angleOfs Matrix of type CV_8UC2 contains quantized gradient orientations
	 *
	 * @param paddingTL Padding from top-left
	 *
	 * @param paddingBR Padding from bottom-right
	 */
	computeGradient(img: Mat, grad: Mat, angleOfs: Mat, paddingTL?: Size, paddingBR?: Size): Mat;
	/**
	 * @param c cloned HOGDescriptor
	 */
	copyTo(c: HOGDescriptor): HOGDescriptor;
	/**
	 * @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
	 *
	 * @param foundLocations Vector of point where each point contains left-top corner point of detected
	 * object boundaries.
	 *
	 * @param weights Vector that will contain confidence values for each detected object.
	 *
	 * @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually
	 * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if
	 * the free coefficient is omitted (which is allowed), you can specify it manually here.
	 *
	 * @param winStride Window stride. It must be a multiple of block stride.
	 *
	 * @param padding Padding
	 *
	 * @param searchLocations Vector of Point includes set of requested locations to be evaluated.
	 */
	detect(img: Mat, foundLocations: any, weights: any, hitThreshold?: double, winStride?: Size, padding?: Size, searchLocations?: Point): Mat;
	/**
	 * @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
	 *
	 * @param foundLocations Vector of point where each point contains left-top corner point of detected
	 * object boundaries.
	 *
	 * @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually
	 * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if
	 * the free coefficient is omitted (which is allowed), you can specify it manually here.
	 *
	 * @param winStride Window stride. It must be a multiple of block stride.
	 *
	 * @param padding Padding
	 *
	 * @param searchLocations Vector of Point includes locations to search.
	 */
	detect(img: Mat, foundLocations: any, hitThreshold?: double, winStride?: Size, padding?: Size, searchLocations?: Point): Mat;
	/**
	 * @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
	 *
	 * @param foundLocations Vector of rectangles where each rectangle contains the detected object.
	 *
	 * @param foundWeights Vector that will contain confidence values for each detected object.
	 *
	 * @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually
	 * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if
	 * the free coefficient is omitted (which is allowed), you can specify it manually here.
	 *
	 * @param winStride Window stride. It must be a multiple of block stride.
	 *
	 * @param padding Padding
	 *
	 * @param scale Coefficient of the detection window increase.
	 *
	 * @param finalThreshold Final threshold
	 *
	 * @param useMeanshiftGrouping indicates grouping algorithm
	 */
	detectMultiScale(img: Mat, foundLocations: any, foundWeights: any, hitThreshold?: double, winStride?: Size, padding?: Size, scale?: double, finalThreshold?: double, useMeanshiftGrouping?: bool): Mat;
	/**
	 * @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
	 *
	 * @param foundLocations Vector of rectangles where each rectangle contains the detected object.
	 *
	 * @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually
	 * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if
	 * the free coefficient is omitted (which is allowed), you can specify it manually here.
	 *
	 * @param winStride Window stride. It must be a multiple of block stride.
	 *
	 * @param padding Padding
	 *
	 * @param scale Coefficient of the detection window increase.
	 *
	 * @param finalThreshold Final threshold
	 *
	 * @param useMeanshiftGrouping indicates grouping algorithm
	 */
	detectMultiScale(img: Mat, foundLocations: any, hitThreshold?: double, winStride?: Size, padding?: Size, scale?: double, finalThreshold?: double, useMeanshiftGrouping?: bool): Mat;
	/**
	 * @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
	 *
	 * @param foundLocations Vector of rectangles where each rectangle contains the detected object.
	 *
	 * @param locations Vector of DetectionROI
	 *
	 * @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually
	 * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if
	 * the free coefficient is omitted (which is allowed), you can specify it manually here.
	 *
	 * @param groupThreshold Minimum possible number of rectangles minus 1. The threshold is used in a
	 * group of rectangles to retain it.
	 */
	detectMultiScaleROI(img: Mat, foundLocations: any, locations: any, hitThreshold?: double, groupThreshold?: int): Mat;
	/**
	 * @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.
	 *
	 * @param locations Vector of Point
	 *
	 * @param foundLocations Vector of Point where each Point is detected object's top-left point.
	 *
	 * @param confidences confidences
	 *
	 * @param hitThreshold Threshold for the distance between features and SVM classifying plane. Usually
	 * it is 0 and should be specified in the detector coefficients (as the last free coefficient). But if
	 * the free coefficient is omitted (which is allowed), you can specify it manually here
	 *
	 * @param winStride winStride
	 *
	 * @param padding padding
	 */
	detectROI(img: Mat, locations: any, foundLocations: any, confidences: any, hitThreshold?: double, winStride?: any, padding?: any): Mat;
	getDescriptorSize(): size_t;
	getWinSigma(): double;
	/**
	 * @param rectList Input/output vector of rectangles. Output vector includes retained and grouped
	 * rectangles. (The Python list is not modified in place.)
	 *
	 * @param weights Input/output vector of weights of rectangles. Output vector includes weights of
	 * retained and grouped rectangles. (The Python list is not modified in place.)
	 *
	 * @param groupThreshold Minimum possible number of rectangles minus 1. The threshold is used in a
	 * group of rectangles to retain it.
	 *
	 * @param eps Relative difference between sides of the rectangles to merge them into a group.
	 */
	groupRectangles(rectList: any, weights: any, groupThreshold: int, eps: double): any;
	/**
	 * @param filename Path of the file to read.
	 *
	 * @param objname The optional name of the node to read (if empty, the first top-level node will be
	 * used).
	 */
	load(filename: String, objname?: String): String;
	/**
	 * @param fn File node
	 */
	read(fn: FileNode): FileNode;
	/**
	 * @param filename File name
	 *
	 * @param objname Object name
	 */
	save(filename: String, objname?: String): String;
	/**
	 * @param svmdetector coefficients for the linear SVM classifier.
	 */
	setSVMDetector(svmdetector: Mat): Mat;
	/**
	 * @param fs File storage
	 *
	 * @param objname Object name
	 */
	write(fs: FileStorage, objname: String): FileStorage;
	static getDaimlerPeopleDetector(): any;
	static getDefaultPeopleDetector(): any;
}
export declare const DEFAULT_NLEVELS: any;
export declare const DESCR_FORMAT_COL_BY_COL: DescriptorStorageFormat;
export declare const DESCR_FORMAT_ROW_BY_ROW: DescriptorStorageFormat;
export declare const L2Hys: HistogramNormType;
export type DescriptorStorageFormat = any;
export type HistogramNormType = any;
/**
 * The function converts an input image from one color space to another. In case of a transformation
 * to-from RGB color space, the order of the channels should be specified explicitly (RGB or BGR). Note
 * that the default color format in OpenCV is often referred to as RGB but it is actually BGR (the
 * bytes are reversed). So the first byte in a standard (24-bit) color image will be an 8-bit Blue
 * component, the second byte will be Green, and the third byte will be Red. The fourth, fifth, and
 * sixth bytes would then be the second pixel (Blue, then Green, then Red), and so on.
 *
 * The conventional ranges for R, G, and B channel values are:
 *
 * 0 to 255 for CV_8U images
 * 0 to 65535 for CV_16U images
 * 0 to 1 for CV_32F images
 *
 * In case of linear transformations, the range does not matter. But in case of a non-linear
 * transformation, an input RGB image should be normalized to the proper value range to get the correct
 * results, for example, for RGB `$\\rightarrow$` L*u*v* transformation. For example, if you have a
 * 32-bit floating-point image directly converted from an 8-bit image without any scaling, then it will
 * have the 0..255 value range instead of 0..1 assumed by the function. So, before calling [cvtColor] ,
 * you need first to scale the image down:
 *
 * ```cpp
 * img *= 1./255;
 * cvtColor(img, img, COLOR_BGR2Luv);
 * ```
 *
 *  If you use [cvtColor] with 8-bit images, the conversion will have some information lost. For many
 * applications, this will not be noticeable but it is recommended to use 32-bit images in applications
 * that need the full range of colors or that convert an image before an operation and then convert
 * back.
 *
 * If conversion adds the alpha channel, its value will set to the maximum of corresponding channel
 * range: 255 for CV_8U, 65535 for CV_16U, 1 for CV_32F.
 *
 * [Color conversions]
 *
 * @param src input image: 8-bit unsigned, 16-bit unsigned ( CV_16UC... ), or single-precision
 * floating-point.
 *
 * @param dst output image of the same size and depth as src.
 *
 * @param code color space conversion code (see ColorConversionCodes).
 *
 * @param dstCn number of channels in the destination image; if the parameter is 0, the number of the
 * channels is derived automatically from src and code.
 */
export declare function cvtColor(src: Mat, dst: Mat, code: int, dstCn?: int): void;
/**
 * This function only supports YUV420 to RGB conversion as of now.
 *
 * @param src1 8-bit image (CV_8U) of the Y plane.
 *
 * @param src2 image containing interleaved U/V plane.
 *
 * @param dst output image.
 *
 * @param code Specifies the type of conversion. It can take any of the following values:
 * COLOR_YUV2BGR_NV12COLOR_YUV2RGB_NV12COLOR_YUV2BGRA_NV12COLOR_YUV2RGBA_NV12COLOR_YUV2BGR_NV21COLOR_YUV2RGB_NV21COLOR_YUV2BGRA_NV21COLOR_YUV2RGBA_NV21
 */
export declare function cvtColorTwoPlane(src1: Mat, src2: Mat, dst: Mat, code: int): void;
/**
 * The function can do the following transformations:
 *
 * Demosaicing using bilinear interpolation[COLOR_BayerBG2BGR] , [COLOR_BayerGB2BGR] ,
 * [COLOR_BayerRG2BGR] , [COLOR_BayerGR2BGR][COLOR_BayerBG2GRAY] , [COLOR_BayerGB2GRAY] ,
 * [COLOR_BayerRG2GRAY] , [COLOR_BayerGR2GRAY]
 * Demosaicing using Variable Number of Gradients.[COLOR_BayerBG2BGR_VNG] , [COLOR_BayerGB2BGR_VNG] ,
 * [COLOR_BayerRG2BGR_VNG] , [COLOR_BayerGR2BGR_VNG]
 * Edge-Aware Demosaicing.[COLOR_BayerBG2BGR_EA] , [COLOR_BayerGB2BGR_EA] , [COLOR_BayerRG2BGR_EA] ,
 * [COLOR_BayerGR2BGR_EA]
 * Demosaicing with alpha channel[COLOR_BayerBG2BGRA] , [COLOR_BayerGB2BGRA] , [COLOR_BayerRG2BGRA] ,
 * [COLOR_BayerGR2BGRA]
 *
 * [cvtColor]
 *
 * @param src input image: 8-bit unsigned or 16-bit unsigned.
 *
 * @param dst output image of the same size and depth as src.
 *
 * @param code Color space conversion code (see the description below).
 *
 * @param dstCn number of channels in the destination image; if the parameter is 0, the number of the
 * channels is derived automatically from src and code.
 */
export declare function demosaicing(src: Mat, dst: Mat, code: int, dstCn?: int): void;
export declare const COLOR_BGR2BGRA: ColorConversionCodes;
export declare const COLOR_RGB2RGBA: ColorConversionCodes;
export declare const COLOR_BGRA2BGR: ColorConversionCodes;
export declare const COLOR_RGBA2RGB: ColorConversionCodes;
export declare const COLOR_BGR2RGBA: ColorConversionCodes;
export declare const COLOR_RGB2BGRA: ColorConversionCodes;
export declare const COLOR_RGBA2BGR: ColorConversionCodes;
export declare const COLOR_BGRA2RGB: ColorConversionCodes;
export declare const COLOR_BGR2RGB: ColorConversionCodes;
export declare const COLOR_RGB2BGR: ColorConversionCodes;
export declare const COLOR_BGRA2RGBA: ColorConversionCodes;
export declare const COLOR_RGBA2BGRA: ColorConversionCodes;
export declare const COLOR_BGR2GRAY: ColorConversionCodes;
export declare const COLOR_RGB2GRAY: ColorConversionCodes;
export declare const COLOR_GRAY2BGR: ColorConversionCodes;
export declare const COLOR_GRAY2RGB: ColorConversionCodes;
export declare const COLOR_GRAY2BGRA: ColorConversionCodes;
export declare const COLOR_GRAY2RGBA: ColorConversionCodes;
export declare const COLOR_BGRA2GRAY: ColorConversionCodes;
export declare const COLOR_RGBA2GRAY: ColorConversionCodes;
export declare const COLOR_BGR2BGR565: ColorConversionCodes;
export declare const COLOR_RGB2BGR565: ColorConversionCodes;
export declare const COLOR_BGR5652BGR: ColorConversionCodes;
export declare const COLOR_BGR5652RGB: ColorConversionCodes;
export declare const COLOR_BGRA2BGR565: ColorConversionCodes;
export declare const COLOR_RGBA2BGR565: ColorConversionCodes;
export declare const COLOR_BGR5652BGRA: ColorConversionCodes;
export declare const COLOR_BGR5652RGBA: ColorConversionCodes;
export declare const COLOR_GRAY2BGR565: ColorConversionCodes;
export declare const COLOR_BGR5652GRAY: ColorConversionCodes;
export declare const COLOR_BGR2BGR555: ColorConversionCodes;
export declare const COLOR_RGB2BGR555: ColorConversionCodes;
export declare const COLOR_BGR5552BGR: ColorConversionCodes;
export declare const COLOR_BGR5552RGB: ColorConversionCodes;
export declare const COLOR_BGRA2BGR555: ColorConversionCodes;
export declare const COLOR_RGBA2BGR555: ColorConversionCodes;
export declare const COLOR_BGR5552BGRA: ColorConversionCodes;
export declare const COLOR_BGR5552RGBA: ColorConversionCodes;
export declare const COLOR_GRAY2BGR555: ColorConversionCodes;
export declare const COLOR_BGR5552GRAY: ColorConversionCodes;
export declare const COLOR_BGR2XYZ: ColorConversionCodes;
export declare const COLOR_RGB2XYZ: ColorConversionCodes;
export declare const COLOR_XYZ2BGR: ColorConversionCodes;
export declare const COLOR_XYZ2RGB: ColorConversionCodes;
export declare const COLOR_BGR2YCrCb: ColorConversionCodes;
export declare const COLOR_RGB2YCrCb: ColorConversionCodes;
export declare const COLOR_YCrCb2BGR: ColorConversionCodes;
export declare const COLOR_YCrCb2RGB: ColorConversionCodes;
export declare const COLOR_BGR2HSV: ColorConversionCodes;
export declare const COLOR_RGB2HSV: ColorConversionCodes;
export declare const COLOR_BGR2Lab: ColorConversionCodes;
export declare const COLOR_RGB2Lab: ColorConversionCodes;
export declare const COLOR_BGR2Luv: ColorConversionCodes;
export declare const COLOR_RGB2Luv: ColorConversionCodes;
export declare const COLOR_BGR2HLS: ColorConversionCodes;
export declare const COLOR_RGB2HLS: ColorConversionCodes;
export declare const COLOR_HSV2BGR: ColorConversionCodes;
export declare const COLOR_HSV2RGB: ColorConversionCodes;
export declare const COLOR_Lab2BGR: ColorConversionCodes;
export declare const COLOR_Lab2RGB: ColorConversionCodes;
export declare const COLOR_Luv2BGR: ColorConversionCodes;
export declare const COLOR_Luv2RGB: ColorConversionCodes;
export declare const COLOR_HLS2BGR: ColorConversionCodes;
export declare const COLOR_HLS2RGB: ColorConversionCodes;
export declare const COLOR_BGR2HSV_FULL: ColorConversionCodes;
export declare const COLOR_RGB2HSV_FULL: ColorConversionCodes;
export declare const COLOR_BGR2HLS_FULL: ColorConversionCodes;
export declare const COLOR_RGB2HLS_FULL: ColorConversionCodes;
export declare const COLOR_HSV2BGR_FULL: ColorConversionCodes;
export declare const COLOR_HSV2RGB_FULL: ColorConversionCodes;
export declare const COLOR_HLS2BGR_FULL: ColorConversionCodes;
export declare const COLOR_HLS2RGB_FULL: ColorConversionCodes;
export declare const COLOR_LBGR2Lab: ColorConversionCodes;
export declare const COLOR_LRGB2Lab: ColorConversionCodes;
export declare const COLOR_LBGR2Luv: ColorConversionCodes;
export declare const COLOR_LRGB2Luv: ColorConversionCodes;
export declare const COLOR_Lab2LBGR: ColorConversionCodes;
export declare const COLOR_Lab2LRGB: ColorConversionCodes;
export declare const COLOR_Luv2LBGR: ColorConversionCodes;
export declare const COLOR_Luv2LRGB: ColorConversionCodes;
export declare const COLOR_BGR2YUV: ColorConversionCodes;
export declare const COLOR_RGB2YUV: ColorConversionCodes;
export declare const COLOR_YUV2BGR: ColorConversionCodes;
export declare const COLOR_YUV2RGB: ColorConversionCodes;
export declare const COLOR_YUV2RGB_NV12: ColorConversionCodes;
export declare const COLOR_YUV2BGR_NV12: ColorConversionCodes;
export declare const COLOR_YUV2RGB_NV21: ColorConversionCodes;
export declare const COLOR_YUV2BGR_NV21: ColorConversionCodes;
export declare const COLOR_YUV420sp2RGB: ColorConversionCodes;
export declare const COLOR_YUV420sp2BGR: ColorConversionCodes;
export declare const COLOR_YUV2RGBA_NV12: ColorConversionCodes;
export declare const COLOR_YUV2BGRA_NV12: ColorConversionCodes;
export declare const COLOR_YUV2RGBA_NV21: ColorConversionCodes;
export declare const COLOR_YUV2BGRA_NV21: ColorConversionCodes;
export declare const COLOR_YUV420sp2RGBA: ColorConversionCodes;
export declare const COLOR_YUV420sp2BGRA: ColorConversionCodes;
export declare const COLOR_YUV2RGB_YV12: ColorConversionCodes;
export declare const COLOR_YUV2BGR_YV12: ColorConversionCodes;
export declare const COLOR_YUV2RGB_IYUV: ColorConversionCodes;
export declare const COLOR_YUV2BGR_IYUV: ColorConversionCodes;
export declare const COLOR_YUV2RGB_I420: ColorConversionCodes;
export declare const COLOR_YUV2BGR_I420: ColorConversionCodes;
export declare const COLOR_YUV420p2RGB: ColorConversionCodes;
export declare const COLOR_YUV420p2BGR: ColorConversionCodes;
export declare const COLOR_YUV2RGBA_YV12: ColorConversionCodes;
export declare const COLOR_YUV2BGRA_YV12: ColorConversionCodes;
export declare const COLOR_YUV2RGBA_IYUV: ColorConversionCodes;
export declare const COLOR_YUV2BGRA_IYUV: ColorConversionCodes;
export declare const COLOR_YUV2RGBA_I420: ColorConversionCodes;
export declare const COLOR_YUV2BGRA_I420: ColorConversionCodes;
export declare const COLOR_YUV420p2RGBA: ColorConversionCodes;
export declare const COLOR_YUV420p2BGRA: ColorConversionCodes;
export declare const COLOR_YUV2GRAY_420: ColorConversionCodes;
export declare const COLOR_YUV2GRAY_NV21: ColorConversionCodes;
export declare const COLOR_YUV2GRAY_NV12: ColorConversionCodes;
export declare const COLOR_YUV2GRAY_YV12: ColorConversionCodes;
export declare const COLOR_YUV2GRAY_IYUV: ColorConversionCodes;
export declare const COLOR_YUV2GRAY_I420: ColorConversionCodes;
export declare const COLOR_YUV420sp2GRAY: ColorConversionCodes;
export declare const COLOR_YUV420p2GRAY: ColorConversionCodes;
export declare const COLOR_YUV2RGB_UYVY: ColorConversionCodes;
export declare const COLOR_YUV2BGR_UYVY: ColorConversionCodes;
export declare const COLOR_YUV2RGB_Y422: ColorConversionCodes;
export declare const COLOR_YUV2BGR_Y422: ColorConversionCodes;
export declare const COLOR_YUV2RGB_UYNV: ColorConversionCodes;
export declare const COLOR_YUV2BGR_UYNV: ColorConversionCodes;
export declare const COLOR_YUV2RGBA_UYVY: ColorConversionCodes;
export declare const COLOR_YUV2BGRA_UYVY: ColorConversionCodes;
export declare const COLOR_YUV2RGBA_Y422: ColorConversionCodes;
export declare const COLOR_YUV2BGRA_Y422: ColorConversionCodes;
export declare const COLOR_YUV2RGBA_UYNV: ColorConversionCodes;
export declare const COLOR_YUV2BGRA_UYNV: ColorConversionCodes;
export declare const COLOR_YUV2RGB_YUY2: ColorConversionCodes;
export declare const COLOR_YUV2BGR_YUY2: ColorConversionCodes;
export declare const COLOR_YUV2RGB_YVYU: ColorConversionCodes;
export declare const COLOR_YUV2BGR_YVYU: ColorConversionCodes;
export declare const COLOR_YUV2RGB_YUYV: ColorConversionCodes;
export declare const COLOR_YUV2BGR_YUYV: ColorConversionCodes;
export declare const COLOR_YUV2RGB_YUNV: ColorConversionCodes;
export declare const COLOR_YUV2BGR_YUNV: ColorConversionCodes;
export declare const COLOR_YUV2RGBA_YUY2: ColorConversionCodes;
export declare const COLOR_YUV2BGRA_YUY2: ColorConversionCodes;
export declare const COLOR_YUV2RGBA_YVYU: ColorConversionCodes;
export declare const COLOR_YUV2BGRA_YVYU: ColorConversionCodes;
export declare const COLOR_YUV2RGBA_YUYV: ColorConversionCodes;
export declare const COLOR_YUV2BGRA_YUYV: ColorConversionCodes;
export declare const COLOR_YUV2RGBA_YUNV: ColorConversionCodes;
export declare const COLOR_YUV2BGRA_YUNV: ColorConversionCodes;
export declare const COLOR_YUV2GRAY_UYVY: ColorConversionCodes;
export declare const COLOR_YUV2GRAY_YUY2: ColorConversionCodes;
export declare const COLOR_YUV2GRAY_Y422: ColorConversionCodes;
export declare const COLOR_YUV2GRAY_UYNV: ColorConversionCodes;
export declare const COLOR_YUV2GRAY_YVYU: ColorConversionCodes;
export declare const COLOR_YUV2GRAY_YUYV: ColorConversionCodes;
export declare const COLOR_YUV2GRAY_YUNV: ColorConversionCodes;
export declare const COLOR_RGBA2mRGBA: ColorConversionCodes;
export declare const COLOR_mRGBA2RGBA: ColorConversionCodes;
export declare const COLOR_RGB2YUV_I420: ColorConversionCodes;
export declare const COLOR_BGR2YUV_I420: ColorConversionCodes;
export declare const COLOR_RGB2YUV_IYUV: ColorConversionCodes;
export declare const COLOR_BGR2YUV_IYUV: ColorConversionCodes;
export declare const COLOR_RGBA2YUV_I420: ColorConversionCodes;
export declare const COLOR_BGRA2YUV_I420: ColorConversionCodes;
export declare const COLOR_RGBA2YUV_IYUV: ColorConversionCodes;
export declare const COLOR_BGRA2YUV_IYUV: ColorConversionCodes;
export declare const COLOR_RGB2YUV_YV12: ColorConversionCodes;
export declare const COLOR_BGR2YUV_YV12: ColorConversionCodes;
export declare const COLOR_RGBA2YUV_YV12: ColorConversionCodes;
export declare const COLOR_BGRA2YUV_YV12: ColorConversionCodes;
export declare const COLOR_BayerBG2BGR: ColorConversionCodes;
export declare const COLOR_BayerGB2BGR: ColorConversionCodes;
export declare const COLOR_BayerRG2BGR: ColorConversionCodes;
export declare const COLOR_BayerGR2BGR: ColorConversionCodes;
export declare const COLOR_BayerBG2RGB: ColorConversionCodes;
export declare const COLOR_BayerGB2RGB: ColorConversionCodes;
export declare const COLOR_BayerRG2RGB: ColorConversionCodes;
export declare const COLOR_BayerGR2RGB: ColorConversionCodes;
export declare const COLOR_BayerBG2GRAY: ColorConversionCodes;
export declare const COLOR_BayerGB2GRAY: ColorConversionCodes;
export declare const COLOR_BayerRG2GRAY: ColorConversionCodes;
export declare const COLOR_BayerGR2GRAY: ColorConversionCodes;
export declare const COLOR_BayerBG2BGR_VNG: ColorConversionCodes;
export declare const COLOR_BayerGB2BGR_VNG: ColorConversionCodes;
export declare const COLOR_BayerRG2BGR_VNG: ColorConversionCodes;
export declare const COLOR_BayerGR2BGR_VNG: ColorConversionCodes;
export declare const COLOR_BayerBG2RGB_VNG: ColorConversionCodes;
export declare const COLOR_BayerGB2RGB_VNG: ColorConversionCodes;
export declare const COLOR_BayerRG2RGB_VNG: ColorConversionCodes;
export declare const COLOR_BayerGR2RGB_VNG: ColorConversionCodes;
export declare const COLOR_BayerBG2BGR_EA: ColorConversionCodes;
export declare const COLOR_BayerGB2BGR_EA: ColorConversionCodes;
export declare const COLOR_BayerRG2BGR_EA: ColorConversionCodes;
export declare const COLOR_BayerGR2BGR_EA: ColorConversionCodes;
export declare const COLOR_BayerBG2RGB_EA: ColorConversionCodes;
export declare const COLOR_BayerGB2RGB_EA: ColorConversionCodes;
export declare const COLOR_BayerRG2RGB_EA: ColorConversionCodes;
export declare const COLOR_BayerGR2RGB_EA: ColorConversionCodes;
export declare const COLOR_BayerBG2BGRA: ColorConversionCodes;
export declare const COLOR_BayerGB2BGRA: ColorConversionCodes;
export declare const COLOR_BayerRG2BGRA: ColorConversionCodes;
export declare const COLOR_BayerGR2BGRA: ColorConversionCodes;
export declare const COLOR_BayerBG2RGBA: ColorConversionCodes;
export declare const COLOR_BayerGB2RGBA: ColorConversionCodes;
export declare const COLOR_BayerRG2RGBA: ColorConversionCodes;
export declare const COLOR_BayerGR2RGBA: ColorConversionCodes;
export declare const COLOR_COLORCVT_MAX: ColorConversionCodes;
/**
 * the color conversion codes
 *
 * [Color conversions]
 *
 */
export type ColorConversionCodes = any;
/**
 * The function [cv::arrowedLine] draws an arrow between pt1 and pt2 points in the image. See also
 * [line].
 *
 * @param img Image.
 *
 * @param pt1 The point the arrow starts from.
 *
 * @param pt2 The point the arrow points to.
 *
 * @param color Line color.
 *
 * @param thickness Line thickness.
 *
 * @param line_type Type of the line. See LineTypes
 *
 * @param shift Number of fractional bits in the point coordinates.
 *
 * @param tipLength The length of the arrow tip in relation to the arrow length
 */
export declare function arrowedLine(img: Mat, pt1: Point, pt2: Point, color: any, thickness?: int, line_type?: int, shift?: int, tipLength?: double): void;
/**
 * The function [cv::circle] draws a simple or filled circle with a given center and radius.
 *
 * @param img Image where the circle is drawn.
 *
 * @param center Center of the circle.
 *
 * @param radius Radius of the circle.
 *
 * @param color Circle color.
 *
 * @param thickness Thickness of the circle outline, if positive. Negative values, like FILLED, mean
 * that a filled circle is to be drawn.
 *
 * @param lineType Type of the circle boundary. See LineTypes
 *
 * @param shift Number of fractional bits in the coordinates of the center and in the radius value.
 */
export declare function circle(img: Mat, center: Point, radius: int, color: any, thickness?: int, lineType?: int, shift?: int): void;
/**
 * The function [cv::clipLine] calculates a part of the line segment that is entirely within the
 * specified rectangle. it returns false if the line segment is completely outside the rectangle.
 * Otherwise, it returns true .
 *
 * @param imgSize Image size. The image rectangle is Rect(0, 0, imgSize.width, imgSize.height) .
 *
 * @param pt1 First line point.
 *
 * @param pt2 Second line point.
 */
export declare function clipLine(imgSize: Size, pt1: any, pt2: any): bool;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param imgSize Image size. The image rectangle is Rect(0, 0, imgSize.width, imgSize.height) .
 *
 * @param pt1 First line point.
 *
 * @param pt2 Second line point.
 */
export declare function clipLine(imgSize: Size, pt1: any, pt2: any): bool;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param imgRect Image rectangle.
 *
 * @param pt1 First line point.
 *
 * @param pt2 Second line point.
 */
export declare function clipLine(imgRect: Rect, pt1: any, pt2: any): bool;
/**
 * The function draws contour outlines in the image if `$\\texttt{thickness} \\ge 0$` or fills the area
 * bounded by the contours if `$\\texttt{thickness}<0$` . The example below shows how to retrieve
 * connected components from the binary image and label them: :
 *
 * ```cpp
 * #include "opencv2/imgproc.hpp"
 * #include "opencv2/highgui.hpp"
 *
 * using namespace cv;
 * using namespace std;
 *
 * int main( int argc, char** argv )
 * {
 *     Mat src;
 *     // the first command-line parameter must be a filename of the binary
 *     // (black-n-white) image
 *     if( argc != 2 || !(src=imread(argv[1], 0)).data)
 *         return -1;
 *
 *     Mat dst = Mat::zeros(src.rows, src.cols, CV_8UC3);
 *
 *     src = src > 1;
 *     namedWindow( "Source", 1 );
 *     imshow( "Source", src );
 *
 *     vector<vector<Point> > contours;
 *     vector<Vec4i> hierarchy;
 *
 *     findContours( src, contours, hierarchy,
 *         RETR_CCOMP, CHAIN_APPROX_SIMPLE );
 *
 *     // iterate through all the top-level contours,
 *     // draw each connected component with its own random color
 *     int idx = 0;
 *     for( ; idx >= 0; idx = hierarchy[idx][0] )
 *     {
 *         Scalar color( rand()&255, rand()&255, rand()&255 );
 *         drawContours( dst, contours, idx, color, FILLED, 8, hierarchy );
 *     }
 *
 *     namedWindow( "Components", 1 );
 *     imshow( "Components", dst );
 *     waitKey(0);
 * }
 * ```
 *
 * When thickness=[FILLED], the function is designed to handle connected components with holes
 * correctly even when no hierarchy date is provided. This is done by analyzing all the outlines
 * together using even-odd rule. This may give incorrect results if you have a joint collection of
 * separately retrieved contours. In order to solve this problem, you need to call [drawContours]
 * separately for each sub-group of contours, or iterate over the collection using contourIdx
 * parameter.
 *
 * @param image Destination image.
 *
 * @param contours All the input contours. Each contour is stored as a point vector.
 *
 * @param contourIdx Parameter indicating a contour to draw. If it is negative, all the contours are
 * drawn.
 *
 * @param color Color of the contours.
 *
 * @param thickness Thickness of lines the contours are drawn with. If it is negative (for example,
 * thickness=FILLED ), the contour interiors are drawn.
 *
 * @param lineType Line connectivity. See LineTypes
 *
 * @param hierarchy Optional information about hierarchy. It is only needed if you want to draw only
 * some of the contours (see maxLevel ).
 *
 * @param maxLevel Maximal level for drawn contours. If it is 0, only the specified contour is drawn.
 * If it is 1, the function draws the contour(s) and all the nested contours. If it is 2, the function
 * draws the contours, all the nested contours, all the nested-to-nested contours, and so on. This
 * parameter is only taken into account when there is hierarchy available.
 *
 * @param offset Optional contour shift parameter. Shift all the drawn contours by the specified
 * $\texttt{offset}=(dx,dy)$ .
 */
export declare function drawContours(image: Mat, contours: MatVector, contourIdx: int, color: any, thickness?: int, lineType?: int, hierarchy?: Mat, maxLevel?: int, offset?: Point): void;
/**
 * The function [cv::drawMarker] draws a marker on a given position in the image. For the moment
 * several marker types are supported, see [MarkerTypes] for more information.
 *
 * @param img Image.
 *
 * @param position The point where the crosshair is positioned.
 *
 * @param color Line color.
 *
 * @param markerType The specific type of marker you want to use, see MarkerTypes
 *
 * @param markerSize The length of the marker axis [default = 20 pixels]
 *
 * @param thickness Line thickness.
 *
 * @param line_type Type of the line, See LineTypes
 */
export declare function drawMarker(img: Mat, position: Point, color: any, markerType?: int, markerSize?: int, thickness?: int, line_type?: int): void;
/**
 * The function [cv::ellipse] with more parameters draws an ellipse outline, a filled ellipse, an
 * elliptic arc, or a filled ellipse sector. The drawing code uses general parametric form. A
 * piecewise-linear curve is used to approximate the elliptic arc boundary. If you need more control of
 * the ellipse rendering, you can retrieve the curve using [ellipse2Poly] and then render it with
 * [polylines] or fill it with [fillPoly]. If you use the first variant of the function and want to
 * draw the whole ellipse, not an arc, pass `startAngle=0` and `endAngle=360`. If `startAngle` is
 * greater than `endAngle`, they are swapped. The figure below explains the meaning of the parameters
 * to draw the blue arc.
 *
 * @param img Image.
 *
 * @param center Center of the ellipse.
 *
 * @param axes Half of the size of the ellipse main axes.
 *
 * @param angle Ellipse rotation angle in degrees.
 *
 * @param startAngle Starting angle of the elliptic arc in degrees.
 *
 * @param endAngle Ending angle of the elliptic arc in degrees.
 *
 * @param color Ellipse color.
 *
 * @param thickness Thickness of the ellipse arc outline, if positive. Otherwise, this indicates that a
 * filled ellipse sector is to be drawn.
 *
 * @param lineType Type of the ellipse boundary. See LineTypes
 *
 * @param shift Number of fractional bits in the coordinates of the center and values of axes.
 */
export declare function ellipse(img: Mat, center: Point, axes: Size, angle: double, startAngle: double, endAngle: double, color: any, thickness?: int, lineType?: int, shift?: int): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param img Image.
 *
 * @param box Alternative ellipse representation via RotatedRect. This means that the function draws an
 * ellipse inscribed in the rotated rectangle.
 *
 * @param color Ellipse color.
 *
 * @param thickness Thickness of the ellipse arc outline, if positive. Otherwise, this indicates that a
 * filled ellipse sector is to be drawn.
 *
 * @param lineType Type of the ellipse boundary. See LineTypes
 */
export declare function ellipse(img: Mat, box: any, color: any, thickness?: int, lineType?: int): void;
/**
 * The function ellipse2Poly computes the vertices of a polyline that approximates the specified
 * elliptic arc. It is used by [ellipse]. If `arcStart` is greater than `arcEnd`, they are swapped.
 *
 * @param center Center of the arc.
 *
 * @param axes Half of the size of the ellipse main axes. See ellipse for details.
 *
 * @param angle Rotation angle of the ellipse in degrees. See ellipse for details.
 *
 * @param arcStart Starting angle of the elliptic arc in degrees.
 *
 * @param arcEnd Ending angle of the elliptic arc in degrees.
 *
 * @param delta Angle between the subsequent polyline vertices. It defines the approximation accuracy.
 *
 * @param pts Output vector of polyline vertices.
 */
export declare function ellipse2Poly(center: Point, axes: Size, angle: int, arcStart: int, arcEnd: int, delta: int, pts: any): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param center Center of the arc.
 *
 * @param axes Half of the size of the ellipse main axes. See ellipse for details.
 *
 * @param angle Rotation angle of the ellipse in degrees. See ellipse for details.
 *
 * @param arcStart Starting angle of the elliptic arc in degrees.
 *
 * @param arcEnd Ending angle of the elliptic arc in degrees.
 *
 * @param delta Angle between the subsequent polyline vertices. It defines the approximation accuracy.
 *
 * @param pts Output vector of polyline vertices.
 */
export declare function ellipse2Poly(center: Size, axes: Size, angle: int, arcStart: int, arcEnd: int, delta: int, pts: any): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function fillConvexPoly(img: Mat, pts: any, npts: int, color: any, lineType?: int, shift?: int): void;
/**
 * The function [cv::fillConvexPoly] draws a filled convex polygon. This function is much faster than
 * the function [fillPoly] . It can fill not only convex polygons but any monotonic polygon without
 * self-intersections, that is, a polygon whose contour intersects every horizontal line (scan line)
 * twice at the most (though, its top-most and/or the bottom edge could be horizontal).
 *
 * @param img Image.
 *
 * @param points Polygon vertices.
 *
 * @param color Polygon color.
 *
 * @param lineType Type of the polygon boundaries. See LineTypes
 *
 * @param shift Number of fractional bits in the vertex coordinates.
 */
export declare function fillConvexPoly(img: Mat, points: Mat, color: any, lineType?: int, shift?: int): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function fillPoly(img: Mat, pts: any, npts: any, ncontours: int, color: any, lineType?: int, shift?: int, offset?: Point): void;
/**
 * The function [cv::fillPoly] fills an area bounded by several polygonal contours. The function can
 * fill complex areas, for example, areas with holes, contours with self-intersections (some of their
 * parts), and so forth.
 *
 * @param img Image.
 *
 * @param pts Array of polygons where each polygon is represented as an array of points.
 *
 * @param color Polygon color.
 *
 * @param lineType Type of the polygon boundaries. See LineTypes
 *
 * @param shift Number of fractional bits in the vertex coordinates.
 *
 * @param offset Optional offset of all points of the contours.
 */
export declare function fillPoly(img: Mat, pts: MatVector, color: any, lineType?: int, shift?: int, offset?: Point): void;
/**
 * The fontSize to use for [cv::putText]
 *
 * [cv::putText]
 *
 * @param fontFace Font to use, see cv::HersheyFonts.
 *
 * @param pixelHeight Pixel height to compute the fontScale for
 *
 * @param thickness Thickness of lines used to render the text.See putText for details.
 */
export declare function getFontScaleFromHeight(fontFace: any, pixelHeight: any, thickness?: any): double;
/**
 * The function [cv::getTextSize] calculates and returns the size of a box that contains the specified
 * text. That is, the following code renders some text, the tight box surrounding it, and the baseline:
 * :
 *
 * ```cpp
 * String text = "Funny text inside the box";
 * int fontFace = FONT_HERSHEY_SCRIPT_SIMPLEX;
 * double fontScale = 2;
 * int thickness = 3;
 *
 * Mat img(600, 800, CV_8UC3, Scalar::all(0));
 *
 * int baseline=0;
 * Size textSize = getTextSize(text, fontFace,
 *                             fontScale, thickness, &baseline);
 * baseline += thickness;
 *
 * // center the text
 * Point textOrg((img.cols - textSize.width)/2,
 *               (img.rows + textSize.height)/2);
 *
 * // draw the box
 * rectangle(img, textOrg + Point(0, baseline),
 *           textOrg + Point(textSize.width, -textSize.height),
 *           Scalar(0,0,255));
 * // ... and the baseline first
 * line(img, textOrg + Point(0, thickness),
 *      textOrg + Point(textSize.width, thickness),
 *      Scalar(0, 0, 255));
 *
 * // then put the text itself
 * putText(img, text, textOrg, fontFace, fontScale,
 *         Scalar::all(255), thickness, 8);
 * ```
 *
 * The size of a box that contains the specified text.
 *
 * [putText]
 *
 * @param text Input text string.
 *
 * @param fontFace Font to use, see HersheyFonts.
 *
 * @param fontScale Font scale factor that is multiplied by the font-specific base size.
 *
 * @param thickness Thickness of lines used to render the text. See putText for details.
 *
 * @param baseLine y-coordinate of the baseline relative to the bottom-most text point.
 */
export declare function getTextSize(text: any, fontFace: int, fontScale: double, thickness: int, baseLine: any): Size;
/**
 * The function line draws the line segment between pt1 and pt2 points in the image. The line is
 * clipped by the image boundaries. For non-antialiased lines with integer coordinates, the 8-connected
 * or 4-connected Bresenham algorithm is used. Thick lines are drawn with rounding endings. Antialiased
 * lines are drawn using Gaussian filtering.
 *
 * @param img Image.
 *
 * @param pt1 First point of the line segment.
 *
 * @param pt2 Second point of the line segment.
 *
 * @param color Line color.
 *
 * @param thickness Line thickness.
 *
 * @param lineType Type of the line. See LineTypes.
 *
 * @param shift Number of fractional bits in the point coordinates.
 */
export declare function line(img: Mat, pt1: Point, pt2: Point, color: any, thickness?: int, lineType?: int, shift?: int): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function polylines(img: Mat, pts: any, npts: any, ncontours: int, isClosed: bool, color: any, thickness?: int, lineType?: int, shift?: int): void;
/**
 * The function [cv::polylines] draws one or more polygonal curves.
 *
 * @param img Image.
 *
 * @param pts Array of polygonal curves.
 *
 * @param isClosed Flag indicating whether the drawn polylines are closed or not. If they are closed,
 * the function draws a line from the last vertex of each curve to its first vertex.
 *
 * @param color Polyline color.
 *
 * @param thickness Thickness of the polyline edges.
 *
 * @param lineType Type of the line segments. See LineTypes
 *
 * @param shift Number of fractional bits in the vertex coordinates.
 */
export declare function polylines(img: Mat, pts: MatVector, isClosed: bool, color: any, thickness?: int, lineType?: int, shift?: int): void;
/**
 * The function [cv::putText] renders the specified text string in the image. Symbols that cannot be
 * rendered using the specified font are replaced by question marks. See [getTextSize] for a text
 * rendering code example.
 *
 * @param img Image.
 *
 * @param text Text string to be drawn.
 *
 * @param org Bottom-left corner of the text string in the image.
 *
 * @param fontFace Font type, see HersheyFonts.
 *
 * @param fontScale Font scale factor that is multiplied by the font-specific base size.
 *
 * @param color Text color.
 *
 * @param thickness Thickness of the lines used to draw a text.
 *
 * @param lineType Line type. See LineTypes
 *
 * @param bottomLeftOrigin When true, the image data origin is at the bottom-left corner. Otherwise, it
 * is at the top-left corner.
 */
export declare function putText(img: Mat, text: any, org: Point, fontFace: int, fontScale: double, color: Scalar, thickness?: int, lineType?: int, bottomLeftOrigin?: bool): void;
/**
 * The function [cv::rectangle] draws a rectangle outline or a filled rectangle whose two opposite
 * corners are pt1 and pt2.
 *
 * @param img Image.
 *
 * @param pt1 Vertex of the rectangle.
 *
 * @param pt2 Vertex of the rectangle opposite to pt1 .
 *
 * @param color Rectangle color or brightness (grayscale image).
 *
 * @param thickness Thickness of lines that make up the rectangle. Negative values, like FILLED, mean
 * that the function has to draw a filled rectangle.
 *
 * @param lineType Type of the line. See LineTypes
 *
 * @param shift Number of fractional bits in the point coordinates.
 */
export declare function rectangle(img: Mat, pt1: Point, pt2: Point, color: any, thickness?: int, lineType?: int, shift?: int): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * use `rec` parameter as alternative specification of the drawn rectangle: `r.tl() and
 * r.br()-Point(1,1)` are opposite corners
 */
export declare function rectangle(img: Mat, rec: Rect, color: any, thickness?: int, lineType?: int, shift?: int): void;
export declare const FONT_HERSHEY_SIMPLEX: HersheyFonts;
export declare const FONT_HERSHEY_PLAIN: HersheyFonts;
export declare const FONT_HERSHEY_DUPLEX: HersheyFonts;
export declare const FONT_HERSHEY_COMPLEX: HersheyFonts;
export declare const FONT_HERSHEY_TRIPLEX: HersheyFonts;
export declare const FONT_HERSHEY_COMPLEX_SMALL: HersheyFonts;
export declare const FONT_HERSHEY_SCRIPT_SIMPLEX: HersheyFonts;
export declare const FONT_HERSHEY_SCRIPT_COMPLEX: HersheyFonts;
export declare const FONT_ITALIC: HersheyFonts;
export declare const FILLED: LineTypes;
export declare const LINE_4: LineTypes;
export declare const LINE_8: LineTypes;
export declare const LINE_AA: LineTypes;
export declare const MARKER_CROSS: MarkerTypes;
export declare const MARKER_TILTED_CROSS: MarkerTypes;
export declare const MARKER_STAR: MarkerTypes;
export declare const MARKER_DIAMOND: MarkerTypes;
export declare const MARKER_SQUARE: MarkerTypes;
export declare const MARKER_TRIANGLE_UP: MarkerTypes;
export declare const MARKER_TRIANGLE_DOWN: MarkerTypes;
/**
 * Only a subset of Hershey fonts  are supported
 *
 */
export type HersheyFonts = any;
/**
 * Only a subset of Hershey fonts  are supported
 *
 */
export type LineTypes = any;
/**
 * Only a subset of Hershey fonts  are supported
 *
 */
export type MarkerTypes = any;
/**
 * The function finds edges in the input image and marks them in the output map edges using the Canny
 * algorithm. The smallest value between threshold1 and threshold2 is used for edge linking. The
 * largest value is used to find initial segments of strong edges. See
 *
 * @param image 8-bit input image.
 *
 * @param edges output edge map; single channels 8-bit image, which has the same size as image .
 *
 * @param threshold1 first threshold for the hysteresis procedure.
 *
 * @param threshold2 second threshold for the hysteresis procedure.
 *
 * @param apertureSize aperture size for the Sobel operator.
 *
 * @param L2gradient a flag, indicating whether a more accurate $L_2$ norm $=\sqrt{(dI/dx)^2 +
 * (dI/dy)^2}$ should be used to calculate the image gradient magnitude ( L2gradient=true ), or whether
 * the default $L_1$ norm $=|dI/dx|+|dI/dy|$ is enough ( L2gradient=false ).
 */
export declare function Canny(image: Mat, edges: Mat, threshold1: double, threshold2: double, apertureSize?: int, L2gradient?: bool): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * Finds edges in an image using the Canny algorithm with custom image gradient.
 *
 * @param dx 16-bit x derivative of input image (CV_16SC1 or CV_16SC3).
 *
 * @param dy 16-bit y derivative of input image (same type as dx).
 *
 * @param edges output edge map; single channels 8-bit image, which has the same size as image .
 *
 * @param threshold1 first threshold for the hysteresis procedure.
 *
 * @param threshold2 second threshold for the hysteresis procedure.
 *
 * @param L2gradient a flag, indicating whether a more accurate $L_2$ norm $=\sqrt{(dI/dx)^2 +
 * (dI/dy)^2}$ should be used to calculate the image gradient magnitude ( L2gradient=true ), or whether
 * the default $L_1$ norm $=|dI/dx|+|dI/dy|$ is enough ( L2gradient=false ).
 */
export declare function Canny(dx: Mat, dy: Mat, edges: Mat, threshold1: double, threshold2: double, L2gradient?: bool): void;
/**
 * For every pixel `$p$` , the function cornerEigenValsAndVecs considers a blockSize `$\\times$`
 * blockSize neighborhood `$S(p)$` . It calculates the covariation matrix of derivatives over the
 * neighborhood as:
 *
 * `\\[M = \\begin{bmatrix} \\sum _{S(p)}(dI/dx)^2 & \\sum _{S(p)}dI/dx dI/dy \\\\ \\sum _{S(p)}dI/dx
 * dI/dy & \\sum _{S(p)}(dI/dy)^2 \\end{bmatrix}\\]`
 *
 * where the derivatives are computed using the Sobel operator.
 *
 * After that, it finds eigenvectors and eigenvalues of `$M$` and stores them in the destination image
 * as `$(\\lambda_1, \\lambda_2, x_1, y_1, x_2, y_2)$` where
 *
 * `$\\lambda_1, \\lambda_2$` are the non-sorted eigenvalues of `$M$`
 * `$x_1, y_1$` are the eigenvectors corresponding to `$\\lambda_1$`
 * `$x_2, y_2$` are the eigenvectors corresponding to `$\\lambda_2$`
 *
 * The output of the function can be used for robust edge or corner detection.
 *
 * [cornerMinEigenVal], [cornerHarris], [preCornerDetect]
 *
 * @param src Input single-channel 8-bit or floating-point image.
 *
 * @param dst Image to store the results. It has the same size as src and the type CV_32FC(6) .
 *
 * @param blockSize Neighborhood size (see details below).
 *
 * @param ksize Aperture parameter for the Sobel operator.
 *
 * @param borderType Pixel extrapolation method. See BorderTypes.
 */
export declare function cornerEigenValsAndVecs(src: Mat, dst: Mat, blockSize: int, ksize: int, borderType?: int): void;
/**
 * The function runs the Harris corner detector on the image. Similarly to cornerMinEigenVal and
 * cornerEigenValsAndVecs , for each pixel `$(x, y)$` it calculates a `$2\\times2$` gradient covariance
 * matrix `$M^{(x,y)}$` over a `$\\texttt{blockSize} \\times \\texttt{blockSize}$` neighborhood. Then,
 * it computes the following characteristic:
 *
 * `\\[\\texttt{dst} (x,y) = \\mathrm{det} M^{(x,y)} - k \\cdot \\left ( \\mathrm{tr} M^{(x,y)} \\right
 * )^2\\]`
 *
 * Corners in the image can be found as the local maxima of this response map.
 *
 * @param src Input single-channel 8-bit or floating-point image.
 *
 * @param dst Image to store the Harris detector responses. It has the type CV_32FC1 and the same size
 * as src .
 *
 * @param blockSize Neighborhood size (see the details on cornerEigenValsAndVecs ).
 *
 * @param ksize Aperture parameter for the Sobel operator.
 *
 * @param k Harris detector free parameter. See the formula above.
 *
 * @param borderType Pixel extrapolation method. See BorderTypes.
 */
export declare function cornerHarris(src: Mat, dst: Mat, blockSize: int, ksize: int, k: double, borderType?: int): void;
/**
 * The function is similar to cornerEigenValsAndVecs but it calculates and stores only the minimal
 * eigenvalue of the covariance matrix of derivatives, that is, `$\\min(\\lambda_1, \\lambda_2)$` in
 * terms of the formulae in the cornerEigenValsAndVecs description.
 *
 * @param src Input single-channel 8-bit or floating-point image.
 *
 * @param dst Image to store the minimal eigenvalues. It has the type CV_32FC1 and the same size as src
 * .
 *
 * @param blockSize Neighborhood size (see the details on cornerEigenValsAndVecs ).
 *
 * @param ksize Aperture parameter for the Sobel operator.
 *
 * @param borderType Pixel extrapolation method. See BorderTypes.
 */
export declare function cornerMinEigenVal(src: Mat, dst: Mat, blockSize: int, ksize?: int, borderType?: int): void;
/**
 * The function iterates to find the sub-pixel accurate location of corners or radial saddle points, as
 * shown on the figure below.
 *
 *  Sub-pixel accurate corner locator is based on the observation that every vector from the center
 * `$q$` to a point `$p$` located within a neighborhood of `$q$` is orthogonal to the image gradient at
 * `$p$` subject to image and measurement noise. Consider the expression:
 *
 * `\\[\\epsilon _i = {DI_{p_i}}^T \\cdot (q - p_i)\\]`
 *
 * where `${DI_{p_i}}$` is an image gradient at one of the points `$p_i$` in a neighborhood of `$q$` .
 * The value of `$q$` is to be found so that `$\\epsilon_i$` is minimized. A system of equations may be
 * set up with `$\\epsilon_i$` set to zero:
 *
 * `\\[\\sum _i(DI_{p_i} \\cdot {DI_{p_i}}^T) \\cdot q - \\sum _i(DI_{p_i} \\cdot {DI_{p_i}}^T \\cdot
 * p_i)\\]`
 *
 * where the gradients are summed within a neighborhood ("search window") of `$q$` . Calling the first
 * gradient term `$G$` and the second gradient term `$b$` gives:
 *
 * `\\[q = G^{-1} \\cdot b\\]`
 *
 * The algorithm sets the center of the neighborhood window at this new center `$q$` and then iterates
 * until the center stays within a set threshold.
 *
 * @param image Input single-channel, 8-bit or float image.
 *
 * @param corners Initial coordinates of the input corners and refined coordinates provided for output.
 *
 * @param winSize Half of the side length of the search window. For example, if winSize=Size(5,5) ,
 * then a $(5*2+1) \times (5*2+1) = 11 \times 11$ search window is used.
 *
 * @param zeroZone Half of the size of the dead region in the middle of the search zone over which the
 * summation in the formula below is not done. It is used sometimes to avoid possible singularities of
 * the autocorrelation matrix. The value of (-1,-1) indicates that there is no such a size.
 *
 * @param criteria Criteria for termination of the iterative process of corner refinement. That is, the
 * process of corner position refinement stops either after criteria.maxCount iterations or when the
 * corner position moves by less than criteria.epsilon on some iteration.
 */
export declare function cornerSubPix(image: Mat, corners: Mat, winSize: Size, zeroZone: Size, criteria: TermCriteria): void;
/**
 * The [LineSegmentDetector] algorithm is defined using the standard values. Only advanced users may
 * want to edit those, as to tailor it for their own application.
 *
 * Implementation has been removed due original code license conflict
 *
 * @param _refine The way found lines will be refined, see LineSegmentDetectorModes
 *
 * @param _scale The scale of the image that will be used to find the lines. Range (0..1].
 *
 * @param _sigma_scale Sigma for Gaussian filter. It is computed as sigma = _sigma_scale/_scale.
 *
 * @param _quant Bound to the quantization error on the gradient norm.
 *
 * @param _ang_th Gradient angle tolerance in degrees.
 *
 * @param _log_eps Detection threshold: -log10(NFA) > log_eps. Used only when advance refinement is
 * chosen.
 *
 * @param _density_th Minimal density of aligned region points in the enclosing rectangle.
 *
 * @param _n_bins Number of bins in pseudo-ordering of gradient modulus.
 */
export declare function createLineSegmentDetector(_refine?: int, _scale?: double, _sigma_scale?: double, _quant?: double, _ang_th?: double, _log_eps?: double, _density_th?: double, _n_bins?: int): any;
/**
 * The function finds the most prominent corners in the image or in the specified image region, as
 * described in Shi94
 *
 * Function calculates the corner quality measure at every source image pixel using the
 * [cornerMinEigenVal] or [cornerHarris] .
 * Function performs a non-maximum suppression (the local maximums in *3 x 3* neighborhood are
 * retained).
 * The corners with the minimal eigenvalue less than `$\\texttt{qualityLevel} \\cdot \\max_{x,y}
 * qualityMeasureMap(x,y)$` are rejected.
 * The remaining corners are sorted by the quality measure in the descending order.
 * Function throws away each corner for which there is a stronger corner at a distance less than
 * maxDistance.
 *
 * The function can be used to initialize a point-based tracker of an object.
 *
 * If the function is called with different values A and B of the parameter qualityLevel , and A > B,
 * the vector of returned corners with qualityLevel=A will be the prefix of the output vector with
 * qualityLevel=B .
 *
 * [cornerMinEigenVal], [cornerHarris], [calcOpticalFlowPyrLK], [estimateRigidTransform],
 *
 * @param image Input 8-bit or floating-point 32-bit, single-channel image.
 *
 * @param corners Output vector of detected corners.
 *
 * @param maxCorners Maximum number of corners to return. If there are more corners than are found, the
 * strongest of them is returned. maxCorners <= 0 implies that no limit on the maximum is set and all
 * detected corners are returned.
 *
 * @param qualityLevel Parameter characterizing the minimal accepted quality of image corners. The
 * parameter value is multiplied by the best corner quality measure, which is the minimal eigenvalue
 * (see cornerMinEigenVal ) or the Harris function response (see cornerHarris ). The corners with the
 * quality measure less than the product are rejected. For example, if the best corner has the quality
 * measure = 1500, and the qualityLevel=0.01 , then all the corners with the quality measure less than
 * 15 are rejected.
 *
 * @param minDistance Minimum possible Euclidean distance between the returned corners.
 *
 * @param mask Optional region of interest. If the image is not empty (it needs to have the type
 * CV_8UC1 and the same size as image ), it specifies the region in which the corners are detected.
 *
 * @param blockSize Size of an average block for computing a derivative covariation matrix over each
 * pixel neighborhood. See cornerEigenValsAndVecs .
 *
 * @param useHarrisDetector Parameter indicating whether to use a Harris detector (see cornerHarris) or
 * cornerMinEigenVal.
 *
 * @param k Free parameter of the Harris detector.
 */
export declare function goodFeaturesToTrack(image: Mat, corners: Mat, maxCorners: int, qualityLevel: double, minDistance: double, mask?: Mat, blockSize?: int, useHarrisDetector?: bool, k?: double): void;
export declare function goodFeaturesToTrack(image: Mat, corners: Mat, maxCorners: int, qualityLevel: double, minDistance: double, mask: Mat, blockSize: int, gradientSize: int, useHarrisDetector?: bool, k?: double): void;
/**
 * The function finds circles in a grayscale image using a modification of the Hough transform.
 *
 * Example: :
 *
 * ```cpp
 * #include <opencv2/imgproc.hpp>
 * #include <opencv2/highgui.hpp>
 * #include <math.h>
 *
 * using namespace cv;
 * using namespace std;
 *
 * int main(int argc, char** argv)
 * {
 *     Mat img, gray;
 *     if( argc != 2 || !(img=imread(argv[1], 1)).data)
 *         return -1;
 *     cvtColor(img, gray, COLOR_BGR2GRAY);
 *     // smooth it, otherwise a lot of false circles may be detected
 *     GaussianBlur( gray, gray, Size(9, 9), 2, 2 );
 *     vector<Vec3f> circles;
 *     HoughCircles(gray, circles, HOUGH_GRADIENT,
 *                  2, gray.rows/4, 200, 100 );
 *     for( size_t i = 0; i < circles.size(); i++ )
 *     {
 *          Point center(cvRound(circles[i][0]), cvRound(circles[i][1]));
 *          int radius = cvRound(circles[i][2]);
 *          // draw the circle center
 *          circle( img, center, 3, Scalar(0,255,0), -1, 8, 0 );
 *          // draw the circle outline
 *          circle( img, center, radius, Scalar(0,0,255), 3, 8, 0 );
 *     }
 *     namedWindow( "circles", 1 );
 *     imshow( "circles", img );
 *
 *     waitKey(0);
 *     return 0;
 * }
 * ```
 *
 * Usually the function detects the centers of circles well. However, it may fail to find correct
 * radii. You can assist to the function by specifying the radius range ( minRadius and maxRadius ) if
 * you know it. Or, you may set maxRadius to a negative number to return centers only without radius
 * search, and find the correct radius using an additional procedure.
 *
 * [fitEllipse], [minEnclosingCircle]
 *
 * @param image 8-bit, single-channel, grayscale input image.
 *
 * @param circles Output vector of found circles. Each vector is encoded as 3 or 4 element
 * floating-point vector $(x, y, radius)$ or $(x, y, radius, votes)$ .
 *
 * @param method Detection method, see HoughModes. Currently, the only implemented method is
 * HOUGH_GRADIENT
 *
 * @param dp Inverse ratio of the accumulator resolution to the image resolution. For example, if dp=1
 * , the accumulator has the same resolution as the input image. If dp=2 , the accumulator has half as
 * big width and height.
 *
 * @param minDist Minimum distance between the centers of the detected circles. If the parameter is too
 * small, multiple neighbor circles may be falsely detected in addition to a true one. If it is too
 * large, some circles may be missed.
 *
 * @param param1 First method-specific parameter. In case of HOUGH_GRADIENT , it is the higher
 * threshold of the two passed to the Canny edge detector (the lower one is twice smaller).
 *
 * @param param2 Second method-specific parameter. In case of HOUGH_GRADIENT , it is the accumulator
 * threshold for the circle centers at the detection stage. The smaller it is, the more false circles
 * may be detected. Circles, corresponding to the larger accumulator values, will be returned first.
 *
 * @param minRadius Minimum circle radius.
 *
 * @param maxRadius Maximum circle radius. If <= 0, uses the maximum image dimension. If < 0, returns
 * centers without finding the radius.
 */
export declare function HoughCircles(image: Mat, circles: Mat, method: int, dp: double, minDist: double, param1?: double, param2?: double, minRadius?: int, maxRadius?: int): void;
/**
 * The function implements the standard or standard multi-scale Hough transform algorithm for line
 * detection. See  for a good explanation of Hough transform.
 *
 * @param image 8-bit, single-channel binary source image. The image may be modified by the function.
 *
 * @param lines Output vector of lines. Each line is represented by a 2 or 3 element vector $(\rho,
 * \theta)$ or $(\rho, \theta, \textrm{votes})$ . $\rho$ is the distance from the coordinate origin
 * $(0,0)$ (top-left corner of the image). $\theta$ is the line rotation angle in radians ( $0 \sim
 * \textrm{vertical line}, \pi/2 \sim \textrm{horizontal line}$ ). $\textrm{votes}$ is the value of
 * accumulator.
 *
 * @param rho Distance resolution of the accumulator in pixels.
 *
 * @param theta Angle resolution of the accumulator in radians.
 *
 * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough
 * votes ( $>\texttt{threshold}$ ).
 *
 * @param srn For the multi-scale Hough transform, it is a divisor for the distance resolution rho .
 * The coarse accumulator distance resolution is rho and the accurate accumulator resolution is rho/srn
 * . If both srn=0 and stn=0 , the classical Hough transform is used. Otherwise, both these parameters
 * should be positive.
 *
 * @param stn For the multi-scale Hough transform, it is a divisor for the distance resolution theta.
 *
 * @param min_theta For standard and multi-scale Hough transform, minimum angle to check for lines.
 * Must fall between 0 and max_theta.
 *
 * @param max_theta For standard and multi-scale Hough transform, maximum angle to check for lines.
 * Must fall between min_theta and CV_PI.
 */
export declare function HoughLines(image: Mat, lines: Mat, rho: double, theta: double, threshold: int, srn?: double, stn?: double, min_theta?: double, max_theta?: double): void;
/**
 * The function implements the probabilistic Hough transform algorithm for line detection, described in
 * Matas00
 *
 * See the line detection example below:
 *
 * ```cpp
 * #include <opencv2/imgproc.hpp>
 * #include <opencv2/highgui.hpp>
 *
 * using namespace cv;
 * using namespace std;
 *
 * int main(int argc, char** argv)
 * {
 *     Mat src, dst, color_dst;
 *     if( argc != 2 || !(src=imread(argv[1], 0)).data)
 *         return -1;
 *
 *     Canny( src, dst, 50, 200, 3 );
 *     cvtColor( dst, color_dst, COLOR_GRAY2BGR );
 *
 *     vector<Vec4i> lines;
 *     HoughLinesP( dst, lines, 1, CV_PI/180, 80, 30, 10 );
 *     for( size_t i = 0; i < lines.size(); i++ )
 *     {
 *         line( color_dst, Point(lines[i][0], lines[i][1]),
 *         Point( lines[i][2], lines[i][3]), Scalar(0,0,255), 3, 8 );
 *     }
 *     namedWindow( "Source", 1 );
 *     imshow( "Source", src );
 *
 *     namedWindow( "Detected Lines", 1 );
 *     imshow( "Detected Lines", color_dst );
 *
 *     waitKey(0);
 *     return 0;
 * }
 * ```
 *
 *  This is a sample picture the function parameters have been tuned for:
 *
 *  And this is the output of the above program in case of the probabilistic Hough transform:
 *
 * [LineSegmentDetector]
 *
 * @param image 8-bit, single-channel binary source image. The image may be modified by the function.
 *
 * @param lines Output vector of lines. Each line is represented by a 4-element vector $(x_1, y_1, x_2,
 * y_2)$ , where $(x_1,y_1)$ and $(x_2, y_2)$ are the ending points of each detected line segment.
 *
 * @param rho Distance resolution of the accumulator in pixels.
 *
 * @param theta Angle resolution of the accumulator in radians.
 *
 * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough
 * votes ( $>\texttt{threshold}$ ).
 *
 * @param minLineLength Minimum line length. Line segments shorter than that are rejected.
 *
 * @param maxLineGap Maximum allowed gap between points on the same line to link them.
 */
export declare function HoughLinesP(image: Mat, lines: Mat, rho: double, theta: double, threshold: int, minLineLength?: double, maxLineGap?: double): void;
/**
 * The function finds lines in a set of points using a modification of the Hough transform.
 *
 * ```cpp
 * #include <opencv2/core.hpp>
 * #include <opencv2/imgproc.hpp>
 *
 * using namespace cv;
 * using namespace std;
 *
 * int main()
 * {
 *     Mat lines;
 *     vector<Vec3d> line3d;
 *     vector<Point2f> point;
 *     const static float Points[20][2] = {
 *     { 0.0f,   369.0f }, { 10.0f,  364.0f }, { 20.0f,  358.0f }, { 30.0f,  352.0f },
 *     { 40.0f,  346.0f }, { 50.0f,  341.0f }, { 60.0f,  335.0f }, { 70.0f,  329.0f },
 *     { 80.0f,  323.0f }, { 90.0f,  318.0f }, { 100.0f, 312.0f }, { 110.0f, 306.0f },
 *     { 120.0f, 300.0f }, { 130.0f, 295.0f }, { 140.0f, 289.0f }, { 150.0f, 284.0f },
 *     { 160.0f, 277.0f }, { 170.0f, 271.0f }, { 180.0f, 266.0f }, { 190.0f, 260.0f }
 *     };
 *
 *     for (int i = 0; i < 20; i++)
 *     {
 *         point.push_back(Point2f(Points[i][0],Points[i][1]));
 *     }
 *
 *     double rhoMin = 0.0f, rhoMax = 360.0f, rhoStep = 1;
 *     double thetaMin = 0.0f, thetaMax = CV_PI / 2.0f, thetaStep = CV_PI / 180.0f;
 *
 *     HoughLinesPointSet(point, lines, 20, 1,
 *                        rhoMin, rhoMax, rhoStep,
 *                        thetaMin, thetaMax, thetaStep);
 *
 *     lines.copyTo(line3d);
 *     printf("votes:%d, rho:%.7f, theta:%.7f\\n",(int)line3d.at(0).val[0], line3d.at(0).val[1],
 * line3d.at(0).val[2]);
 * }
 * ```
 *
 * @param _point Input vector of points. Each vector must be encoded as a Point vector $(x,y)$. Type
 * must be CV_32FC2 or CV_32SC2.
 *
 * @param _lines Output vector of found lines. Each vector is encoded as a vector<Vec3d> $(votes, rho,
 * theta)$. The larger the value of 'votes', the higher the reliability of the Hough line.
 *
 * @param lines_max Max count of hough lines.
 *
 * @param threshold Accumulator threshold parameter. Only those lines are returned that get enough
 * votes ( $>\texttt{threshold}$ )
 *
 * @param min_rho Minimum Distance value of the accumulator in pixels.
 *
 * @param max_rho Maximum Distance value of the accumulator in pixels.
 *
 * @param rho_step Distance resolution of the accumulator in pixels.
 *
 * @param min_theta Minimum angle value of the accumulator in radians.
 *
 * @param max_theta Maximum angle value of the accumulator in radians.
 *
 * @param theta_step Angle resolution of the accumulator in radians.
 */
export declare function HoughLinesPointSet(_point: Mat, _lines: Mat, lines_max: int, threshold: int, min_rho: double, max_rho: double, rho_step: double, min_theta: double, max_theta: double, theta_step: double): void;
/**
 * The function calculates the complex spatial derivative-based function of the source image
 *
 * `\\[\\texttt{dst} = (D_x \\texttt{src} )^2 \\cdot D_{yy} \\texttt{src} + (D_y \\texttt{src} )^2
 * \\cdot D_{xx} \\texttt{src} - 2 D_x \\texttt{src} \\cdot D_y \\texttt{src} \\cdot D_{xy}
 * \\texttt{src}\\]`
 *
 * where `$D_x$`, `$D_y$` are the first image derivatives, `$D_{xx}$`, `$D_{yy}$` are the second image
 * derivatives, and `$D_{xy}$` is the mixed derivative.
 *
 * The corners can be found as local maximums of the functions, as shown below:
 *
 * ```cpp
 * Mat corners, dilated_corners;
 * preCornerDetect(image, corners, 3);
 * // dilation with 3x3 rectangular structuring element
 * dilate(corners, dilated_corners, Mat(), 1);
 * Mat corner_mask = corners == dilated_corners;
 * ```
 *
 * @param src Source single-channel 8-bit of floating-point image.
 *
 * @param dst Output image that has the type CV_32F and the same size as src .
 *
 * @param ksize Aperture size of the Sobel .
 *
 * @param borderType Pixel extrapolation method. See BorderTypes.
 */
export declare function preCornerDetect(src: Mat, dst: Mat, ksize: int, borderType?: int): void;
/**
 * classical or standard Hough transform. Every line is represented by two floating-point numbers
 * `$(\\rho, \\theta)$` , where `$\\rho$` is a distance between (0,0) point and the line, and
 * `$\\theta$` is the angle between x-axis and the normal to the line. Thus, the matrix must be (the
 * created sequence will be) of CV_32FC2 type
 *
 */
export declare const HOUGH_STANDARD: HoughModes;
/**
 * probabilistic Hough transform (more efficient in case if the picture contains a few long linear
 * segments). It returns line segments rather than the whole line. Each segment is represented by
 * starting and ending points, and the matrix must be (the created sequence will be) of the CV_32SC4
 * type.
 *
 */
export declare const HOUGH_PROBABILISTIC: HoughModes;
/**
 * multi-scale variant of the classical Hough transform. The lines are encoded the same way as
 * HOUGH_STANDARD.
 *
 */
export declare const HOUGH_MULTI_SCALE: HoughModes;
export declare const HOUGH_GRADIENT: HoughModes;
export declare const LSD_REFINE_NONE: LineSegmentDetectorModes;
export declare const LSD_REFINE_STD: LineSegmentDetectorModes;
/**
 * Advanced refinement. Number of false alarms is calculated, lines are refined through increase of
 * precision, decrement in size, etc.
 *
 */
export declare const LSD_REFINE_ADV: LineSegmentDetectorModes;
export type HoughModes = any;
export type LineSegmentDetectorModes = any;
/**
 * The function applies bilateral filtering to the input image, as described in  bilateralFilter can
 * reduce unwanted noise very well while keeping edges fairly sharp. However, it is very slow compared
 * to most filters.
 *
 * Sigma values*: For simplicity, you can set the 2 sigma values to be the same. If they are small (<
 * 10), the filter will not have much effect, whereas if they are large (> 150), they will have a very
 * strong effect, making the image look "cartoonish".
 *
 * Filter size*: Large filters (d > 5) are very slow, so it is recommended to use d=5 for real-time
 * applications, and perhaps d=9 for offline applications that need heavy noise filtering.
 *
 * This filter does not work inplace.
 *
 * @param src Source 8-bit or floating-point, 1-channel or 3-channel image.
 *
 * @param dst Destination image of the same size and type as src .
 *
 * @param d Diameter of each pixel neighborhood that is used during filtering. If it is non-positive,
 * it is computed from sigmaSpace.
 *
 * @param sigmaColor Filter sigma in the color space. A larger value of the parameter means that
 * farther colors within the pixel neighborhood (see sigmaSpace) will be mixed together, resulting in
 * larger areas of semi-equal color.
 *
 * @param sigmaSpace Filter sigma in the coordinate space. A larger value of the parameter means that
 * farther pixels will influence each other as long as their colors are close enough (see sigmaColor ).
 * When d>0, it specifies the neighborhood size regardless of sigmaSpace. Otherwise, d is proportional
 * to sigmaSpace.
 *
 * @param borderType border mode used to extrapolate pixels outside of the image, see BorderTypes
 */
export declare function bilateralFilter(src: Mat, dst: Mat, d: int, sigmaColor: double, sigmaSpace: double, borderType?: int): void;
/**
 * The function smooths an image using the kernel:
 *
 * `\\[\\texttt{K} = \\frac{1}{\\texttt{ksize.width*ksize.height}} \\begin{bmatrix} 1 & 1 & 1 & \\cdots
 * & 1 & 1 \\\\ 1 & 1 & 1 & \\cdots & 1 & 1 \\\\ \\hdotsfor{6} \\\\ 1 & 1 & 1 & \\cdots & 1 & 1 \\\\
 * \\end{bmatrix}\\]`
 *
 * The call `blur(src, dst, ksize, anchor, borderType)` is equivalent to `boxFilter(src, dst,
 * src.type(), anchor, true, borderType)`.
 *
 * [boxFilter], [bilateralFilter], [GaussianBlur], [medianBlur]
 *
 * @param src input image; it can have any number of channels, which are processed independently, but
 * the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
 *
 * @param dst output image of the same size and type as src.
 *
 * @param ksize blurring kernel size.
 *
 * @param anchor anchor point; default value Point(-1,-1) means that the anchor is at the kernel
 * center.
 *
 * @param borderType border mode used to extrapolate pixels outside of the image, see BorderTypes
 */
declare function blur$1(src: Mat, dst: Mat, ksize: Size, anchor?: Point, borderType?: int): void;
/**
 * The function smooths an image using the kernel:
 *
 * `\\[\\texttt{K} = \\alpha \\begin{bmatrix} 1 & 1 & 1 & \\cdots & 1 & 1 \\\\ 1 & 1 & 1 & \\cdots & 1
 * & 1 \\\\ \\hdotsfor{6} \\\\ 1 & 1 & 1 & \\cdots & 1 & 1 \\end{bmatrix}\\]`
 *
 * where
 *
 * `\\[\\alpha = \\fork{\\frac{1}{\\texttt{ksize.width*ksize.height}}}{when
 * \\texttt{normalize=true}}{1}{otherwise}\\]`
 *
 * Unnormalized box filter is useful for computing various integral characteristics over each pixel
 * neighborhood, such as covariance matrices of image derivatives (used in dense optical flow
 * algorithms, and so on). If you need to compute pixel sums over variable-size windows, use
 * [integral].
 *
 * [blur], [bilateralFilter], [GaussianBlur], [medianBlur], [integral]
 *
 * @param src input image.
 *
 * @param dst output image of the same size and type as src.
 *
 * @param ddepth the output image depth (-1 to use src.depth()).
 *
 * @param ksize blurring kernel size.
 *
 * @param anchor anchor point; default value Point(-1,-1) means that the anchor is at the kernel
 * center.
 *
 * @param normalize flag, specifying whether the kernel is normalized by its area or not.
 *
 * @param borderType border mode used to extrapolate pixels outside of the image, see BorderTypes
 */
export declare function boxFilter(src: Mat, dst: Mat, ddepth: int, ksize: Size, anchor?: Point, normalize?: bool, borderType?: int): void;
/**
 * The function constructs a vector of images and builds the Gaussian pyramid by recursively applying
 * pyrDown to the previously built pyramid layers, starting from `dst[0]==src`.
 *
 * @param src Source image. Check pyrDown for the list of supported types.
 *
 * @param dst Destination vector of maxlevel+1 images of the same type as src. dst[0] will be the same
 * as src. dst[1] is the next pyramid layer, a smoothed and down-sized src, and so on.
 *
 * @param maxlevel 0-based index of the last (the smallest) pyramid layer. It must be non-negative.
 *
 * @param borderType Pixel extrapolation method, see BorderTypes (BORDER_CONSTANT isn't supported)
 */
export declare function buildPyramid(src: Mat, dst: MatVector, maxlevel: int, borderType?: int): void;
/**
 * The function dilates the source image using the specified structuring element that determines the
 * shape of a pixel neighborhood over which the maximum is taken: `\\[\\texttt{dst} (x,y) = \\max
 * _{(x',y'): \\, \\texttt{element} (x',y') \\ne0 } \\texttt{src} (x+x',y+y')\\]`
 *
 * The function supports the in-place mode. Dilation can be applied several ( iterations ) times. In
 * case of multi-channel images, each channel is processed independently.
 *
 * [erode], [morphologyEx], [getStructuringElement]
 *
 * @param src input image; the number of channels can be arbitrary, but the depth should be one of
 * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
 *
 * @param dst output image of the same size and type as src.
 *
 * @param kernel structuring element used for dilation; if elemenat=Mat(), a 3 x 3 rectangular
 * structuring element is used. Kernel can be created using getStructuringElement
 *
 * @param anchor position of the anchor within the element; default value (-1, -1) means that the
 * anchor is at the element center.
 *
 * @param iterations number of times dilation is applied.
 *
 * @param borderType pixel extrapolation method, see BorderTypes
 *
 * @param borderValue border value in case of a constant border
 */
export declare function dilate(src: Mat, dst: Mat, kernel: Mat, anchor?: Point, iterations?: int, borderType?: int, borderValue?: any): void;
/**
 * The function erodes the source image using the specified structuring element that determines the
 * shape of a pixel neighborhood over which the minimum is taken:
 *
 * `\\[\\texttt{dst} (x,y) = \\min _{(x',y'): \\, \\texttt{element} (x',y') \\ne0 } \\texttt{src}
 * (x+x',y+y')\\]`
 *
 * The function supports the in-place mode. Erosion can be applied several ( iterations ) times. In
 * case of multi-channel images, each channel is processed independently.
 *
 * [dilate], [morphologyEx], [getStructuringElement]
 *
 * @param src input image; the number of channels can be arbitrary, but the depth should be one of
 * CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
 *
 * @param dst output image of the same size and type as src.
 *
 * @param kernel structuring element used for erosion; if element=Mat(), a 3 x 3 rectangular
 * structuring element is used. Kernel can be created using getStructuringElement.
 *
 * @param anchor position of the anchor within the element; default value (-1, -1) means that the
 * anchor is at the element center.
 *
 * @param iterations number of times erosion is applied.
 *
 * @param borderType pixel extrapolation method, see BorderTypes
 *
 * @param borderValue border value in case of a constant border
 */
export declare function erode(src: Mat, dst: Mat, kernel: Mat, anchor?: Point, iterations?: int, borderType?: int, borderValue?: any): void;
/**
 * The function applies an arbitrary linear filter to an image. In-place operation is supported. When
 * the aperture is partially outside the image, the function interpolates outlier pixel values
 * according to the specified border mode.
 *
 * The function does actually compute correlation, not the convolution:
 *
 * `\\[\\texttt{dst} (x,y) = \\sum _{ \\stackrel{0\\leq x' < \\texttt{kernel.cols},}{0\\leq y' <
 * \\texttt{kernel.rows}} } \\texttt{kernel} (x',y')* \\texttt{src} (x+x'- \\texttt{anchor.x} ,y+y'-
 * \\texttt{anchor.y} )\\]`
 *
 * That is, the kernel is not mirrored around the anchor point. If you need a real convolution, flip
 * the kernel using [flip] and set the new anchor to `(kernel.cols - anchor.x - 1, kernel.rows -
 * anchor.y - 1)`.
 *
 * The function uses the DFT-based algorithm in case of sufficiently large kernels (~`11 x 11` or
 * larger) and the direct algorithm for small kernels.
 *
 * [sepFilter2D], [dft], [matchTemplate]
 *
 * @param src input image.
 *
 * @param dst output image of the same size and the same number of channels as src.
 *
 * @param ddepth desired depth of the destination image, see combinations
 *
 * @param kernel convolution kernel (or rather a correlation kernel), a single-channel floating point
 * matrix; if you want to apply different kernels to different channels, split the image into separate
 * color planes using split and process them individually.
 *
 * @param anchor anchor of the kernel that indicates the relative position of a filtered point within
 * the kernel; the anchor should lie within the kernel; default value (-1,-1) means that the anchor is
 * at the kernel center.
 *
 * @param delta optional value added to the filtered pixels before storing them in dst.
 *
 * @param borderType pixel extrapolation method, see BorderTypes
 */
export declare function filter2D(src: Mat, dst: Mat, ddepth: int, kernel: Mat, anchor?: Point, delta?: double, borderType?: int): void;
/**
 * The function convolves the source image with the specified Gaussian kernel. In-place filtering is
 * supported.
 *
 * [sepFilter2D], [filter2D], [blur], [boxFilter], [bilateralFilter], [medianBlur]
 *
 * @param src input image; the image can have any number of channels, which are processed
 * independently, but the depth should be CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.
 *
 * @param dst output image of the same size and type as src.
 *
 * @param ksize Gaussian kernel size. ksize.width and ksize.height can differ but they both must be
 * positive and odd. Or, they can be zero's and then they are computed from sigma.
 *
 * @param sigmaX Gaussian kernel standard deviation in X direction.
 *
 * @param sigmaY Gaussian kernel standard deviation in Y direction; if sigmaY is zero, it is set to be
 * equal to sigmaX, if both sigmas are zeros, they are computed from ksize.width and ksize.height,
 * respectively (see getGaussianKernel for details); to fully control the result regardless of possible
 * future modifications of all this semantics, it is recommended to specify all of ksize, sigmaX, and
 * sigmaY.
 *
 * @param borderType pixel extrapolation method, see BorderTypes
 */
export declare function GaussianBlur(src: Mat, dst: Mat, ksize: Size, sigmaX: double, sigmaY?: double, borderType?: int): void;
/**
 * The function computes and returns the filter coefficients for spatial image derivatives. When
 * `ksize=FILTER_SCHARR`, the Scharr `$3 \\times 3$` kernels are generated (see [Scharr]). Otherwise,
 * Sobel kernels are generated (see [Sobel]). The filters are normally passed to [sepFilter2D] or to
 *
 * @param kx Output matrix of row filter coefficients. It has the type ktype .
 *
 * @param ky Output matrix of column filter coefficients. It has the type ktype .
 *
 * @param dx Derivative order in respect of x.
 *
 * @param dy Derivative order in respect of y.
 *
 * @param ksize Aperture size. It can be FILTER_SCHARR, 1, 3, 5, or 7.
 *
 * @param normalize Flag indicating whether to normalize (scale down) the filter coefficients or not.
 * Theoretically, the coefficients should have the denominator $=2^{ksize*2-dx-dy-2}$. If you are going
 * to filter floating-point images, you are likely to use the normalized kernels. But if you compute
 * derivatives of an 8-bit image, store the results in a 16-bit image, and wish to preserve all the
 * fractional bits, you may want to set normalize=false .
 *
 * @param ktype Type of filter coefficients. It can be CV_32f or CV_64F .
 */
export declare function getDerivKernels(kx: Mat, ky: Mat, dx: int, dy: int, ksize: int, normalize?: bool, ktype?: int): void;
/**
 * For more details about gabor filter equations and parameters, see: .
 *
 * @param ksize Size of the filter returned.
 *
 * @param sigma Standard deviation of the gaussian envelope.
 *
 * @param theta Orientation of the normal to the parallel stripes of a Gabor function.
 *
 * @param lambd Wavelength of the sinusoidal factor.
 *
 * @param gamma Spatial aspect ratio.
 *
 * @param psi Phase offset.
 *
 * @param ktype Type of filter coefficients. It can be CV_32F or CV_64F .
 */
export declare function getGaborKernel(ksize: Size, sigma: double, theta: double, lambd: double, gamma: double, psi?: double, ktype?: int): Mat;
/**
 * The function computes and returns the `$\\texttt{ksize} \\times 1$` matrix of Gaussian filter
 * coefficients:
 *
 * `\\[G_i= \\alpha *e^{-(i-( \\texttt{ksize} -1)/2)^2/(2* \\texttt{sigma}^2)},\\]`
 *
 * where `$i=0..\\texttt{ksize}-1$` and `$\\alpha$` is the scale factor chosen so that `$\\sum_i
 * G_i=1$`.
 *
 * Two of such generated kernels can be passed to sepFilter2D. Those functions automatically recognize
 * smoothing kernels (a symmetrical kernel with sum of weights equal to 1) and handle them accordingly.
 * You may also use the higher-level GaussianBlur.
 *
 * [sepFilter2D], [getDerivKernels], [getStructuringElement], [GaussianBlur]
 *
 * @param ksize Aperture size. It should be odd ( $\texttt{ksize} \mod 2 = 1$ ) and positive.
 *
 * @param sigma Gaussian standard deviation. If it is non-positive, it is computed from ksize as sigma
 * = 0.3*((ksize-1)*0.5 - 1) + 0.8.
 *
 * @param ktype Type of filter coefficients. It can be CV_32F or CV_64F .
 */
export declare function getGaussianKernel(ksize: int, sigma: double, ktype?: int): Mat;
/**
 * The function constructs and returns the structuring element that can be further passed to [erode],
 * [dilate] or [morphologyEx]. But you can also construct an arbitrary binary mask yourself and use it
 * as the structuring element.
 *
 * @param shape Element shape that could be one of MorphShapes
 *
 * @param ksize Size of the structuring element.
 *
 * @param anchor Anchor position within the element. The default value $(-1, -1)$ means that the anchor
 * is at the center. Note that only the shape of a cross-shaped element depends on the anchor position.
 * In other cases the anchor just regulates how much the result of the morphological operation is
 * shifted.
 */
export declare function getStructuringElement(shape: int, ksize: Size, anchor?: Point): Mat;
/**
 * The function calculates the Laplacian of the source image by adding up the second x and y
 * derivatives calculated using the Sobel operator:
 *
 * `\\[\\texttt{dst} = \\Delta \\texttt{src} = \\frac{\\partial^2 \\texttt{src}}{\\partial x^2} +
 * \\frac{\\partial^2 \\texttt{src}}{\\partial y^2}\\]`
 *
 * This is done when `ksize > 1`. When `ksize == 1`, the Laplacian is computed by filtering the image
 * with the following `$3 \\times 3$` aperture:
 *
 * `\\[\\vecthreethree {0}{1}{0}{1}{-4}{1}{0}{1}{0}\\]`
 *
 * [Sobel], [Scharr]
 *
 * @param src Source image.
 *
 * @param dst Destination image of the same size and the same number of channels as src .
 *
 * @param ddepth Desired depth of the destination image.
 *
 * @param ksize Aperture size used to compute the second-derivative filters. See getDerivKernels for
 * details. The size must be positive and odd.
 *
 * @param scale Optional scale factor for the computed Laplacian values. By default, no scaling is
 * applied. See getDerivKernels for details.
 *
 * @param delta Optional delta value that is added to the results prior to storing them in dst .
 *
 * @param borderType Pixel extrapolation method, see BorderTypes
 */
export declare function Laplacian(src: Mat, dst: Mat, ddepth: int, ksize?: int, scale?: double, delta?: double, borderType?: int): void;
/**
 * The function smoothes an image using the median filter with the `$\\texttt{ksize} \\times
 * \\texttt{ksize}$` aperture. Each channel of a multi-channel image is processed independently.
 * In-place operation is supported.
 *
 * The median filter uses [BORDER_REPLICATE] internally to cope with border pixels, see [BorderTypes]
 *
 * [bilateralFilter], [blur], [boxFilter], [GaussianBlur]
 *
 * @param src input 1-, 3-, or 4-channel image; when ksize is 3 or 5, the image depth should be CV_8U,
 * CV_16U, or CV_32F, for larger aperture sizes, it can only be CV_8U.
 *
 * @param dst destination array of the same size and type as src.
 *
 * @param ksize aperture linear size; it must be odd and greater than 1, for example: 3, 5, 7 ...
 */
export declare function medianBlur(src: Mat, dst: Mat, ksize: int): void;
export declare function morphologyDefaultBorderValue(): Scalar;
/**
 * The function [cv::morphologyEx] can perform advanced morphological transformations using an erosion
 * and dilation as basic operations.
 *
 * Any of the operations can be done in-place. In case of multi-channel images, each channel is
 * processed independently.
 *
 * [dilate], [erode], [getStructuringElement]
 *
 * The number of iterations is the number of times erosion or dilatation operation will be applied. For
 * instance, an opening operation ([MORPH_OPEN]) with two iterations is equivalent to apply
 * successively: erode -> erode -> dilate -> dilate (and not erode -> dilate -> erode -> dilate).
 *
 * @param src Source image. The number of channels can be arbitrary. The depth should be one of CV_8U,
 * CV_16U, CV_16S, CV_32F or CV_64F.
 *
 * @param dst Destination image of the same size and type as source image.
 *
 * @param op Type of a morphological operation, see MorphTypes
 *
 * @param kernel Structuring element. It can be created using getStructuringElement.
 *
 * @param anchor Anchor position with the kernel. Negative values mean that the anchor is at the kernel
 * center.
 *
 * @param iterations Number of times erosion and dilation are applied.
 *
 * @param borderType Pixel extrapolation method, see BorderTypes
 *
 * @param borderValue Border value in case of a constant border. The default value has a special
 * meaning.
 */
export declare function morphologyEx(src: Mat, dst: Mat, op: int | MorphTypes, kernel: Mat, anchor?: Point, iterations?: int, borderType?: int, borderValue?: any): void;
/**
 * By default, size of the output image is computed as `Size((src.cols+1)/2, (src.rows+1)/2)`, but in
 * any case, the following conditions should be satisfied:
 *
 * `\\[\\begin{array}{l} | \\texttt{dstsize.width} *2-src.cols| \\leq 2 \\\\ | \\texttt{dstsize.height}
 * *2-src.rows| \\leq 2 \\end{array}\\]`
 *
 * The function performs the downsampling step of the Gaussian pyramid construction. First, it
 * convolves the source image with the kernel:
 *
 * `\\[\\frac{1}{256} \\begin{bmatrix} 1 & 4 & 6 & 4 & 1 \\\\ 4 & 16 & 24 & 16 & 4 \\\\ 6 & 24 & 36 &
 * 24 & 6 \\\\ 4 & 16 & 24 & 16 & 4 \\\\ 1 & 4 & 6 & 4 & 1 \\end{bmatrix}\\]`
 *
 * Then, it downsamples the image by rejecting even rows and columns.
 *
 * @param src input image.
 *
 * @param dst output image; it has the specified size and the same type as src.
 *
 * @param dstsize size of the output image.
 *
 * @param borderType Pixel extrapolation method, see BorderTypes (BORDER_CONSTANT isn't supported)
 */
export declare function pyrDown(src: Mat, dst: Mat, dstsize?: any, borderType?: int): void;
/**
 * The function implements the filtering stage of meanshift segmentation, that is, the output of the
 * function is the filtered "posterized" image with color gradients and fine-grain texture flattened.
 * At every pixel (X,Y) of the input image (or down-sized input image, see below) the function executes
 * meanshift iterations, that is, the pixel (X,Y) neighborhood in the joint space-color hyperspace is
 * considered:
 *
 * `\\[(x,y): X- \\texttt{sp} \\le x \\le X+ \\texttt{sp} , Y- \\texttt{sp} \\le y \\le Y+ \\texttt{sp}
 * , ||(R,G,B)-(r,g,b)|| \\le \\texttt{sr}\\]`
 *
 * where (R,G,B) and (r,g,b) are the vectors of color components at (X,Y) and (x,y), respectively
 * (though, the algorithm does not depend on the color space used, so any 3-component color space can
 * be used instead). Over the neighborhood the average spatial value (X',Y') and average color vector
 * (R',G',B') are found and they act as the neighborhood center on the next iteration:
 *
 * `\\[(X,Y)~(X',Y'), (R,G,B)~(R',G',B').\\]`
 *
 * After the iterations over, the color components of the initial pixel (that is, the pixel from where
 * the iterations started) are set to the final value (average color at the last iteration):
 *
 * `\\[I(X,Y) <- (R*,G*,B*)\\]`
 *
 * When maxLevel > 0, the gaussian pyramid of maxLevel+1 levels is built, and the above procedure is
 * run on the smallest layer first. After that, the results are propagated to the larger layer and the
 * iterations are run again only on those pixels where the layer colors differ by more than sr from the
 * lower-resolution layer of the pyramid. That makes boundaries of color regions sharper. Note that the
 * results will be actually different from the ones obtained by running the meanshift procedure on the
 * whole original image (i.e. when maxLevel==0).
 *
 * @param src The source 8-bit, 3-channel image.
 *
 * @param dst The destination image of the same format and the same size as the source.
 *
 * @param sp The spatial window radius.
 *
 * @param sr The color window radius.
 *
 * @param maxLevel Maximum level of the pyramid for the segmentation.
 *
 * @param termcrit Termination criteria: when to stop meanshift iterations.
 */
export declare function pyrMeanShiftFiltering(src: Mat, dst: Mat, sp: double, sr: double, maxLevel?: int, termcrit?: TermCriteria): void;
/**
 * By default, size of the output image is computed as `Size(src.cols\\*2, (src.rows\\*2)`, but in any
 * case, the following conditions should be satisfied:
 *
 * `\\[\\begin{array}{l} | \\texttt{dstsize.width} -src.cols*2| \\leq ( \\texttt{dstsize.width} \\mod
 * 2) \\\\ | \\texttt{dstsize.height} -src.rows*2| \\leq ( \\texttt{dstsize.height} \\mod 2)
 * \\end{array}\\]`
 *
 * The function performs the upsampling step of the Gaussian pyramid construction, though it can
 * actually be used to construct the Laplacian pyramid. First, it upsamples the source image by
 * injecting even zero rows and columns and then convolves the result with the same kernel as in
 * pyrDown multiplied by 4.
 *
 * @param src input image.
 *
 * @param dst output image. It has the specified size and the same type as src .
 *
 * @param dstsize size of the output image.
 *
 * @param borderType Pixel extrapolation method, see BorderTypes (only BORDER_DEFAULT is supported)
 */
export declare function pyrUp(src: Mat, dst: Mat, dstsize?: any, borderType?: int): void;
/**
 * The function computes the first x- or y- spatial image derivative using the Scharr operator. The
 * call
 *
 * `\\[\\texttt{Scharr(src, dst, ddepth, dx, dy, scale, delta, borderType)}\\]`
 *
 * is equivalent to
 *
 * `\\[\\texttt{Sobel(src, dst, ddepth, dx, dy, FILTER_SCHARR, scale, delta, borderType)} .\\]`
 *
 * [cartToPolar]
 *
 * @param src input image.
 *
 * @param dst output image of the same size and the same number of channels as src.
 *
 * @param ddepth output image depth, see combinations
 *
 * @param dx order of the derivative x.
 *
 * @param dy order of the derivative y.
 *
 * @param scale optional scale factor for the computed derivative values; by default, no scaling is
 * applied (see getDerivKernels for details).
 *
 * @param delta optional delta value that is added to the results prior to storing them in dst.
 *
 * @param borderType pixel extrapolation method, see BorderTypes
 */
export declare function Scharr(src: Mat, dst: Mat, ddepth: int, dx: int, dy: int, scale?: double, delta?: double, borderType?: int): void;
/**
 * The function applies a separable linear filter to the image. That is, first, every row of src is
 * filtered with the 1D kernel kernelX. Then, every column of the result is filtered with the 1D kernel
 * kernelY. The final result shifted by delta is stored in dst .
 *
 * [filter2D], [Sobel], [GaussianBlur], [boxFilter], [blur]
 *
 * @param src Source image.
 *
 * @param dst Destination image of the same size and the same number of channels as src .
 *
 * @param ddepth Destination image depth, see combinations
 *
 * @param kernelX Coefficients for filtering each row.
 *
 * @param kernelY Coefficients for filtering each column.
 *
 * @param anchor Anchor position within the kernel. The default value $(-1,-1)$ means that the anchor
 * is at the kernel center.
 *
 * @param delta Value added to the filtered results before storing them.
 *
 * @param borderType Pixel extrapolation method, see BorderTypes
 */
export declare function sepFilter2D(src: Mat, dst: Mat, ddepth: int, kernelX: Mat, kernelY: Mat, anchor?: Point, delta?: double, borderType?: int): void;
/**
 * In all cases except one, the `$\\texttt{ksize} \\times \\texttt{ksize}$` separable kernel is used to
 * calculate the derivative. When `$\\texttt{ksize = 1}$`, the `$3 \\times 1$` or `$1 \\times 3$`
 * kernel is used (that is, no Gaussian smoothing is done). `ksize = 1` can only be used for the first
 * or the second x- or y- derivatives.
 *
 * There is also the special value `ksize = [FILTER_SCHARR] (-1)` that corresponds to the `$3\\times3$`
 * Scharr filter that may give more accurate results than the `$3\\times3$` Sobel. The Scharr aperture
 * is
 *
 * `\\[\\vecthreethree{-3}{0}{3}{-10}{0}{10}{-3}{0}{3}\\]`
 *
 * for the x-derivative, or transposed for the y-derivative.
 *
 * The function calculates an image derivative by convolving the image with the appropriate kernel:
 *
 * `\\[\\texttt{dst} = \\frac{\\partial^{xorder+yorder} \\texttt{src}}{\\partial x^{xorder} \\partial
 * y^{yorder}}\\]`
 *
 * The Sobel operators combine Gaussian smoothing and differentiation, so the result is more or less
 * resistant to the noise. Most often, the function is called with ( xorder = 1, yorder = 0, ksize = 3)
 * or ( xorder = 0, yorder = 1, ksize = 3) to calculate the first x- or y- image derivative. The first
 * case corresponds to a kernel of:
 *
 * `\\[\\vecthreethree{-1}{0}{1}{-2}{0}{2}{-1}{0}{1}\\]`
 *
 * The second case corresponds to a kernel of:
 *
 * `\\[\\vecthreethree{-1}{-2}{-1}{0}{0}{0}{1}{2}{1}\\]`
 *
 * [Scharr], [Laplacian], [sepFilter2D], [filter2D], [GaussianBlur], [cartToPolar]
 *
 * @param src input image.
 *
 * @param dst output image of the same size and the same number of channels as src .
 *
 * @param ddepth output image depth, see combinations; in the case of 8-bit input images it will result
 * in truncated derivatives.
 *
 * @param dx order of the derivative x.
 *
 * @param dy order of the derivative y.
 *
 * @param ksize size of the extended Sobel kernel; it must be 1, 3, 5, or 7.
 *
 * @param scale optional scale factor for the computed derivative values; by default, no scaling is
 * applied (see getDerivKernels for details).
 *
 * @param delta optional delta value that is added to the results prior to storing them in dst.
 *
 * @param borderType pixel extrapolation method, see BorderTypes
 */
export declare function Sobel(src: Mat, dst: Mat, ddepth: int, dx: int, dy: int, ksize?: int, scale?: double, delta?: double, borderType?: int): void;
/**
 * Equivalent to calling:
 *
 * ```cpp
 * Sobel( src, dx, CV_16SC1, 1, 0, 3 );
 * Sobel( src, dy, CV_16SC1, 0, 1, 3 );
 * ```
 *
 * [Sobel]
 *
 * @param src input image.
 *
 * @param dx output image with first-order derivative in x.
 *
 * @param dy output image with first-order derivative in y.
 *
 * @param ksize size of Sobel kernel. It must be 3.
 *
 * @param borderType pixel extrapolation method, see BorderTypes
 */
export declare function spatialGradient(src: Mat, dx: Mat, dy: Mat, ksize?: int, borderType?: int): void;
/**
 * For every pixel `$ (x, y) $` in the source image, the function calculates the sum of squares of
 * those neighboring pixel values which overlap the filter placed over the pixel `$ (x, y) $`.
 *
 * The unnormalized square box filter can be useful in computing local image statistics such as the the
 * local variance and standard deviation around the neighborhood of a pixel.
 *
 * [boxFilter]
 *
 * @param src input image
 *
 * @param dst output image of the same size and type as _src
 *
 * @param ddepth the output image depth (-1 to use src.depth())
 *
 * @param ksize kernel size
 *
 * @param anchor kernel anchor point. The default value of Point(-1, -1) denotes that the anchor is at
 * the kernel center.
 *
 * @param normalize flag, specifying whether the kernel is to be normalized by it's area or not.
 *
 * @param borderType border mode used to extrapolate pixels outside of the image, see BorderTypes
 */
export declare function sqrBoxFilter(src: Mat, dst: Mat, ddepth: int, ksize: Size, anchor?: Point, normalize?: bool, borderType?: int): void;
export declare const MORPH_RECT: MorphShapes;
/**
 * a cross-shaped structuring element: `\\[E_{ij} = \\fork{1}{if i=\\texttt{anchor.y} or
 * j=\\texttt{anchor.x}}{0}{otherwise}\\]`
 *
 */
export declare const MORPH_CROSS: MorphShapes;
/**
 * an elliptic structuring element, that is, a filled ellipse inscribed into the rectangle Rect(0, 0,
 * esize.width, 0.esize.height)
 *
 */
export declare const MORPH_ELLIPSE: MorphShapes;
export declare const MORPH_ERODE: MorphTypes;
export declare const MORPH_DILATE: MorphTypes;
/**
 * an opening operation `\\[\\texttt{dst} = \\mathrm{open} ( \\texttt{src} , \\texttt{element} )=
 * \\mathrm{dilate} ( \\mathrm{erode} ( \\texttt{src} , \\texttt{element} ))\\]`
 *
 */
export declare const MORPH_OPEN: MorphTypes;
/**
 * a closing operation `\\[\\texttt{dst} = \\mathrm{close} ( \\texttt{src} , \\texttt{element} )=
 * \\mathrm{erode} ( \\mathrm{dilate} ( \\texttt{src} , \\texttt{element} ))\\]`
 *
 */
export declare const MORPH_CLOSE: MorphTypes;
/**
 * a morphological gradient `\\[\\texttt{dst} = \\mathrm{morph\\_grad} ( \\texttt{src} ,
 * \\texttt{element} )= \\mathrm{dilate} ( \\texttt{src} , \\texttt{element} )- \\mathrm{erode} (
 * \\texttt{src} , \\texttt{element} )\\]`
 *
 */
export declare const MORPH_GRADIENT: MorphTypes;
/**
 * "top hat" `\\[\\texttt{dst} = \\mathrm{tophat} ( \\texttt{src} , \\texttt{element} )= \\texttt{src}
 * - \\mathrm{open} ( \\texttt{src} , \\texttt{element} )\\]`
 *
 */
export declare const MORPH_TOPHAT: MorphTypes;
/**
 * "black hat" `\\[\\texttt{dst} = \\mathrm{blackhat} ( \\texttt{src} , \\texttt{element} )=
 * \\mathrm{close} ( \\texttt{src} , \\texttt{element} )- \\texttt{src}\\]`
 *
 */
export declare const MORPH_BLACKHAT: MorphTypes;
/**
 * "hit or miss" .- Only supported for CV_8UC1 binary images. A tutorial can be found in the
 * documentation
 *
 */
export declare const MORPH_HITMISS: MorphTypes;
export declare const FILTER_SCHARR: SpecialFilter;
export type MorphShapes = any;
export type MorphTypes = any;
export type SpecialFilter = any;
/**
 * The function [cv::calcBackProject] calculates the back project of the histogram. That is, similarly
 * to [calcHist] , at each location (x, y) the function collects the values from the selected channels
 * in the input images and finds the corresponding histogram bin. But instead of incrementing it, the
 * function reads the bin value, scales it by scale , and stores in backProject(x,y) . In terms of
 * statistics, the function computes probability of each element value in respect with the empirical
 * probability distribution represented by the histogram. See how, for example, you can find and track
 * a bright-colored object in a scene:
 *
 * Before tracking, show the object to the camera so that it covers almost the whole frame. Calculate a
 * hue histogram. The histogram may have strong maximums, corresponding to the dominant colors in the
 * object.
 * When tracking, calculate a back projection of a hue plane of each input video frame using that
 * pre-computed histogram. Threshold the back projection to suppress weak colors. It may also make
 * sense to suppress pixels with non-sufficient color saturation and too dark or too bright pixels.
 * Find connected components in the resulting picture and choose, for example, the largest component.
 *
 * This is an approximate algorithm of the CamShift color object tracker.
 *
 * [calcHist], [compareHist]
 *
 * @param images Source arrays. They all should have the same depth, CV_8U, CV_16U or CV_32F , and the
 * same size. Each of them can have an arbitrary number of channels.
 *
 * @param nimages Number of source images.
 *
 * @param channels The list of channels used to compute the back projection. The number of channels
 * must match the histogram dimensionality. The first array channels are numerated from 0 to
 * images[0].channels()-1 , the second array channels are counted from images[0].channels() to
 * images[0].channels() + images[1].channels()-1, and so on.
 *
 * @param hist Input histogram that can be dense or sparse.
 *
 * @param backProject Destination back projection array that is a single-channel array of the same size
 * and depth as images[0] .
 *
 * @param ranges Array of arrays of the histogram bin boundaries in each dimension. See calcHist .
 *
 * @param scale Optional scale factor for the output back projection.
 *
 * @param uniform Flag indicating whether the histogram is uniform or not (see above).
 */
export declare function calcBackProject(images: any, nimages: int, channels: any, hist: Mat, backProject: Mat, ranges: any, scale?: double, uniform?: bool): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function calcBackProject(images: any, nimages: int, channels: any, hist: any, backProject: Mat, ranges: any, scale?: double, uniform?: bool): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function calcBackProject(images: MatVector, channels: any, hist: Mat, dst: Mat, ranges: any, scale: double): void;
/**
 * The function [cv::calcHist] calculates the histogram of one or more arrays. The elements of a tuple
 * used to increment a histogram bin are taken from the corresponding input arrays at the same
 * location. The sample below shows how to compute a 2D Hue-Saturation histogram for a color image. :
 *
 * ```cpp
 * #include <opencv2/imgproc.hpp>
 * #include <opencv2/highgui.hpp>
 *
 * using namespace cv;
 *
 * int main( int argc, char** argv )
 * {
 *     Mat src, hsv;
 *     if( argc != 2 || !(src=imread(argv[1], 1)).data )
 *         return -1;
 *
 *     cvtColor(src, hsv, COLOR_BGR2HSV);
 *
 *     // Quantize the hue to 30 levels
 *     // and the saturation to 32 levels
 *     int hbins = 30, sbins = 32;
 *     int histSize[] = {hbins, sbins};
 *     // hue varies from 0 to 179, see cvtColor
 *     float hranges[] = { 0, 180 };
 *     // saturation varies from 0 (black-gray-white) to
 *     // 255 (pure spectrum color)
 *     float sranges[] = { 0, 256 };
 *     const float* ranges[] = { hranges, sranges };
 *     MatND hist;
 *     // we compute the histogram from the 0-th and 1-st channels
 *     int channels[] = {0, 1};
 *
 *     calcHist( &hsv, 1, channels, Mat(), // do not use mask
 *              hist, 2, histSize, ranges,
 *              true, // the histogram is uniform
 *              false );
 *     double maxVal=0;
 *     minMaxLoc(hist, 0, &maxVal, 0, 0);
 *
 *     int scale = 10;
 *     Mat histImg = Mat::zeros(sbins*scale, hbins*10, CV_8UC3);
 *
 *     for( int h = 0; h < hbins; h++ )
 *         for( int s = 0; s < sbins; s++ )
 *         {
 *             float binVal = hist.at<float>(h, s);
 *             int intensity = cvRound(binVal*255/maxVal);
 *             rectangle( histImg, Point(h*scale, s*scale),
 *                         Point( (h+1)*scale - 1, (s+1)*scale - 1),
 *                         Scalar::all(intensity),
 *                         -1 );
 *         }
 *
 *     namedWindow( "Source", 1 );
 *     imshow( "Source", src );
 *
 *     namedWindow( "H-S Histogram", 1 );
 *     imshow( "H-S Histogram", histImg );
 *     waitKey();
 * }
 * ```
 *
 * @param images Source arrays. They all should have the same depth, CV_8U, CV_16U or CV_32F , and the
 * same size. Each of them can have an arbitrary number of channels.
 *
 * @param nimages Number of source images.
 *
 * @param channels List of the dims channels used to compute the histogram. The first array channels
 * are numerated from 0 to images[0].channels()-1 , the second array channels are counted from
 * images[0].channels() to images[0].channels() + images[1].channels()-1, and so on.
 *
 * @param mask Optional mask. If the matrix is not empty, it must be an 8-bit array of the same size as
 * images[i] . The non-zero mask elements mark the array elements counted in the histogram.
 *
 * @param hist Output histogram, which is a dense or sparse dims -dimensional array.
 *
 * @param dims Histogram dimensionality that must be positive and not greater than CV_MAX_DIMS (equal
 * to 32 in the current OpenCV version).
 *
 * @param histSize Array of histogram sizes in each dimension.
 *
 * @param ranges Array of the dims arrays of the histogram bin boundaries in each dimension. When the
 * histogram is uniform ( uniform =true), then for each dimension i it is enough to specify the lower
 * (inclusive) boundary $L_0$ of the 0-th histogram bin and the upper (exclusive) boundary
 * $U_{\texttt{histSize}[i]-1}$ for the last histogram bin histSize[i]-1 . That is, in case of a
 * uniform histogram each of ranges[i] is an array of 2 elements. When the histogram is not uniform (
 * uniform=false ), then each of ranges[i] contains histSize[i]+1 elements: $L_0, U_0=L_1, U_1=L_2,
 * ..., U_{\texttt{histSize[i]}-2}=L_{\texttt{histSize[i]}-1}, U_{\texttt{histSize[i]}-1}$ . The array
 * elements, that are not between $L_0$ and $U_{\texttt{histSize[i]}-1}$ , are not counted in the
 * histogram.
 *
 * @param uniform Flag indicating whether the histogram is uniform or not (see above).
 *
 * @param accumulate Accumulation flag. If it is set, the histogram is not cleared in the beginning
 * when it is allocated. This feature enables you to compute a single histogram from several sets of
 * arrays, or to update the histogram in time.
 */
export declare function calcHist(images: any, nimages: int, channels: any, mask: Mat, hist: Mat, dims: int, histSize: any, ranges: any, uniform?: bool, accumulate?: bool): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * this variant uses SparseMat for output
 */
export declare function calcHist(images: any, nimages: int, channels: any, mask: Mat, hist: any, dims: int, histSize: any, ranges: any, uniform?: bool, accumulate?: bool): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function calcHist(images: MatVector, channels: any, mask: Mat, hist: Mat, histSize: any, ranges: any, accumulate?: bool): void;
/**
 * The function [cv::compareHist] compares two dense or two sparse histograms using the specified
 * method.
 *
 * The function returns `$d(H_1, H_2)$` .
 *
 * While the function works well with 1-, 2-, 3-dimensional dense histograms, it may not be suitable
 * for high-dimensional sparse histograms. In such histograms, because of aliasing and sampling
 * problems, the coordinates of non-zero histogram bins can slightly shift. To compare such histograms
 * or more general sparse configurations of weighted points, consider using the [EMD] function.
 *
 * @param H1 First compared histogram.
 *
 * @param H2 Second compared histogram of the same size as H1 .
 *
 * @param method Comparison method, see HistCompMethods
 */
export declare function compareHist(H1: Mat, H2: Mat, method: int): double;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function compareHist(H1: any, H2: any, method: int): double;
/**
 * @param clipLimit Threshold for contrast limiting.
 *
 * @param tileGridSize Size of grid for histogram equalization. Input image will be divided into
 * equally sized rectangular tiles. tileGridSize defines the number of tiles in row and column.
 */
export declare function createCLAHE(clipLimit?: double, tileGridSize?: Size): any;
/**
 * The function computes the earth mover distance and/or a lower boundary of the distance between the
 * two weighted point configurations. One of the applications described in RubnerSept98, Rubner2000 is
 * multi-dimensional histogram comparison for image retrieval. EMD is a transportation problem that is
 * solved using some modification of a simplex algorithm, thus the complexity is exponential in the
 * worst case, though, on average it is much faster. In the case of a real metric the lower boundary
 * can be calculated even faster (using linear-time algorithm) and it can be used to determine roughly
 * whether the two signatures are far enough so that they cannot relate to the same object.
 *
 * @param signature1 First signature, a $\texttt{size1}\times \texttt{dims}+1$ floating-point matrix.
 * Each row stores the point weight followed by the point coordinates. The matrix is allowed to have a
 * single column (weights only) if the user-defined cost matrix is used. The weights must be
 * non-negative and have at least one non-zero value.
 *
 * @param signature2 Second signature of the same format as signature1 , though the number of rows may
 * be different. The total weights may be different. In this case an extra "dummy" point is added to
 * either signature1 or signature2. The weights must be non-negative and have at least one non-zero
 * value.
 *
 * @param distType Used metric. See DistanceTypes.
 *
 * @param cost User-defined $\texttt{size1}\times \texttt{size2}$ cost matrix. Also, if a cost matrix
 * is used, lower boundary lowerBound cannot be calculated because it needs a metric function.
 *
 * @param lowerBound Optional input/output parameter: lower boundary of a distance between the two
 * signatures that is a distance between mass centers. The lower boundary may not be calculated if the
 * user-defined cost matrix is used, the total weights of point configurations are not equal, or if the
 * signatures consist of weights only (the signature matrices have a single column). You must**
 * initialize *lowerBound . If the calculated distance between mass centers is greater or equal to
 * *lowerBound (it means that the signatures are far enough), the function does not calculate EMD. In
 * any case *lowerBound is set to the calculated distance between mass centers on return. Thus, if you
 * want to calculate both distance between mass centers and EMD, *lowerBound should be set to 0.
 *
 * @param flow Resultant $\texttt{size1} \times \texttt{size2}$ flow matrix: $\texttt{flow}_{i,j}$ is a
 * flow from $i$ -th point of signature1 to $j$ -th point of signature2 .
 */
export declare function EMD(signature1: Mat, signature2: Mat, distType: int, cost?: Mat, lowerBound?: any, flow?: Mat): float;
/**
 * The function equalizes the histogram of the input image using the following algorithm:
 *
 * Calculate the histogram `$H$` for src .
 * Normalize the histogram so that the sum of histogram bins is 255.
 * Compute the integral of the histogram: `\\[H'_i = \\sum _{0 \\le j < i} H(j)\\]`
 * Transform the image using `$H'$` as a look-up table: `$\\texttt{dst}(x,y) = H'(\\texttt{src}(x,y))$`
 *
 * The algorithm normalizes the brightness and increases the contrast of the image.
 *
 * @param src Source 8-bit single channel image.
 *
 * @param dst Destination image of the same size and type as src .
 */
export declare function equalizeHist(src: Mat, dst: Mat): void;
export declare function wrapperEMD(signature1: Mat, signature2: Mat, distType: int, cost?: Mat, lowerBound?: any, flow?: Mat): float;
/**
 * Correlation `\\[d(H_1,H_2) = \\frac{\\sum_I (H_1(I) - \\bar{H_1}) (H_2(I) -
 * \\bar{H_2})}{\\sqrt{\\sum_I(H_1(I) - \\bar{H_1})^2 \\sum_I(H_2(I) - \\bar{H_2})^2}}\\]` where
 * `\\[\\bar{H_k} = \\frac{1}{N} \\sum _J H_k(J)\\]` and `$N$` is a total number of histogram bins.
 *
 */
export declare const HISTCMP_CORREL: HistCompMethods;
/**
 * Chi-Square `\\[d(H_1,H_2) = \\sum _I \\frac{\\left(H_1(I)-H_2(I)\\right)^2}{H_1(I)}\\]`
 *
 */
export declare const HISTCMP_CHISQR: HistCompMethods;
/**
 * Intersection `\\[d(H_1,H_2) = \\sum _I \\min (H_1(I), H_2(I))\\]`
 *
 */
export declare const HISTCMP_INTERSECT: HistCompMethods;
/**
 * Bhattacharyya distance (In fact, OpenCV computes Hellinger distance, which is related to
 * Bhattacharyya coefficient.) `\\[d(H_1,H_2) = \\sqrt{1 - \\frac{1}{\\sqrt{\\bar{H_1} \\bar{H_2} N^2}}
 * \\sum_I \\sqrt{H_1(I) \\cdot H_2(I)}}\\]`
 *
 */
export declare const HISTCMP_BHATTACHARYYA: HistCompMethods;
export declare const HISTCMP_HELLINGER: HistCompMethods;
/**
 * Alternative Chi-Square `\\[d(H_1,H_2) = 2 * \\sum _I
 * \\frac{\\left(H_1(I)-H_2(I)\\right)^2}{H_1(I)+H_2(I)}\\]` This alternative formula is regularly used
 * for texture comparison. See e.g. Puzicha1997
 *
 */
export declare const HISTCMP_CHISQR_ALT: HistCompMethods;
/**
 * Kullback-Leibler divergence `\\[d(H_1,H_2) = \\sum _I H_1(I) \\log
 * \\left(\\frac{H_1(I)}{H_2(I)}\\right)\\]`
 *
 */
export declare const HISTCMP_KL_DIV: HistCompMethods;
/**
 * Histogram comparison methods
 *
 */
export type HistCompMethods = any;
/**
 * The function transforms a grayscale image to a binary image according to the formulae:
 *
 * **THRESH_BINARY** `\\[dst(x,y) = \\fork{\\texttt{maxValue}}{if \\(src(x,y) >
 * T(x,y)\\)}{0}{otherwise}\\]`
 * **THRESH_BINARY_INV** `\\[dst(x,y) = \\fork{0}{if \\(src(x,y) >
 * T(x,y)\\)}{\\texttt{maxValue}}{otherwise}\\]` where `$T(x,y)$` is a threshold calculated
 * individually for each pixel (see adaptiveMethod parameter).
 *
 * The function can process the image in-place.
 *
 * [threshold], [blur], [GaussianBlur]
 *
 * @param src Source 8-bit single-channel image.
 *
 * @param dst Destination image of the same size and the same type as src.
 *
 * @param maxValue Non-zero value assigned to the pixels for which the condition is satisfied
 *
 * @param adaptiveMethod Adaptive thresholding algorithm to use, see AdaptiveThresholdTypes. The
 * BORDER_REPLICATE | BORDER_ISOLATED is used to process boundaries.
 *
 * @param thresholdType Thresholding type that must be either THRESH_BINARY or THRESH_BINARY_INV, see
 * ThresholdTypes.
 *
 * @param blockSize Size of a pixel neighborhood that is used to calculate a threshold value for the
 * pixel: 3, 5, 7, and so on.
 *
 * @param C Constant subtracted from the mean or weighted mean (see the details below). Normally, it is
 * positive but may be zero or negative as well.
 */
export declare function adaptiveThreshold(src: Mat, dst: Mat, maxValue: double, adaptiveMethod: int, thresholdType: int, blockSize: int, C: double): void;
/**
 * Performs linear blending of two images: `\\[ \\texttt{dst}(i,j) =
 * \\texttt{weights1}(i,j)*\\texttt{src1}(i,j) + \\texttt{weights2}(i,j)*\\texttt{src2}(i,j) \\]`
 *
 * @param src1 It has a type of CV_8UC(n) or CV_32FC(n), where n is a positive integer.
 *
 * @param src2 It has the same type and size as src1.
 *
 * @param weights1 It has a type of CV_32FC1 and the same size with src1.
 *
 * @param weights2 It has a type of CV_32FC1 and the same size with src1.
 *
 * @param dst It is created if it does not have the same size and type with src1.
 */
export declare function blendLinear(src1: Mat, src2: Mat, weights1: Mat, weights2: Mat, dst: Mat): void;
/**
 * The function [cv::distanceTransform] calculates the approximate or precise distance from every
 * binary image pixel to the nearest zero pixel. For zero image pixels, the distance will obviously be
 * zero.
 *
 * When maskSize == [DIST_MASK_PRECISE] and distanceType == [DIST_L2] , the function runs the algorithm
 * described in Felzenszwalb04 . This algorithm is parallelized with the TBB library.
 *
 * In other cases, the algorithm Borgefors86 is used. This means that for a pixel the function finds
 * the shortest path to the nearest zero pixel consisting of basic shifts: horizontal, vertical,
 * diagonal, or knight's move (the latest is available for a `$5\\times 5$` mask). The overall distance
 * is calculated as a sum of these basic distances. Since the distance function should be symmetric,
 * all of the horizontal and vertical shifts must have the same cost (denoted as a ), all the diagonal
 * shifts must have the same cost (denoted as `b`), and all knight's moves must have the same cost
 * (denoted as `c`). For the [DIST_C] and [DIST_L1] types, the distance is calculated precisely,
 * whereas for [DIST_L2] (Euclidean distance) the distance can be calculated only with a relative error
 * (a `$5\\times 5$` mask gives more accurate results). For `a`,`b`, and `c`, OpenCV uses the values
 * suggested in the original paper:
 *
 * DIST_L1: `a = 1, b = 2`
 * DIST_L2:
 *
 * `3 x 3`: `a=0.955, b=1.3693`
 * `5 x 5`: `a=1, b=1.4, c=2.1969`
 *
 * DIST_C: `a = 1, b = 1`
 *
 * Typically, for a fast, coarse distance estimation [DIST_L2], a `$3\\times 3$` mask is used. For a
 * more accurate distance estimation [DIST_L2], a `$5\\times 5$` mask or the precise algorithm is used.
 * Note that both the precise and the approximate algorithms are linear on the number of pixels.
 *
 * This variant of the function does not only compute the minimum distance for each pixel `$(x, y)$`
 * but also identifies the nearest connected component consisting of zero pixels
 * (labelType==[DIST_LABEL_CCOMP]) or the nearest zero pixel (labelType==[DIST_LABEL_PIXEL]). Index of
 * the component/pixel is stored in `labels(x, y)`. When labelType==[DIST_LABEL_CCOMP], the function
 * automatically finds connected components of zero pixels in the input image and marks them with
 * distinct labels. When labelType==[DIST_LABEL_CCOMP], the function scans through the input image and
 * marks all the zero pixels with distinct labels.
 *
 * In this mode, the complexity is still linear. That is, the function provides a very fast way to
 * compute the Voronoi diagram for a binary image. Currently, the second variant can use only the
 * approximate distance transform algorithm, i.e. maskSize=[DIST_MASK_PRECISE] is not supported yet.
 *
 * @param src 8-bit, single-channel (binary) source image.
 *
 * @param dst Output image with calculated distances. It is a 8-bit or 32-bit floating-point,
 * single-channel image of the same size as src.
 *
 * @param labels Output 2D array of labels (the discrete Voronoi diagram). It has the type CV_32SC1 and
 * the same size as src.
 *
 * @param distanceType Type of distance, see DistanceTypes
 *
 * @param maskSize Size of the distance transform mask, see DistanceTransformMasks. DIST_MASK_PRECISE
 * is not supported by this variant. In case of the DIST_L1 or DIST_C distance type, the parameter is
 * forced to 3 because a $3\times 3$ mask gives the same result as $5\times 5$ or any larger aperture.
 *
 * @param labelType Type of the label array to build, see DistanceTransformLabelTypes.
 */
export declare function distanceTransform(src: Mat, dst: Mat, labels: Mat, distanceType: int, maskSize: int, labelType?: int): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param src 8-bit, single-channel (binary) source image.
 *
 * @param dst Output image with calculated distances. It is a 8-bit or 32-bit floating-point,
 * single-channel image of the same size as src .
 *
 * @param distanceType Type of distance, see DistanceTypes
 *
 * @param maskSize Size of the distance transform mask, see DistanceTransformMasks. In case of the
 * DIST_L1 or DIST_C distance type, the parameter is forced to 3 because a $3\times 3$ mask gives the
 * same result as $5\times 5$ or any larger aperture.
 *
 * @param dstType Type of output image. It can be CV_8U or CV_32F. Type CV_8U can be used only for the
 * first variant of the function and distanceType == DIST_L1.
 */
export declare function distanceTransform(src: Mat, dst: Mat, distanceType: int, maskSize: int, dstType?: int): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * variant without `mask` parameter
 */
export declare function floodFill(image: Mat, seedPoint: Point, newVal: Scalar, rect?: any, loDiff?: Scalar, upDiff?: Scalar, flags?: int): int;
/**
 * The function [cv::floodFill] fills a connected component starting from the seed point with the
 * specified color. The connectivity is determined by the color/brightness closeness of the neighbor
 * pixels. The pixel at `$(x,y)$` is considered to belong to the repainted domain if:
 *
 * in case of a grayscale image and floating range `\\[\\texttt{src} (x',y')- \\texttt{loDiff} \\leq
 * \\texttt{src} (x,y) \\leq \\texttt{src} (x',y')+ \\texttt{upDiff}\\]`
 * in case of a grayscale image and fixed range `\\[\\texttt{src} ( \\texttt{seedPoint} .x,
 * \\texttt{seedPoint} .y)- \\texttt{loDiff} \\leq \\texttt{src} (x,y) \\leq \\texttt{src} (
 * \\texttt{seedPoint} .x, \\texttt{seedPoint} .y)+ \\texttt{upDiff}\\]`
 * in case of a color image and floating range `\\[\\texttt{src} (x',y')_r- \\texttt{loDiff} _r \\leq
 * \\texttt{src} (x,y)_r \\leq \\texttt{src} (x',y')_r+ \\texttt{upDiff} _r,\\]` `\\[\\texttt{src}
 * (x',y')_g- \\texttt{loDiff} _g \\leq \\texttt{src} (x,y)_g \\leq \\texttt{src} (x',y')_g+
 * \\texttt{upDiff} _g\\]` and `\\[\\texttt{src} (x',y')_b- \\texttt{loDiff} _b \\leq \\texttt{src}
 * (x,y)_b \\leq \\texttt{src} (x',y')_b+ \\texttt{upDiff} _b\\]`
 * in case of a color image and fixed range `\\[\\texttt{src} ( \\texttt{seedPoint} .x,
 * \\texttt{seedPoint} .y)_r- \\texttt{loDiff} _r \\leq \\texttt{src} (x,y)_r \\leq \\texttt{src} (
 * \\texttt{seedPoint} .x, \\texttt{seedPoint} .y)_r+ \\texttt{upDiff} _r,\\]` `\\[\\texttt{src} (
 * \\texttt{seedPoint} .x, \\texttt{seedPoint} .y)_g- \\texttt{loDiff} _g \\leq \\texttt{src} (x,y)_g
 * \\leq \\texttt{src} ( \\texttt{seedPoint} .x, \\texttt{seedPoint} .y)_g+ \\texttt{upDiff} _g\\]` and
 * `\\[\\texttt{src} ( \\texttt{seedPoint} .x, \\texttt{seedPoint} .y)_b- \\texttt{loDiff} _b \\leq
 * \\texttt{src} (x,y)_b \\leq \\texttt{src} ( \\texttt{seedPoint} .x, \\texttt{seedPoint} .y)_b+
 * \\texttt{upDiff} _b\\]`
 *
 * where `$src(x',y')$` is the value of one of pixel neighbors that is already known to belong to the
 * component. That is, to be added to the connected component, a color/brightness of the pixel should
 * be close enough to:
 *
 * Color/brightness of one of its neighbors that already belong to the connected component in case of a
 * floating range.
 * Color/brightness of the seed point in case of a fixed range.
 *
 * Use these functions to either mark a connected component with the specified color in-place, or build
 * a mask and then extract the contour, or copy the region to another image, and so on.
 *
 * Since the mask is larger than the filled image, a pixel `$(x, y)$` in image corresponds to the pixel
 * `$(x+1, y+1)$` in the mask .
 *
 * [findContours]
 *
 * @param image Input/output 1- or 3-channel, 8-bit, or floating-point image. It is modified by the
 * function unless the FLOODFILL_MASK_ONLY flag is set in the second variant of the function. See the
 * details below.
 *
 * @param mask Operation mask that should be a single-channel 8-bit image, 2 pixels wider and 2 pixels
 * taller than image. Since this is both an input and output parameter, you must take responsibility of
 * initializing it. Flood-filling cannot go across non-zero pixels in the input mask. For example, an
 * edge detector output can be used as a mask to stop filling at edges. On output, pixels in the mask
 * corresponding to filled pixels in the image are set to 1 or to the a value specified in flags as
 * described below. Additionally, the function fills the border of the mask with ones to simplify
 * internal processing. It is therefore possible to use the same mask in multiple calls to the function
 * to make sure the filled areas do not overlap.
 *
 * @param seedPoint Starting point.
 *
 * @param newVal New value of the repainted domain pixels.
 *
 * @param rect Optional output parameter set by the function to the minimum bounding rectangle of the
 * repainted domain.
 *
 * @param loDiff Maximal lower brightness/color difference between the currently observed pixel and one
 * of its neighbors belonging to the component, or a seed pixel being added to the component.
 *
 * @param upDiff Maximal upper brightness/color difference between the currently observed pixel and one
 * of its neighbors belonging to the component, or a seed pixel being added to the component.
 *
 * @param flags Operation flags. The first 8 bits contain a connectivity value. The default value of 4
 * means that only the four nearest neighbor pixels (those that share an edge) are considered. A
 * connectivity value of 8 means that the eight nearest neighbor pixels (those that share a corner)
 * will be considered. The next 8 bits (8-16) contain a value between 1 and 255 with which to fill the
 * mask (the default value is 1). For example, 4 | ( 255 << 8 ) will consider 4 nearest neighbours and
 * fill the mask with a value of 255. The following additional options occupy higher bits and therefore
 * may be further combined with the connectivity and mask fill values using bit-wise or (|), see
 * FloodFillFlags.
 */
export declare function floodFill(image: Mat, mask: Mat, seedPoint: Point, newVal: Scalar, rect?: any, loDiff?: Scalar, upDiff?: Scalar, flags?: int): int;
/**
 * The function implements the .
 *
 * @param img Input 8-bit 3-channel image.
 *
 * @param mask Input/output 8-bit single-channel mask. The mask is initialized by the function when
 * mode is set to GC_INIT_WITH_RECT. Its elements may have one of the GrabCutClasses.
 *
 * @param rect ROI containing a segmented object. The pixels outside of the ROI are marked as "obvious
 * background". The parameter is only used when mode==GC_INIT_WITH_RECT .
 *
 * @param bgdModel Temporary array for the background model. Do not modify it while you are processing
 * the same image.
 *
 * @param fgdModel Temporary arrays for the foreground model. Do not modify it while you are processing
 * the same image.
 *
 * @param iterCount Number of iterations the algorithm should make before returning the result. Note
 * that the result can be refined with further calls with mode==GC_INIT_WITH_MASK or mode==GC_EVAL .
 *
 * @param mode Operation mode that could be one of the GrabCutModes
 */
export declare function grabCut(img: Mat, mask: Mat, rect: Rect, bgdModel: Mat, fgdModel: Mat, iterCount: int, mode?: int): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function integral(src: Mat, sum: Mat, sdepth?: int): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function integral(src: Mat, sum: Mat, sqsum: Mat, sdepth?: int, sqdepth?: int): void;
/**
 * The function calculates one or more integral images for the source image as follows:
 *
 * `\\[\\texttt{sum} (X,Y) = \\sum _{x<X,y<Y} \\texttt{image} (x,y)\\]`
 *
 * `\\[\\texttt{sqsum} (X,Y) = \\sum _{x<X,y<Y} \\texttt{image} (x,y)^2\\]`
 *
 * `\\[\\texttt{tilted} (X,Y) = \\sum _{y<Y,abs(x-X+1) \\leq Y-y-1} \\texttt{image} (x,y)\\]`
 *
 * Using these integral images, you can calculate sum, mean, and standard deviation over a specific
 * up-right or rotated rectangular region of the image in a constant time, for example:
 *
 * `\\[\\sum _{x_1 \\leq x < x_2, \\, y_1 \\leq y < y_2} \\texttt{image} (x,y) = \\texttt{sum}
 * (x_2,y_2)- \\texttt{sum} (x_1,y_2)- \\texttt{sum} (x_2,y_1)+ \\texttt{sum} (x_1,y_1)\\]`
 *
 * It makes possible to do a fast blurring or fast block correlation with a variable window size, for
 * example. In case of multi-channel images, sums for each channel are accumulated independently.
 *
 * As a practical example, the next figure shows the calculation of the integral of a straight
 * rectangle Rect(3,3,3,2) and of a tilted rectangle Rect(5,1,2,3) . The selected pixels in the
 * original image are shown, as well as the relative pixels in the integral images sum and tilted .
 *
 * @param src input image as $W \times H$, 8-bit or floating-point (32f or 64f).
 *
 * @param sum integral image as $(W+1)\times (H+1)$ , 32-bit integer or floating-point (32f or 64f).
 *
 * @param sqsum integral image for squared pixel values; it is $(W+1)\times (H+1)$, double-precision
 * floating-point (64f) array.
 *
 * @param tilted integral for the image rotated by 45 degrees; it is $(W+1)\times (H+1)$ array with the
 * same data type as sum.
 *
 * @param sdepth desired depth of the integral and the tilted integral images, CV_32S, CV_32F, or
 * CV_64F.
 *
 * @param sqdepth desired depth of the integral image of squared pixel values, CV_32F or CV_64F.
 */
export declare function integral(src: Mat, sum: Mat, sqsum: Mat, tilted: Mat, sdepth?: int, sqdepth?: int): void;
/**
 * The function applies fixed-level thresholding to a multiple-channel array. The function is typically
 * used to get a bi-level (binary) image out of a grayscale image ( [compare] could be also used for
 * this purpose) or for removing a noise, that is, filtering out pixels with too small or too large
 * values. There are several types of thresholding supported by the function. They are determined by
 * type parameter.
 *
 * Also, the special values [THRESH_OTSU] or [THRESH_TRIANGLE] may be combined with one of the above
 * values. In these cases, the function determines the optimal threshold value using the Otsu's or
 * Triangle algorithm and uses it instead of the specified thresh.
 *
 * Currently, the Otsu's and Triangle methods are implemented only for 8-bit single-channel images.
 *
 * the computed threshold value if Otsu's or Triangle methods used.
 *
 * [adaptiveThreshold], [findContours], [compare], [min], [max]
 *
 * @param src input array (multiple-channel, 8-bit or 32-bit floating point).
 *
 * @param dst output array of the same size and type and the same number of channels as src.
 *
 * @param thresh threshold value.
 *
 * @param maxval maximum value to use with the THRESH_BINARY and THRESH_BINARY_INV thresholding types.
 *
 * @param type thresholding type (see ThresholdTypes).
 */
export declare function threshold(src: Mat, dst: Mat, thresh: double, maxval: double, type: int): double;
/**
 * The function implements one of the variants of watershed, non-parametric marker-based segmentation
 * algorithm, described in Meyer92 .
 *
 * Before passing the image to the function, you have to roughly outline the desired regions in the
 * image markers with positive (>0) indices. So, every region is represented as one or more connected
 * components with the pixel values 1, 2, 3, and so on. Such markers can be retrieved from a binary
 * mask using [findContours] and [drawContours] (see the watershed.cpp demo). The markers are "seeds"
 * of the future image regions. All the other pixels in markers , whose relation to the outlined
 * regions is not known and should be defined by the algorithm, should be set to 0's. In the function
 * output, each pixel in markers is set to a value of the "seed" components or to -1 at boundaries
 * between the regions.
 *
 * Any two neighbor connected components are not necessarily separated by a watershed boundary (-1's
 * pixels); for example, they can touch each other in the initial marker image passed to the function.
 *
 * [findContours]
 *
 * @param image Input 8-bit 3-channel image.
 *
 * @param markers Input/output 32-bit single-channel image (map) of markers. It should have the same
 * size as image .
 */
export declare function watershed(image: Mat, markers: Mat): void;
/**
 * the threshold value `$T(x,y)$` is a mean of the `$\\texttt{blockSize} \\times \\texttt{blockSize}$`
 * neighborhood of `$(x, y)$` minus C
 *
 */
export declare const ADAPTIVE_THRESH_MEAN_C: AdaptiveThresholdTypes;
/**
 * the threshold value `$T(x, y)$` is a weighted sum (cross-correlation with a Gaussian window) of the
 * `$\\texttt{blockSize} \\times \\texttt{blockSize}$` neighborhood of `$(x, y)$` minus C . The default
 * sigma (standard deviation) is used for the specified blockSize . See [getGaussianKernel]
 *
 */
export declare const ADAPTIVE_THRESH_GAUSSIAN_C: AdaptiveThresholdTypes;
/**
 * each connected component of zeros in src (as well as all the non-zero pixels closest to the
 * connected component) will be assigned the same label
 *
 */
export declare const DIST_LABEL_CCOMP: DistanceTransformLabelTypes;
/**
 * each zero pixel (and all the non-zero pixels closest to it) gets its own label.
 *
 */
export declare const DIST_LABEL_PIXEL: DistanceTransformLabelTypes;
export declare const DIST_MASK_3: DistanceTransformMasks;
export declare const DIST_MASK_5: DistanceTransformMasks;
export declare const DIST_MASK_PRECISE: DistanceTransformMasks;
export declare const DIST_USER: DistanceTypes;
export declare const DIST_L1: DistanceTypes;
export declare const DIST_L2: DistanceTypes;
export declare const DIST_C: DistanceTypes;
export declare const DIST_L12: DistanceTypes;
export declare const DIST_FAIR: DistanceTypes;
export declare const DIST_WELSCH: DistanceTypes;
export declare const DIST_HUBER: DistanceTypes;
/**
 * If set, the difference between the current pixel and seed pixel is considered. Otherwise, the
 * difference between neighbor pixels is considered (that is, the range is floating).
 *
 */
export declare const FLOODFILL_FIXED_RANGE: FloodFillFlags;
/**
 * If set, the function does not change the image ( newVal is ignored), and only fills the mask with
 * the value specified in bits 8-16 of flags as described above. This option only make sense in
 * function variants that have the mask parameter.
 *
 */
export declare const FLOODFILL_MASK_ONLY: FloodFillFlags;
export declare const GC_BGD: GrabCutClasses;
export declare const GC_FGD: GrabCutClasses;
export declare const GC_PR_BGD: GrabCutClasses;
export declare const GC_PR_FGD: GrabCutClasses;
/**
 * The function initializes the state and the mask using the provided rectangle. After that it runs
 * iterCount iterations of the algorithm.
 *
 */
export declare const GC_INIT_WITH_RECT: GrabCutModes;
/**
 * The function initializes the state using the provided mask. Note that GC_INIT_WITH_RECT and
 * GC_INIT_WITH_MASK can be combined. Then, all the pixels outside of the ROI are automatically
 * initialized with GC_BGD .
 *
 */
export declare const GC_INIT_WITH_MASK: GrabCutModes;
/**
 * The value means that the algorithm should just resume.
 *
 */
export declare const GC_EVAL: GrabCutModes;
/**
 * The value means that the algorithm should just run the grabCut algorithm (a single iteration) with
 * the fixed model
 *
 */
export declare const GC_EVAL_FREEZE_MODEL: GrabCutModes;
export declare const THRESH_BINARY: ThresholdTypes;
export declare const THRESH_BINARY_INV: ThresholdTypes;
export declare const THRESH_TRUNC: ThresholdTypes;
export declare const THRESH_TOZERO: ThresholdTypes;
export declare const THRESH_TOZERO_INV: ThresholdTypes;
export declare const THRESH_MASK: ThresholdTypes;
export declare const THRESH_OTSU: ThresholdTypes;
export declare const THRESH_TRIANGLE: ThresholdTypes;
/**
 * adaptive threshold algorithm
 *
 * [adaptiveThreshold]
 *
 */
export type AdaptiveThresholdTypes = any;
/**
 * adaptive threshold algorithm
 *
 * [adaptiveThreshold]
 *
 */
export type DistanceTransformLabelTypes = any;
/**
 * adaptive threshold algorithm
 *
 * [adaptiveThreshold]
 *
 */
export type DistanceTransformMasks = any;
/**
 * adaptive threshold algorithm
 *
 * [adaptiveThreshold]
 *
 */
export type DistanceTypes = any;
/**
 * adaptive threshold algorithm
 *
 * [adaptiveThreshold]
 *
 */
export type FloodFillFlags = any;
/**
 * adaptive threshold algorithm
 *
 * [adaptiveThreshold]
 *
 */
export type GrabCutClasses = any;
/**
 * adaptive threshold algorithm
 *
 * [adaptiveThreshold]
 *
 */
export type GrabCutModes = any;
/**
 * adaptive threshold algorithm
 *
 * [adaptiveThreshold]
 *
 */
export type ThresholdTypes = any;
/**
 * The function slides through image , compares the overlapped patches of size `$w \\times h$` against
 * templ using the specified method and stores the comparison results in result . Here are the formulae
 * for the available comparison methods ( `$I$` denotes image, `$T$` template, `$R$` result ). The
 * summation is done over template and/or the image patch: `$x' = 0...w-1, y' = 0...h-1$`
 *
 * After the function finishes the comparison, the best matches can be found as global minimums (when
 * [TM_SQDIFF] was used) or maximums (when [TM_CCORR] or [TM_CCOEFF] was used) using the [minMaxLoc]
 * function. In case of a color image, template summation in the numerator and each sum in the
 * denominator is done over all of the channels and separate mean values are used for each channel.
 * That is, the function can take a color template and a color image. The result will still be a
 * single-channel image, which is easier to analyze.
 *
 * @param image Image where the search is running. It must be 8-bit or 32-bit floating-point.
 *
 * @param templ Searched template. It must be not greater than the source image and have the same data
 * type.
 *
 * @param result Map of comparison results. It must be single-channel 32-bit floating-point. If image
 * is $W \times H$ and templ is $w \times h$ , then result is $(W-w+1) \times (H-h+1)$ .
 *
 * @param method Parameter specifying the comparison method, see TemplateMatchModes
 *
 * @param mask Mask of searched template. It must have the same datatype and size with templ. It is not
 * set by default. Currently, only the TM_SQDIFF and TM_CCORR_NORMED methods are supported.
 */
export declare function matchTemplate(image: Mat, templ: Mat, result: Mat, method: int, mask?: Mat): void;
export declare const TM_SQDIFF: TemplateMatchModes;
export declare const TM_SQDIFF_NORMED: TemplateMatchModes;
export declare const TM_CCORR: TemplateMatchModes;
export declare const TM_CCORR_NORMED: TemplateMatchModes;
/**
 * `\\[R(x,y)= \\sum _{x',y'} (T'(x',y') \\cdot I'(x+x',y+y'))\\]` where `\\[\\begin{array}{l}
 * T'(x',y')=T(x',y') - 1/(w \\cdot h) \\cdot \\sum _{x'',y''} T(x'',y'') \\\\
 * I'(x+x',y+y')=I(x+x',y+y') - 1/(w \\cdot h) \\cdot \\sum _{x'',y''} I(x+x'',y+y'') \\end{array}\\]`
 *
 */
export declare const TM_CCOEFF: TemplateMatchModes;
export declare const TM_CCOEFF_NORMED: TemplateMatchModes;
export type TemplateMatchModes = any;
/**
 * The function [cv::approxPolyDP] approximates a curve or a polygon with another curve/polygon with
 * less vertices so that the distance between them is less or equal to the specified precision. It uses
 * the Douglas-Peucker algorithm
 *
 * @param curve Input vector of a 2D point stored in std::vector or Mat
 *
 * @param approxCurve Result of the approximation. The type should match the type of the input curve.
 *
 * @param epsilon Parameter specifying the approximation accuracy. This is the maximum distance between
 * the original curve and its approximation.
 *
 * @param closed If true, the approximated curve is closed (its first and last vertices are connected).
 * Otherwise, it is not closed.
 */
export declare function approxPolyDP(curve: Mat, approxCurve: Mat, epsilon: double, closed: bool): void;
/**
 * The function computes a curve length or a closed contour perimeter.
 *
 * @param curve Input vector of 2D points, stored in std::vector or Mat.
 *
 * @param closed Flag indicating whether the curve is closed or not.
 */
export declare function arcLength(curve: Mat, closed: bool): double;
/**
 * The function calculates and returns the minimal up-right bounding rectangle for the specified point
 * set or non-zero pixels of gray-scale image.
 *
 * @param array Input gray-scale image or 2D point set, stored in std::vector or Mat.
 */
export declare function boundingRect(array: Mat): Rect;
/**
 * The function finds the four vertices of a rotated rectangle. This function is useful to draw the
 * rectangle. In C++, instead of using this function, you can directly use [RotatedRect::points]
 * method. Please visit the [tutorial on Creating Bounding rotated boxes and ellipses for contours] for
 * more information.
 *
 * @param box The input rotated rectangle. It may be the output of
 *
 * @param points The output array of four vertices of rectangles.
 */
export declare function boxPoints(box: RotatedRect, points: Mat): void;
/**
 * image with 4 or 8 way connectivity - returns N, the total number of labels [0, N-1] where 0
 * represents the background label. ltype specifies the output label image type, an important
 * consideration based on the total number of labels or alternatively the total number of pixels in the
 * source image. ccltype specifies the connected components labeling algorithm to use, currently Grana
 * (BBDT) and Wu's (SAUF) algorithms are supported, see the [ConnectedComponentsAlgorithmsTypes] for
 * details. Note that SAUF algorithm forces a row major ordering of labels while BBDT does not. This
 * function uses parallel version of both Grana and Wu's algorithms if at least one allowed parallel
 * framework is enabled and if the rows of the image are at least twice the number returned by
 * [getNumberOfCPUs].
 *
 * @param image the 8-bit single-channel image to be labeled
 *
 * @param labels destination labeled image
 *
 * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively
 *
 * @param ltype output image label type. Currently CV_32S and CV_16U are supported.
 *
 * @param ccltype connected components algorithm type (see the ConnectedComponentsAlgorithmsTypes).
 */
export declare function connectedComponents(image: Mat, labels: Mat, connectivity: int, ltype: int, ccltype: int): int;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param image the 8-bit single-channel image to be labeled
 *
 * @param labels destination labeled image
 *
 * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively
 *
 * @param ltype output image label type. Currently CV_32S and CV_16U are supported.
 */
export declare function connectedComponents(image: Mat, labels: Mat, connectivity?: int, ltype?: int): int;
/**
 * image with 4 or 8 way connectivity - returns N, the total number of labels [0, N-1] where 0
 * represents the background label. ltype specifies the output label image type, an important
 * consideration based on the total number of labels or alternatively the total number of pixels in the
 * source image. ccltype specifies the connected components labeling algorithm to use, currently
 * Grana's (BBDT) and Wu's (SAUF) algorithms are supported, see the
 * [ConnectedComponentsAlgorithmsTypes] for details. Note that SAUF algorithm forces a row major
 * ordering of labels while BBDT does not. This function uses parallel version of both Grana and Wu's
 * algorithms (statistics included) if at least one allowed parallel framework is enabled and if the
 * rows of the image are at least twice the number returned by [getNumberOfCPUs].
 *
 * @param image the 8-bit single-channel image to be labeled
 *
 * @param labels destination labeled image
 *
 * @param stats statistics output for each label, including the background label, see below for
 * available statistics. Statistics are accessed via stats(label, COLUMN) where COLUMN is one of
 * ConnectedComponentsTypes. The data type is CV_32S.
 *
 * @param centroids centroid output for each label, including the background label. Centroids are
 * accessed via centroids(label, 0) for x and centroids(label, 1) for y. The data type CV_64F.
 *
 * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively
 *
 * @param ltype output image label type. Currently CV_32S and CV_16U are supported.
 *
 * @param ccltype connected components algorithm type (see ConnectedComponentsAlgorithmsTypes).
 */
export declare function connectedComponentsWithStats(image: Mat, labels: Mat, stats: Mat, centroids: Mat, connectivity: int, ltype: int, ccltype: int): int;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 *
 * @param image the 8-bit single-channel image to be labeled
 *
 * @param labels destination labeled image
 *
 * @param stats statistics output for each label, including the background label, see below for
 * available statistics. Statistics are accessed via stats(label, COLUMN) where COLUMN is one of
 * ConnectedComponentsTypes. The data type is CV_32S.
 *
 * @param centroids centroid output for each label, including the background label. Centroids are
 * accessed via centroids(label, 0) for x and centroids(label, 1) for y. The data type CV_64F.
 *
 * @param connectivity 8 or 4 for 8-way or 4-way connectivity respectively
 *
 * @param ltype output image label type. Currently CV_32S and CV_16U are supported.
 */
export declare function connectedComponentsWithStats(image: Mat, labels: Mat, stats: Mat, centroids: Mat, connectivity?: int, ltype?: int): int;
/**
 * The function computes a contour area. Similarly to moments , the area is computed using the Green
 * formula. Thus, the returned area and the number of non-zero pixels, if you draw the contour using
 * [drawContours] or [fillPoly] , can be different. Also, the function will most certainly give a wrong
 * results for contours with self-intersections.
 *
 * Example:
 *
 * ```cpp
 * vector<Point> contour;
 * contour.push_back(Point2f(0, 0));
 * contour.push_back(Point2f(10, 0));
 * contour.push_back(Point2f(10, 10));
 * contour.push_back(Point2f(5, 4));
 *
 * double area0 = contourArea(contour);
 * vector<Point> approx;
 * approxPolyDP(contour, approx, 5, true);
 * double area1 = contourArea(approx);
 *
 * cout << "area0 =" << area0 << endl <<
 *         "area1 =" << area1 << endl <<
 *         "approx poly vertices" << approx.size() << endl;
 * ```
 *
 * @param contour Input vector of 2D points (contour vertices), stored in std::vector or Mat.
 *
 * @param oriented Oriented area flag. If it is true, the function returns a signed area value,
 * depending on the contour orientation (clockwise or counter-clockwise). Using this feature you can
 * determine orientation of a contour by taking the sign of an area. By default, the parameter is
 * false, which means that the absolute value is returned.
 */
export declare function contourArea(contour: Mat, oriented?: bool): double;
/**
 * The function [cv::convexHull] finds the convex hull of a 2D point set using the Sklansky's algorithm
 * Sklansky82 that has *O(N logN)* complexity in the current implementation.
 *
 * `points` and `hull` should be different arrays, inplace processing isn't supported.
 * Check [the corresponding tutorial] for more details.
 *
 * useful links:
 *
 * @param points Input 2D point set, stored in std::vector or Mat.
 *
 * @param hull Output convex hull. It is either an integer vector of indices or vector of points. In
 * the first case, the hull elements are 0-based indices of the convex hull points in the original
 * array (since the set of convex hull points is a subset of the original point set). In the second
 * case, hull elements are the convex hull points themselves.
 *
 * @param clockwise Orientation flag. If it is true, the output convex hull is oriented clockwise.
 * Otherwise, it is oriented counter-clockwise. The assumed coordinate system has its X axis pointing
 * to the right, and its Y axis pointing upwards.
 *
 * @param returnPoints Operation flag. In case of a matrix, when the flag is true, the function returns
 * convex hull points. Otherwise, it returns indices of the convex hull points. When the output array
 * is std::vector, the flag is ignored, and the output depends on the type of the vector:
 * std::vector<int> implies returnPoints=false, std::vector<Point> implies returnPoints=true.
 */
export declare function convexHull(points: Mat, hull: Mat, clockwise?: bool, returnPoints?: bool): void;
/**
 * The figure below displays convexity defects of a hand contour:
 *
 * @param contour Input contour.
 *
 * @param convexhull Convex hull obtained using convexHull that should contain indices of the contour
 * points that make the hull.
 *
 * @param convexityDefects The output vector of convexity defects. In C++ and the new Python/Java
 * interface each convexity defect is represented as 4-element integer vector (a.k.a. Vec4i):
 * (start_index, end_index, farthest_pt_index, fixpt_depth), where indices are 0-based indices in the
 * original contour of the convexity defect beginning, end and the farthest point, and fixpt_depth is
 * fixed-point approximation (with 8 fractional bits) of the distance between the farthest contour
 * point and the hull. That is, to get the floating-point value of the depth will be fixpt_depth/256.0.
 */
export declare function convexityDefects(contour: Mat, convexhull: Mat, convexityDefects: Mat): void;
export declare function createGeneralizedHoughBallard(): any;
export declare function createGeneralizedHoughGuil(): any;
/**
 * The function retrieves contours from the binary image using the algorithm Suzuki85 . The contours
 * are a useful tool for shape analysis and object detection and recognition. See squares.cpp in the
 * OpenCV sample directory.
 *
 * Since opencv 3.2 source image is not modified by this function.
 *
 * @param image Source, an 8-bit single-channel image. Non-zero pixels are treated as 1's. Zero pixels
 * remain 0's, so the image is treated as binary . You can use compare, inRange, threshold ,
 * adaptiveThreshold, Canny, and others to create a binary image out of a grayscale or color one. If
 * mode equals to RETR_CCOMP or RETR_FLOODFILL, the input can also be a 32-bit integer image of labels
 * (CV_32SC1).
 *
 * @param contours Detected contours. Each contour is stored as a vector of points (e.g.
 * std::vector<std::vector<cv::Point> >).
 *
 * @param hierarchy Optional output vector (e.g. std::vector<cv::Vec4i>), containing information about
 * the image topology. It has as many elements as the number of contours. For each i-th contour
 * contours[i], the elements hierarchy[i][0] , hierarchy[i][1] , hierarchy[i][2] , and hierarchy[i][3]
 * are set to 0-based indices in contours of the next and previous contours at the same hierarchical
 * level, the first child contour and the parent contour, respectively. If for the contour i there are
 * no next, previous, parent, or nested contours, the corresponding elements of hierarchy[i] will be
 * negative.
 *
 * @param mode Contour retrieval mode, see RetrievalModes
 *
 * @param method Contour approximation method, see ContourApproximationModes
 *
 * @param offset Optional offset by which every contour point is shifted. This is useful if the
 * contours are extracted from the image ROI and then they should be analyzed in the whole image
 * context.
 */
export declare function findContours(image: Mat, contours: MatVector, hierarchy: Mat, mode: int, method: int, offset?: Point): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function findContours(image: Mat, contours: MatVector, mode: int, method: int, offset?: Point): void;
/**
 * The function calculates the ellipse that fits (in a least-squares sense) a set of 2D points best of
 * all. It returns the rotated rectangle in which the ellipse is inscribed. The first algorithm
 * described by Fitzgibbon95 is used. Developer should keep in mind that it is possible that the
 * returned ellipse/rotatedRect data contains negative indices, due to the data points being close to
 * the border of the containing [Mat] element.
 *
 * @param points Input 2D point set, stored in std::vector<> or Mat
 */
export declare function fitEllipse(points: Mat): RotatedRect;
/**
 * The function calculates the ellipse that fits a set of 2D points. It returns the rotated rectangle
 * in which the ellipse is inscribed. The Approximate Mean Square (AMS) proposed by Taubin1991 is used.
 *
 * For an ellipse, this basis set is `$ \\chi= \\left(x^2, x y, y^2, x, y, 1\\right) $`, which is a set
 * of six free coefficients `$
 * A^T=\\left\\{A_{\\text{xx}},A_{\\text{xy}},A_{\\text{yy}},A_x,A_y,A_0\\right\\} $`. However, to
 * specify an ellipse, all that is needed is five numbers; the major and minor axes lengths `$ (a,b)
 * $`, the position `$ (x_0,y_0) $`, and the orientation `$ \\theta $`. This is because the basis set
 * includes lines, quadratics, parabolic and hyperbolic functions as well as elliptical functions as
 * possible fits. If the fit is found to be a parabolic or hyperbolic function then the standard
 * [fitEllipse] method is used. The AMS method restricts the fit to parabolic, hyperbolic and
 * elliptical curves by imposing the condition that `$ A^T ( D_x^T D_x + D_y^T D_y) A = 1 $` where the
 * matrices `$ Dx $` and `$ Dy $` are the partial derivatives of the design matrix `$ D $` with respect
 * to x and y. The matrices are formed row by row applying the following to each of the points in the
 * set: `\\begin{align*} D(i,:)&=\\left\\{x_i^2, x_i y_i, y_i^2, x_i, y_i, 1\\right\\} &
 * D_x(i,:)&=\\left\\{2 x_i,y_i,0,1,0,0\\right\\} & D_y(i,:)&=\\left\\{0,x_i,2 y_i,0,1,0\\right\\}
 * \\end{align*}` The AMS method minimizes the cost function `\\begin{equation*} \\epsilon ^2=\\frac{
 * A^T D^T D A }{ A^T (D_x^T D_x + D_y^T D_y) A^T } \\end{equation*}`
 *
 * The minimum cost is found by solving the generalized eigenvalue problem.
 *
 * `\\begin{equation*} D^T D A = \\lambda \\left( D_x^T D_x + D_y^T D_y\\right) A \\end{equation*}`
 *
 * @param points Input 2D point set, stored in std::vector<> or Mat
 */
export declare function fitEllipseAMS(points: Mat): RotatedRect;
/**
 * The function calculates the ellipse that fits a set of 2D points. It returns the rotated rectangle
 * in which the ellipse is inscribed. The Direct least square (Direct) method by Fitzgibbon1999 is
 * used.
 *
 * For an ellipse, this basis set is `$ \\chi= \\left(x^2, x y, y^2, x, y, 1\\right) $`, which is a set
 * of six free coefficients `$
 * A^T=\\left\\{A_{\\text{xx}},A_{\\text{xy}},A_{\\text{yy}},A_x,A_y,A_0\\right\\} $`. However, to
 * specify an ellipse, all that is needed is five numbers; the major and minor axes lengths `$ (a,b)
 * $`, the position `$ (x_0,y_0) $`, and the orientation `$ \\theta $`. This is because the basis set
 * includes lines, quadratics, parabolic and hyperbolic functions as well as elliptical functions as
 * possible fits. The Direct method confines the fit to ellipses by ensuring that `$ 4 A_{xx} A_{yy}-
 * A_{xy}^2 > 0 $`. The condition imposed is that `$ 4 A_{xx} A_{yy}- A_{xy}^2=1 $` which satisfies the
 * inequality and as the coefficients can be arbitrarily scaled is not overly restrictive.
 *
 * `\\begin{equation*} \\epsilon ^2= A^T D^T D A \\quad \\text{with} \\quad A^T C A =1 \\quad
 * \\text{and} \\quad C=\\left(\\begin{matrix} 0 & 0 & 2 & 0 & 0 & 0 \\\\ 0 & -1 & 0 & 0 & 0 & 0 \\\\ 2
 * & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 & 0 & 0
 * \\end{matrix} \\right) \\end{equation*}`
 *
 * The minimum cost is found by solving the generalized eigenvalue problem.
 *
 * `\\begin{equation*} D^T D A = \\lambda \\left( C\\right) A \\end{equation*}`
 *
 * The system produces only one positive eigenvalue `$ \\lambda$` which is chosen as the solution with
 * its eigenvector `$\\mathbf{u}$`. These are used to find the coefficients
 *
 * `\\begin{equation*} A = \\sqrt{\\frac{1}{\\mathbf{u}^T C \\mathbf{u}}} \\mathbf{u} \\end{equation*}`
 * The scaling factor guarantees that `$A^T C A =1$`.
 *
 * @param points Input 2D point set, stored in std::vector<> or Mat
 */
export declare function fitEllipseDirect(points: Mat): RotatedRect;
/**
 * The function fitLine fits a line to a 2D or 3D point set by minimizing `$\\sum_i \\rho(r_i)$` where
 * `$r_i$` is a distance between the `$i^{th}$` point, the line and `$\\rho(r)$` is a distance
 * function, one of the following:
 *
 * DIST_L2 `\\[\\rho (r) = r^2/2 \\quad \\text{(the simplest and the fastest least-squares method)}\\]`
 * DIST_L1 `\\[\\rho (r) = r\\]`
 * DIST_L12 `\\[\\rho (r) = 2 \\cdot ( \\sqrt{1 + \\frac{r^2}{2}} - 1)\\]`
 * DIST_FAIR `\\[\\rho \\left (r \\right ) = C^2 \\cdot \\left ( \\frac{r}{C} - \\log{\\left(1 +
 * \\frac{r}{C}\\right)} \\right ) \\quad \\text{where} \\quad C=1.3998\\]`
 * DIST_WELSCH `\\[\\rho \\left (r \\right ) = \\frac{C^2}{2} \\cdot \\left ( 1 -
 * \\exp{\\left(-\\left(\\frac{r}{C}\\right)^2\\right)} \\right ) \\quad \\text{where} \\quad
 * C=2.9846\\]`
 * DIST_HUBER `\\[\\rho (r) = \\fork{r^2/2}{if \\(r < C\\)}{C \\cdot (r-C/2)}{otherwise} \\quad
 * \\text{where} \\quad C=1.345\\]`
 *
 * The algorithm is based on the M-estimator (  ) technique that iteratively fits the line using the
 * weighted least-squares algorithm. After each iteration the weights `$w_i$` are adjusted to be
 * inversely proportional to `$\\rho(r_i)$` .
 *
 * @param points Input vector of 2D or 3D points, stored in std::vector<> or Mat.
 *
 * @param line Output line parameters. In case of 2D fitting, it should be a vector of 4 elements (like
 * Vec4f) - (vx, vy, x0, y0), where (vx, vy) is a normalized vector collinear to the line and (x0, y0)
 * is a point on the line. In case of 3D fitting, it should be a vector of 6 elements (like Vec6f) -
 * (vx, vy, vz, x0, y0, z0), where (vx, vy, vz) is a normalized vector collinear to the line and (x0,
 * y0, z0) is a point on the line.
 *
 * @param distType Distance used by the M-estimator, see DistanceTypes
 *
 * @param param Numerical parameter ( C ) for some types of distances. If it is 0, an optimal value is
 * chosen.
 *
 * @param reps Sufficient accuracy for the radius (distance between the coordinate origin and the
 * line).
 *
 * @param aeps Sufficient accuracy for the angle. 0.01 would be a good default value for reps and aeps.
 */
export declare function fitLine(points: Mat, line: Mat, distType: int, param: double, reps: double, aeps: double): void;
/**
 * The function calculates seven Hu invariants (introduced in Hu62; see also ) defined as:
 *
 * `\\[\\begin{array}{l} hu[0]= \\eta _{20}+ \\eta _{02} \\\\ hu[1]=( \\eta _{20}- \\eta _{02})^{2}+4
 * \\eta _{11}^{2} \\\\ hu[2]=( \\eta _{30}-3 \\eta _{12})^{2}+ (3 \\eta _{21}- \\eta _{03})^{2} \\\\
 * hu[3]=( \\eta _{30}+ \\eta _{12})^{2}+ ( \\eta _{21}+ \\eta _{03})^{2} \\\\ hu[4]=( \\eta _{30}-3
 * \\eta _{12})( \\eta _{30}+ \\eta _{12})[( \\eta _{30}+ \\eta _{12})^{2}-3( \\eta _{21}+ \\eta
 * _{03})^{2}]+(3 \\eta _{21}- \\eta _{03})( \\eta _{21}+ \\eta _{03})[3( \\eta _{30}+ \\eta
 * _{12})^{2}-( \\eta _{21}+ \\eta _{03})^{2}] \\\\ hu[5]=( \\eta _{20}- \\eta _{02})[( \\eta _{30}+
 * \\eta _{12})^{2}- ( \\eta _{21}+ \\eta _{03})^{2}]+4 \\eta _{11}( \\eta _{30}+ \\eta _{12})( \\eta
 * _{21}+ \\eta _{03}) \\\\ hu[6]=(3 \\eta _{21}- \\eta _{03})( \\eta _{21}+ \\eta _{03})[3( \\eta
 * _{30}+ \\eta _{12})^{2}-( \\eta _{21}+ \\eta _{03})^{2}]-( \\eta _{30}-3 \\eta _{12})( \\eta _{21}+
 * \\eta _{03})[3( \\eta _{30}+ \\eta _{12})^{2}-( \\eta _{21}+ \\eta _{03})^{2}] \\\\ \\end{array}\\]`
 *
 * where `$\\eta_{ji}$` stands for `$\\texttt{Moments::nu}_{ji}$` .
 *
 * These values are proved to be invariants to the image scale, rotation, and reflection except the
 * seventh one, whose sign is changed by reflection. This invariance is proved with the assumption of
 * infinite image resolution. In case of raster images, the computed Hu invariants for the original and
 * transformed images are a bit different.
 *
 * [matchShapes]
 *
 * @param moments Input moments computed with moments .
 *
 * @param hu Output Hu invariants.
 */
export declare function HuMoments(moments: any, hu: double): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function HuMoments(m: any, hu: Mat): void;
export declare function intersectConvexConvex(_p1: Mat, _p2: Mat, _p12: Mat, handleNested?: bool): float;
/**
 * The function tests whether the input contour is convex or not. The contour must be simple, that is,
 * without self-intersections. Otherwise, the function output is undefined.
 *
 * @param contour Input vector of 2D points, stored in std::vector<> or Mat
 */
export declare function isContourConvex(contour: Mat): bool;
/**
 * The function compares two shapes. All three implemented methods use the Hu invariants (see
 * [HuMoments])
 *
 * @param contour1 First contour or grayscale image.
 *
 * @param contour2 Second contour or grayscale image.
 *
 * @param method Comparison method, see ShapeMatchModes
 *
 * @param parameter Method-specific parameter (not supported now).
 */
export declare function matchShapes(contour1: Mat, contour2: Mat, method: int, parameter: double): double;
/**
 * The function calculates and returns the minimum-area bounding rectangle (possibly rotated) for a
 * specified point set. Developer should keep in mind that the returned [RotatedRect] can contain
 * negative indices when data is close to the containing [Mat] element boundary.
 *
 * @param points Input vector of 2D points, stored in std::vector<> or Mat
 */
export declare function minAreaRect(points: Mat): RotatedRect;
/**
 * The function finds the minimal enclosing circle of a 2D point set using an iterative algorithm.
 *
 * @param points Input vector of 2D points, stored in std::vector<> or Mat
 */
export declare function minEnclosingCircle(points: Mat): Circle;
/**
 * The function finds a triangle of minimum area enclosing the given set of 2D points and returns its
 * area. The output for a given 2D point set is shown in the image below. 2D points are depicted in
 * red* and the enclosing triangle in *yellow*.
 *
 *  The implementation of the algorithm is based on O'Rourke's ORourke86 and Klee and Laskowski's
 * KleeLaskowski85 papers. O'Rourke provides a `$\\theta(n)$` algorithm for finding the minimal
 * enclosing triangle of a 2D convex polygon with n vertices. Since the [minEnclosingTriangle] function
 * takes a 2D point set as input an additional preprocessing step of computing the convex hull of the
 * 2D point set is required. The complexity of the [convexHull] function is `$O(n log(n))$` which is
 * higher than `$\\theta(n)$`. Thus the overall complexity of the function is `$O(n log(n))$`.
 *
 * @param points Input vector of 2D points with depth CV_32S or CV_32F, stored in std::vector<> or Mat
 *
 * @param triangle Output vector of three 2D points defining the vertices of the triangle. The depth of
 * the OutputArray must be CV_32F.
 */
export declare function minEnclosingTriangle(points: Mat, triangle: Mat): double;
/**
 * The function computes moments, up to the 3rd order, of a vector shape or a rasterized shape. The
 * results are returned in the structure [cv::Moments].
 *
 * moments.
 *
 * Only applicable to contour moments calculations from Python bindings: Note that the numpy type for
 * the input array should be either np.int32 or np.float32.
 *
 * [contourArea], [arcLength]
 *
 * @param array Raster image (single-channel, 8-bit or floating-point 2D array) or an array ( $1 \times
 * N$ or $N \times 1$ ) of 2D points (Point or Point2f ).
 *
 * @param binaryImage If it is true, all non-zero image pixels are treated as 1's. The parameter is
 * used for images only.
 */
export declare function moments(array: Mat, binaryImage?: bool): Moments;
/**
 * The function determines whether the point is inside a contour, outside, or lies on an edge (or
 * coincides with a vertex). It returns positive (inside), negative (outside), or zero (on an edge)
 * value, correspondingly. When measureDist=false , the return value is +1, -1, and 0, respectively.
 * Otherwise, the return value is a signed distance between the point and the nearest contour edge.
 *
 * See below a sample output of the function where each image pixel is tested against the contour:
 *
 * @param contour Input contour.
 *
 * @param pt Point tested against the contour.
 *
 * @param measureDist If true, the function estimates the signed distance from the point to the nearest
 * contour edge. Otherwise, the function only checks if the point is inside a contour or not.
 */
export declare function pointPolygonTest(contour: Mat, pt: Point, measureDist: bool): double;
/**
 * If there is then the vertices of the intersecting region are returned as well.
 *
 * Below are some examples of intersection configurations. The hatched pattern indicates the
 * intersecting region and the red vertices are returned by the function.
 *
 * One of [RectanglesIntersectTypes]
 *
 * @param rect1 First rectangle
 *
 * @param rect2 Second rectangle
 *
 * @param intersectingRegion The output array of the vertices of the intersecting region. It returns at
 * most 8 vertices. Stored as std::vector<cv::Point2f> or cv::Mat as Mx1 of type CV_32FC2.
 */
export declare function rotatedRectangleIntersection(rect1: any, rect2: any, intersectingRegion: Mat): int;
export declare const CCL_WU: ConnectedComponentsAlgorithmsTypes;
export declare const CCL_DEFAULT: ConnectedComponentsAlgorithmsTypes;
export declare const CCL_GRANA: ConnectedComponentsAlgorithmsTypes;
/**
 * The leftmost (x) coordinate which is the inclusive start of the bounding box in the horizontal
 * direction.
 *
 */
export declare const CC_STAT_LEFT: ConnectedComponentsTypes;
/**
 * The topmost (y) coordinate which is the inclusive start of the bounding box in the vertical
 * direction.
 *
 */
export declare const CC_STAT_TOP: ConnectedComponentsTypes;
export declare const CC_STAT_WIDTH: ConnectedComponentsTypes;
export declare const CC_STAT_HEIGHT: ConnectedComponentsTypes;
export declare const CC_STAT_AREA: ConnectedComponentsTypes;
export declare const CC_STAT_MAX: ConnectedComponentsTypes;
/**
 * stores absolutely all the contour points. That is, any 2 subsequent points (x1,y1) and (x2,y2) of
 * the contour will be either horizontal, vertical or diagonal neighbors, that is,
 * max(abs(x1-x2),abs(y2-y1))==1.
 *
 */
export declare const CHAIN_APPROX_NONE: ContourApproximationModes;
/**
 * compresses horizontal, vertical, and diagonal segments and leaves only their end points. For
 * example, an up-right rectangular contour is encoded with 4 points.
 *
 */
export declare const CHAIN_APPROX_SIMPLE: ContourApproximationModes;
/**
 * applies one of the flavors of the Teh-Chin chain approximation algorithm TehChin89
 *
 */
export declare const CHAIN_APPROX_TC89_L1: ContourApproximationModes;
/**
 * applies one of the flavors of the Teh-Chin chain approximation algorithm TehChin89
 *
 */
export declare const CHAIN_APPROX_TC89_KCOS: ContourApproximationModes;
export declare const INTERSECT_NONE: RectanglesIntersectTypes;
export declare const INTERSECT_PARTIAL: RectanglesIntersectTypes;
export declare const INTERSECT_FULL: RectanglesIntersectTypes;
/**
 * retrieves only the extreme outer contours. It sets `hierarchy[i][2]=hierarchy[i][3]=-1` for all the
 * contours.
 *
 */
export declare const RETR_EXTERNAL: RetrievalModes;
/**
 * retrieves all of the contours without establishing any hierarchical relationships.
 *
 */
export declare const RETR_LIST: RetrievalModes;
/**
 * retrieves all of the contours and organizes them into a two-level hierarchy. At the top level, there
 * are external boundaries of the components. At the second level, there are boundaries of the holes.
 * If there is another contour inside a hole of a connected component, it is still put at the top
 * level.
 *
 */
export declare const RETR_CCOMP: RetrievalModes;
/**
 * retrieves all of the contours and reconstructs a full hierarchy of nested contours.
 *
 */
export declare const RETR_TREE: RetrievalModes;
export declare const RETR_FLOODFILL: RetrievalModes;
export declare const CONTOURS_MATCH_I1: ShapeMatchModes;
export declare const CONTOURS_MATCH_I2: ShapeMatchModes;
export declare const CONTOURS_MATCH_I3: ShapeMatchModes;
export type ConnectedComponentsAlgorithmsTypes = any;
export type ConnectedComponentsTypes = any;
export type ContourApproximationModes = any;
export type RectanglesIntersectTypes = any;
export type RetrievalModes = any;
export type ShapeMatchModes = any;
/**
 * The function converts a pair of maps for remap from one representation to another. The following
 * options ( (map1.type(), map2.type()) `$\\rightarrow$` (dstmap1.type(), dstmap2.type()) ) are
 * supported:
 *
 * `$\\texttt{(CV_32FC1, CV_32FC1)} \\rightarrow \\texttt{(CV_16SC2, CV_16UC1)}$`. This is the most
 * frequently used conversion operation, in which the original floating-point maps (see remap ) are
 * converted to a more compact and much faster fixed-point representation. The first output array
 * contains the rounded coordinates and the second array (created only when nninterpolation=false )
 * contains indices in the interpolation tables.
 * `$\\texttt{(CV_32FC2)} \\rightarrow \\texttt{(CV_16SC2, CV_16UC1)}$`. The same as above but the
 * original maps are stored in one 2-channel matrix.
 * Reverse conversion. Obviously, the reconstructed floating-point maps will not be exactly the same as
 * the originals.
 *
 * [remap], [undistort], [initUndistortRectifyMap]
 *
 * @param map1 The first input map of type CV_16SC2, CV_32FC1, or CV_32FC2 .
 *
 * @param map2 The second input map of type CV_16UC1, CV_32FC1, or none (empty matrix), respectively.
 *
 * @param dstmap1 The first output map that has the type dstmap1type and the same size as src .
 *
 * @param dstmap2 The second output map.
 *
 * @param dstmap1type Type of the first output map that should be CV_16SC2, CV_32FC1, or CV_32FC2 .
 *
 * @param nninterpolation Flag indicating whether the fixed-point maps are used for the
 * nearest-neighbor or for a more complex interpolation.
 */
export declare function convertMaps(map1: Mat, map2: Mat, dstmap1: Mat, dstmap2: Mat, dstmap1type: int, nninterpolation?: bool): void;
/**
 * The function calculates the `$2 \\times 3$` matrix of an affine transform so that:
 *
 * `\\[\\begin{bmatrix} x'_i \\\\ y'_i \\end{bmatrix} = \\texttt{map_matrix} \\cdot \\begin{bmatrix}
 * x_i \\\\ y_i \\\\ 1 \\end{bmatrix}\\]`
 *
 * where
 *
 * `\\[dst(i)=(x'_i,y'_i), src(i)=(x_i, y_i), i=0,1,2\\]`
 *
 * [warpAffine], [transform]
 *
 * @param src Coordinates of triangle vertices in the source image.
 *
 * @param dst Coordinates of the corresponding triangle vertices in the destination image.
 */
export declare function getAffineTransform(src: any, dst: any): Mat;
export declare function getAffineTransform(src: Mat, dst: Mat): Mat;
/**
 * The function calculates the `$3 \\times 3$` matrix of a perspective transform so that:
 *
 * `\\[\\begin{bmatrix} t_i x'_i \\\\ t_i y'_i \\\\ t_i \\end{bmatrix} = \\texttt{map_matrix} \\cdot
 * \\begin{bmatrix} x_i \\\\ y_i \\\\ 1 \\end{bmatrix}\\]`
 *
 * where
 *
 * `\\[dst(i)=(x'_i,y'_i), src(i)=(x_i, y_i), i=0,1,2,3\\]`
 *
 * [findHomography], [warpPerspective], [perspectiveTransform]
 *
 * @param src Coordinates of quadrangle vertices in the source image.
 *
 * @param dst Coordinates of the corresponding quadrangle vertices in the destination image.
 *
 * @param solveMethod method passed to cv::solve (DecompTypes)
 */
export declare function getPerspectiveTransform(src: Mat, dst: Mat, solveMethod?: int): Mat;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function getPerspectiveTransform(src: any, dst: any, solveMethod?: int): Mat;
/**
 * The function getRectSubPix extracts pixels from src:
 *
 * `\\[patch(x, y) = src(x + \\texttt{center.x} - ( \\texttt{dst.cols} -1)*0.5, y + \\texttt{center.y}
 * - ( \\texttt{dst.rows} -1)*0.5)\\]`
 *
 * where the values of the pixels at non-integer coordinates are retrieved using bilinear
 * interpolation. Every channel of multi-channel images is processed independently. Also the image
 * should be a single channel or three channel image. While the center of the rectangle must be inside
 * the image, parts of the rectangle may be outside.
 *
 * [warpAffine], [warpPerspective]
 *
 * @param image Source image.
 *
 * @param patchSize Size of the extracted patch.
 *
 * @param center Floating point coordinates of the center of the extracted rectangle within the source
 * image. The center must be inside the image.
 *
 * @param patch Extracted patch that has the size patchSize and the same number of channels as src .
 *
 * @param patchType Depth of the extracted pixels. By default, they have the same depth as src .
 */
export declare function getRectSubPix(image: Mat, patchSize: Size, center: Point, patch: Mat, patchType?: int): void;
/**
 * The function calculates the following matrix:
 *
 * `\\[\\begin{bmatrix} \\alpha & \\beta & (1- \\alpha ) \\cdot \\texttt{center.x} - \\beta \\cdot
 * \\texttt{center.y} \\\\ - \\beta & \\alpha & \\beta \\cdot \\texttt{center.x} + (1- \\alpha ) \\cdot
 * \\texttt{center.y} \\end{bmatrix}\\]`
 *
 * where
 *
 * `\\[\\begin{array}{l} \\alpha = \\texttt{scale} \\cdot \\cos \\texttt{angle} , \\\\ \\beta =
 * \\texttt{scale} \\cdot \\sin \\texttt{angle} \\end{array}\\]`
 *
 * The transformation maps the rotation center to itself. If this is not the target, adjust the shift.
 *
 * [getAffineTransform], [warpAffine], [transform]
 *
 * @param center Center of the rotation in the source image.
 *
 * @param angle Rotation angle in degrees. Positive values mean counter-clockwise rotation (the
 * coordinate origin is assumed to be the top-left corner).
 *
 * @param scale Isotropic scale factor.
 */
export declare function getRotationMatrix2D(center: Point, angle: double, scale: double): Mat;
/**
 * The function computes an inverse affine transformation represented by `$2 \\times 3$` matrix M:
 *
 * `\\[\\begin{bmatrix} a_{11} & a_{12} & b_1 \\\\ a_{21} & a_{22} & b_2 \\end{bmatrix}\\]`
 *
 * The result is also a `$2 \\times 3$` matrix of the same type as M.
 *
 * @param M Original affine transformation.
 *
 * @param iM Output reverse affine transformation.
 */
export declare function invertAffineTransform(M: Mat, iM: Mat): void;
export declare function linearPolar(src: Mat, dst: Mat, center: Point, maxRadius: double, flags: int): void;
export declare function logPolar(src: Mat, dst: Mat, center: Point, M: double, flags: int): void;
/**
 * The function remap transforms the source image using the specified map:
 *
 * `\\[\\texttt{dst} (x,y) = \\texttt{src} (map_x(x,y),map_y(x,y))\\]`
 *
 * where values of pixels with non-integer coordinates are computed using one of available
 * interpolation methods. `$map_x$` and `$map_y$` can be encoded as separate floating-point maps in
 * `$map_1$` and `$map_2$` respectively, or interleaved floating-point maps of `$(x,y)$` in `$map_1$`,
 * or fixed-point maps created by using convertMaps. The reason you might want to convert from floating
 * to fixed-point representations of a map is that they can yield much faster (2x) remapping
 * operations. In the converted case, `$map_1$` contains pairs (cvFloor(x), cvFloor(y)) and `$map_2$`
 * contains indices in a table of interpolation coefficients.
 *
 * This function cannot operate in-place.
 *
 * Due to current implementation limitations the size of an input and output images should be less than
 * 32767x32767.
 *
 * @param src Source image.
 *
 * @param dst Destination image. It has the same size as map1 and the same type as src .
 *
 * @param map1 The first map of either (x,y) points or just x values having the type CV_16SC2 ,
 * CV_32FC1, or CV_32FC2. See convertMaps for details on converting a floating point representation to
 * fixed-point for speed.
 *
 * @param map2 The second map of y values having the type CV_16UC1, CV_32FC1, or none (empty map if
 * map1 is (x,y) points), respectively.
 *
 * @param interpolation Interpolation method (see InterpolationFlags). The method INTER_AREA is not
 * supported by this function.
 *
 * @param borderMode Pixel extrapolation method (see BorderTypes). When borderMode=BORDER_TRANSPARENT,
 * it means that the pixels in the destination image that corresponds to the "outliers" in the source
 * image are not modified by the function.
 *
 * @param borderValue Value used in case of a constant border. By default, it is 0.
 */
export declare function remap(src: Mat, dst: Mat, map1: Mat, map2: Mat, interpolation: int, borderMode?: int, borderValue?: any): void;
/**
 * The function resize resizes the image src down to or up to the specified size. Note that the initial
 * dst type or size are not taken into account. Instead, the size and type are derived from the
 * `src`,`dsize`,`fx`, and `fy`. If you want to resize src so that it fits the pre-created dst, you may
 * call the function as follows:
 *
 * ```cpp
 * // explicitly specify dsize=dst.size(); fx and fy will be computed from that.
 * resize(src, dst, dst.size(), 0, 0, interpolation);
 * ```
 *
 *  If you want to decimate the image by factor of 2 in each direction, you can call the function this
 * way:
 *
 * ```cpp
 * // specify fx and fy and let the function compute the destination image size.
 * resize(src, dst, Size(), 0.5, 0.5, interpolation);
 * ```
 *
 *  To shrink an image, it will generally look best with [INTER_AREA] interpolation, whereas to enlarge
 * an image, it will generally look best with c::INTER_CUBIC (slow) or [INTER_LINEAR] (faster but still
 * looks OK).
 *
 * [warpAffine], [warpPerspective], [remap]
 *
 * @param src input image.
 *
 * @param dst output image; it has the size dsize (when it is non-zero) or the size computed from
 * src.size(), fx, and fy; the type of dst is the same as of src.
 *
 * @param dsize output image size; if it equals zero, it is computed as: \[\texttt{dsize =
 * Size(round(fx*src.cols), round(fy*src.rows))}\] Either dsize or both fx and fy must be non-zero.
 *
 * @param fx scale factor along the horizontal axis; when it equals 0, it is computed as
 * \[\texttt{(double)dsize.width/src.cols}\]
 *
 * @param fy scale factor along the vertical axis; when it equals 0, it is computed as
 * \[\texttt{(double)dsize.height/src.rows}\]
 *
 * @param interpolation interpolation method, see InterpolationFlags
 */
export declare function resize(src: Mat, dst: Mat, dsize: Size, fx?: double, fy?: double, interpolation?: int): void;
/**
 * The function warpAffine transforms the source image using the specified matrix:
 *
 * `\\[\\texttt{dst} (x,y) = \\texttt{src} ( \\texttt{M} _{11} x + \\texttt{M} _{12} y + \\texttt{M}
 * _{13}, \\texttt{M} _{21} x + \\texttt{M} _{22} y + \\texttt{M} _{23})\\]`
 *
 * when the flag [WARP_INVERSE_MAP] is set. Otherwise, the transformation is first inverted with
 * [invertAffineTransform] and then put in the formula above instead of M. The function cannot operate
 * in-place.
 *
 * [warpPerspective], [resize], [remap], [getRectSubPix], [transform]
 *
 * @param src input image.
 *
 * @param dst output image that has the size dsize and the same type as src .
 *
 * @param M $2\times 3$ transformation matrix.
 *
 * @param dsize size of the output image.
 *
 * @param flags combination of interpolation methods (see InterpolationFlags) and the optional flag
 * WARP_INVERSE_MAP that means that M is the inverse transformation (
 * $\texttt{dst}\rightarrow\texttt{src}$ ).
 *
 * @param borderMode pixel extrapolation method (see BorderTypes); when borderMode=BORDER_TRANSPARENT,
 * it means that the pixels in the destination image corresponding to the "outliers" in the source
 * image are not modified by the function.
 *
 * @param borderValue value used in case of a constant border; by default, it is 0.
 */
export declare function warpAffine(src: Mat, dst: Mat, M: Mat, dsize: Size, flags?: int, borderMode?: int, borderValue?: any): void;
/**
 * The function warpPerspective transforms the source image using the specified matrix:
 *
 * `\\[\\texttt{dst} (x,y) = \\texttt{src} \\left ( \\frac{M_{11} x + M_{12} y + M_{13}}{M_{31} x +
 * M_{32} y + M_{33}} , \\frac{M_{21} x + M_{22} y + M_{23}}{M_{31} x + M_{32} y + M_{33}} \\right
 * )\\]`
 *
 * when the flag [WARP_INVERSE_MAP] is set. Otherwise, the transformation is first inverted with invert
 * and then put in the formula above instead of M. The function cannot operate in-place.
 *
 * [warpAffine], [resize], [remap], [getRectSubPix], [perspectiveTransform]
 *
 * @param src input image.
 *
 * @param dst output image that has the size dsize and the same type as src .
 *
 * @param M $3\times 3$ transformation matrix.
 *
 * @param dsize size of the output image.
 *
 * @param flags combination of interpolation methods (INTER_LINEAR or INTER_NEAREST) and the optional
 * flag WARP_INVERSE_MAP, that sets M as the inverse transformation (
 * $\texttt{dst}\rightarrow\texttt{src}$ ).
 *
 * @param borderMode pixel extrapolation method (BORDER_CONSTANT or BORDER_REPLICATE).
 *
 * @param borderValue value used in case of a constant border; by default, it equals 0.
 */
export declare function warpPerspective(src: Mat, dst: Mat, M: Mat, dsize: Size, flags?: int, borderMode?: int, borderValue?: any): void;
/**
 * <a name="da/d54/group__imgproc__transform_1polar_remaps_reference_image"></a>
 *  Transform the source image using the following transformation: `\\[ dst(\\rho , \\phi ) = src(x,y)
 * \\]`
 *
 * where `\\[ \\begin{array}{l} \\vec{I} = (x - center.x, \\;y - center.y) \\\\ \\phi = Kangle \\cdot
 * \\texttt{angle} (\\vec{I}) \\\\ \\rho = \\left\\{\\begin{matrix} Klin \\cdot \\texttt{magnitude}
 * (\\vec{I}) & default \\\\ Klog \\cdot log_e(\\texttt{magnitude} (\\vec{I})) & if \\; semilog \\\\
 * \\end{matrix}\\right. \\end{array} \\]`
 *
 * and `\\[ \\begin{array}{l} Kangle = dsize.height / 2\\Pi \\\\ Klin = dsize.width / maxRadius \\\\
 * Klog = dsize.width / log_e(maxRadius) \\\\ \\end{array} \\]`
 *
 * Polar mapping can be linear or semi-log. Add one of [WarpPolarMode] to `flags` to specify the polar
 * mapping mode.
 *
 * Linear is the default mode.
 *
 * The semilog mapping emulates the human "foveal" vision that permit very high acuity on the line of
 * sight (central vision) in contrast to peripheral vision where acuity is minor.
 *
 * if both values in `dsize <=0` (default), the destination image will have (almost) same area of
 * source bounding circle: `\\[\\begin{array}{l} dsize.area \\leftarrow (maxRadius^2 \\cdot \\Pi) \\\\
 * dsize.width = \\texttt{cvRound}(maxRadius) \\\\ dsize.height = \\texttt{cvRound}(maxRadius \\cdot
 * \\Pi) \\\\ \\end{array}\\]`
 * if only `dsize.height <= 0`, the destination image area will be proportional to the bounding circle
 * area but scaled by `Kx * Kx`: `\\[\\begin{array}{l} dsize.height = \\texttt{cvRound}(dsize.width
 * \\cdot \\Pi) \\\\ \\end{array} \\]`
 * if both values in `dsize > 0`, the destination image will have the given size therefore the area of
 * the bounding circle will be scaled to `dsize`.
 *
 * You can get reverse mapping adding [WARP_INVERSE_MAP] to `flags`
 *
 * ```cpp
 *         // direct transform
 *         warpPolar(src, lin_polar_img, Size(),center, maxRadius, flags);                     //
 * linear Polar
 *         warpPolar(src, log_polar_img, Size(),center, maxRadius, flags + WARP_POLAR_LOG);    //
 * semilog Polar
 *         // inverse transform
 *         warpPolar(lin_polar_img, recovered_lin_polar_img, src.size(), center, maxRadius, flags +
 * WARP_INVERSE_MAP);
 *         warpPolar(log_polar_img, recovered_log_polar, src.size(), center, maxRadius, flags +
 * WARP_POLAR_LOG + WARP_INVERSE_MAP);
 * ```
 *
 *  In addiction, to calculate the original coordinate from a polar mapped coordinate `$(rho, phi)->(x,
 * y)$`:
 *
 * ```cpp
 *         double angleRad, magnitude;
 *         double Kangle = dst.rows / CV_2PI;
 *         angleRad = phi / Kangle;
 *         if (flags & WARP_POLAR_LOG)
 *         {
 *             double Klog = dst.cols / std::log(maxRadius);
 *             magnitude = std::exp(rho / Klog);
 *         }
 *         else
 *         {
 *             double Klin = dst.cols / maxRadius;
 *             magnitude = rho / Klin;
 *         }
 *         int x = cvRound(center.x + magnitude * cos(angleRad));
 *         int y = cvRound(center.y + magnitude * sin(angleRad));
 * ```
 *
 * The function can not operate in-place.
 * To calculate magnitude and angle in degrees [cartToPolar] is used internally thus angles are
 * measured from 0 to 360 with accuracy about 0.3 degrees.
 * This function uses [remap]. Due to current implementation limitations the size of an input and
 * output images should be less than 32767x32767.
 *
 * [cv::remap]
 *
 * @param src Source image.
 *
 * @param dst Destination image. It will have same type as src.
 *
 * @param dsize The destination image size (see description for valid options).
 *
 * @param center The transformation center.
 *
 * @param maxRadius The radius of the bounding circle to transform. It determines the inverse magnitude
 * scale parameter too.
 *
 * @param flags A combination of interpolation methods, InterpolationFlags + WarpPolarMode.
 * Add WARP_POLAR_LINEAR to select linear polar mapping (default)Add WARP_POLAR_LOG to select semilog
 * polar mappingAdd WARP_INVERSE_MAP for reverse mapping.
 */
export declare function warpPolar(src: Mat, dst: Mat, dsize: Size, center: Point, maxRadius: double, flags: int): void;
/**
 * nearest neighbor interpolation
 *
 */
export declare const INTER_NEAREST: InterpolationFlags;
/**
 * bilinear interpolation
 *
 */
export declare const INTER_LINEAR: InterpolationFlags;
/**
 * bicubic interpolation
 *
 */
export declare const INTER_CUBIC: InterpolationFlags;
/**
 * resampling using pixel area relation. It may be a preferred method for image decimation, as it gives
 * moire'-free results. But when the image is zoomed, it is similar to the INTER_NEAREST method.
 *
 */
export declare const INTER_AREA: InterpolationFlags;
/**
 * Lanczos interpolation over 8x8 neighborhood
 *
 */
export declare const INTER_LANCZOS4: InterpolationFlags;
/**
 * Bit exact bilinear interpolation
 *
 */
export declare const INTER_LINEAR_EXACT: InterpolationFlags;
/**
 * mask for interpolation codes
 *
 */
export declare const INTER_MAX: InterpolationFlags;
/**
 * flag, fills all of the destination image pixels. If some of them correspond to outliers in the
 * source image, they are set to zero
 *
 */
export declare const WARP_FILL_OUTLIERS: InterpolationFlags;
/**
 * flag, inverse transformation
 *
 * For example, [linearPolar] or [logPolar] transforms:
 *
 * flag is **not** set: `$dst( \\rho , \\phi ) = src(x,y)$`
 * flag is set: `$dst(x,y) = src( \\rho , \\phi )$`
 *
 */
export declare const WARP_INVERSE_MAP: InterpolationFlags;
export declare const INTER_BITS: InterpolationMasks;
export declare const INTER_BITS2: InterpolationMasks;
export declare const INTER_TAB_SIZE: InterpolationMasks;
export declare const INTER_TAB_SIZE2: InterpolationMasks;
export declare const WARP_POLAR_LINEAR: WarpPolarMode;
export declare const WARP_POLAR_LOG: WarpPolarMode;
export type InterpolationFlags = any;
export type InterpolationMasks = any;
export type WarpPolarMode = any;
export declare class Logger {
	static error(fmt: any, arg121: any): int;
	static fatal(fmt: any, arg122: any): int;
	static info(fmt: any, arg123: any): int;
	/**
	 *   Print log message
	 *
	 * @param level Log level
	 *
	 * @param fmt Message format
	 */
	static log(level: int, fmt: any, arg124: any): int;
	/**
	 *   Sets the logging destination
	 *
	 * @param name Filename or NULL for console
	 */
	static setDestination(name: any): void;
	/**
	 *   Sets the logging level. All messages with lower priority will be ignored.
	 *
	 * @param level Logging level
	 */
	static setLevel(level: int): void;
	static warn(fmt: any, arg125: any): int;
}
/**
 * Lsh hash table. As its key is a sub-feature, and as usually the size of it is pretty small, we keep
 * it as a continuous memory array. The value is an index in the corpus of features (we keep it as an
 * unsigned int for pure memory reasons, it could be a size_t)
 *
 * Source:
 * [opencv2/flann/lsh_table.h](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/flann/lsh_table.h#L261).
 *
 */
export declare class LshTable {
	/**
	 *   Default constructor
	 */
	constructor();
	/**
	 *   Default constructor Create the mask and allocate the memory
	 *
	 * @param feature_size is the size of the feature (considered as a ElementType[])
	 *
	 * @param key_size is the number of bits that are turned on in the feature
	 */
	constructor(feature_size: any, key_size: any);
	constructor(feature_size: any, subsignature_size: any);
	/**
	 *   Add a feature to the table
	 *
	 * @param value the value to store for that feature
	 *
	 * @param feature the feature itself
	 */
	add(value: any, feature: any): void;
	/**
	 *   Add a set of features to the table
	 *
	 * @param dataset the values to store
	 */
	add(dataset: Matrix): Matrix;
	/**
	 *   Get a bucket given the key
	 */
	getBucketFromKey(key: BucketKey): Bucket;
	/**
	 *   Compute the sub-signature of a feature
	 */
	getKey(arg50: any): size_t;
	/**
	 *   Return the Subsignature of a feature
	 *
	 * @param feature the feature to analyze
	 */
	getKey(feature: any): size_t;
	/**
	 *   Get statistics about the table
	 */
	getStats(): LshStats;
	getStats(): LshStats;
}
export declare const kArray: SpeedLevel;
export declare const kBitsetHash: SpeedLevel;
export declare const kHash: SpeedLevel;
/**
 * defines the speed fo the implementation kArray uses a vector for storing data kBitsetHash uses a
 * hash map but checks for the validity of a key with a bitset kHash uses a hash map only
 *
 */
export type SpeedLevel = any;
/**
 * <a name="d3/d63/classcv_1_1Mat_1CVMat_Details"></a> The class [Mat](#d3/d63/classcv_1_1Mat})
 * represents an n-dimensional dense numerical single-channel or multi-channel array. It can be used to
 * store real or complex-valued vectors and matrices, grayscale or color images, voxel volumes, vector
 * fields, point clouds, tensors, histograms (though, very high-dimensional histograms may be better
 * stored in a [SparseMat](#dd/da9/classcv_1_1SparseMat}) ). The data layout of the array `M` is
 * defined by the array `M.step[]`, so that the address of element `$(i_0,...,i_{M.dims-1})$`, where
 * `$0\\leq i_k<M.size[k]$`, is computed as: `\\[addr(M_{i_0,...,i_{M.dims-1}}) = M.data +
 * M.step[0]*i_0 + M.step[1]*i_1 + ... + M.step[M.dims-1]*i_{M.dims-1}\\]` In case of a 2-dimensional
 * array, the above formula is reduced to: `\\[addr(M_{i,j}) = M.data + M.step[0]*i + M.step[1]*j\\]`
 * Note that `M.step[i] >= M.step[i+1]` (in fact, `M.step[i] >= M.step[i+1]*M.size[i+1]` ). This means
 * that 2-dimensional matrices are stored row-by-row, 3-dimensional matrices are stored plane-by-plane,
 * and so on. M.step[M.dims-1] is minimal and always equal to the element size M.elemSize() .
 *
 * So, the data layout in [Mat](#d3/d63/classcv_1_1Mat}) is compatible with the majority of dense array
 * types from the standard toolkits and SDKs, such as Numpy (ndarray), Win32 (independent device
 * bitmaps), and others, that is, with any array that uses *steps* (or *strides*) to compute the
 * position of a pixel. Due to this compatibility, it is possible to make a
 * [Mat](#d3/d63/classcv_1_1Mat}) header for user-allocated data and process it in-place using OpenCV
 * functions.
 *
 * There are many different ways to create a [Mat](#d3/d63/classcv_1_1Mat}) object. The most popular
 * options are listed below:
 *
 * Use the create(nrows, ncols, type) method or the similar Mat(nrows, ncols, type[, fillValue])
 * constructor. A new array of the specified size and type is allocated. type has the same meaning as
 * in the cvCreateMat method. For example, CV_8UC1 means a 8-bit single-channel array, CV_32FC2 means a
 * 2-channel (complex) floating-point array, and so on.
 *
 * ```cpp
 * // make a 7x7 complex matrix filled with 1+3j.
 * Mat M(7,7,CV_32FC2,Scalar(1,3));
 * // and now turn M to a 100x60 15-channel 8-bit matrix.
 * // The old content will be deallocated
 * M.create(100,60,CV_8UC(15));
 * ```
 *
 *  As noted in the introduction to this chapter,
 * [create()](#d3/d63/classcv_1_1Mat_1a55ced2c8d844d683ea9a725c60037ad0}) allocates only a new array
 * when the shape or type of the current array are different from the specified ones.
 * Create a multi-dimensional array:
 *
 * ```cpp
 * // create a 100x100x100 8-bit array
 * int sz[] = {100, 100, 100};
 * Mat bigCube(3, sz, CV_8U, Scalar::all(0));
 * ```
 *
 *  It passes the number of dimensions =1 to the [Mat](#d3/d63/classcv_1_1Mat}) constructor but the
 * created array will be 2-dimensional with the number of columns set to 1. So,
 * [Mat::dims](#d3/d63/classcv_1_1Mat_1a39cf614aa52567e9a945cd2609bd767b}) is always >= 2 (can also be
 * 0 when the array is empty).
 * Use a copy constructor or assignment operator where there can be an array or expression on the right
 * side (see below). As noted in the introduction, the array assignment is an O(1) operation because it
 * only copies the header and increases the reference counter. The
 * [Mat::clone()](#d3/d63/classcv_1_1Mat_1adff2ea98da45eae0833e73582dd4a660}) method can be used to get
 * a full (deep) copy of the array when you need it.
 * Construct a header for a part of another array. It can be a single row, single column, several rows,
 * several columns, rectangular region in the array (called a *minor* in algebra) or a diagonal. Such
 * operations are also O(1) because the new header references the same data. You can actually modify a
 * part of the array using this feature, for example:
 *
 * ```cpp
 * // add the 5-th row, multiplied by 3 to the 3rd row
 * M.row(3) = M.row(3) + M.row(5)*3;
 * // now copy the 7-th column to the 1-st column
 * // M.col(1) = M.col(7); // this will not work
 * Mat M1 = M.col(1);
 * M.col(7).copyTo(M1);
 * // create a new 320x240 image
 * Mat img(Size(320,240),CV_8UC3);
 * // select a ROI
 * Mat roi(img, Rect(10,10,100,100));
 * // fill the ROI with (0,255,0) (which is green in RGB space);
 * // the original 320x240 image will be modified
 * roi = Scalar(0,255,0);
 * ```
 *
 *  Due to the additional datastart and dataend members, it is possible to compute a relative sub-array
 * position in the main *container* array using
 * [locateROI()](#d3/d63/classcv_1_1Mat_1a40b5b3371a9c2a4b2b8ce0c8068d7c96}):
 *
 * ```cpp
 * Mat A = Mat::eye(10, 10, CV_32S);
 * // extracts A columns, 1 (inclusive) to 3 (exclusive).
 * Mat B = A(Range::all(), Range(1, 3));
 * // extracts B rows, 5 (inclusive) to 9 (exclusive).
 * // that is, C \\~ A(Range(5, 9), Range(1, 3))
 * Mat C = B(Range(5, 9), Range::all());
 * Size size; Point ofs;
 * C.locateROI(size, ofs);
 * // size will be (width=10,height=10) and the ofs will be (x=1, y=5)
 * ```
 *
 *  As in case of whole matrices, if you need a deep copy, use the
 * `[clone()](#d3/d63/classcv_1_1Mat_1adff2ea98da45eae0833e73582dd4a660})` method of the extracted
 * sub-matrices.
 * Make a header for user-allocated data. It can be useful to do the following:
 *
 * Process "foreign" data using OpenCV (for example, when you implement a DirectShow* filter or a
 * processing module for gstreamer, and so on). For example:
 *
 * ```cpp
 * void process_video_frame(const unsigned char* pixels,
 *                          int width, int height, int step)
 * {
 *     Mat img(height, width, CV_8UC3, pixels, step);
 *     GaussianBlur(img, img, Size(7,7), 1.5, 1.5);
 * }
 * ```
 *
 * Quickly initialize small matrices and/or get a super-fast element access.
 *
 * ```cpp
 * double m[3][3] = {{a, b, c}, {d, e, f}, {g, h, i}};
 * Mat M = Mat(3, 3, CV_64F, m).inv();
 * ```
 *
 * Use MATLAB-style array initializers,
 * [zeros()](#d3/d63/classcv_1_1Mat_1a0b57b6a326c8876d944d188a46e0f556}),
 * [ones()](#d3/d63/classcv_1_1Mat_1a69ae0402d116fc9c71908d8508dc2f09}),
 * [eye()](#d3/d63/classcv_1_1Mat_1a2cf9b9acde7a9852542bbc20ef851ed2}), for example:
 *
 * ```cpp
 * // create a double-precision identity matrix and add it to M.
 * M += Mat::eye(M.rows, M.cols, CV_64F);
 * ```
 *
 * Use a comma-separated initializer:
 *
 * ```cpp
 * // create a 3x3 double-precision identity matrix
 * Mat M = (Mat_<double>(3,3) << 1, 0, 0, 0, 1, 0, 0, 0, 1);
 * ```
 *
 *  With this approach, you first call a constructor of the [Mat](#d3/d63/classcv_1_1Mat}) class with
 * the proper parameters, and then you just put `<< operator` followed by comma-separated values that
 * can be constants, variables, expressions, and so on. Also, note the extra parentheses required to
 * avoid compilation errors.
 *
 * Once the array is created, it is automatically managed via a reference-counting mechanism. If the
 * array header is built on top of user-allocated data, you should handle the data by yourself. The
 * array data is deallocated when no one points to it. If you want to release the data pointed by a
 * array header before the array destructor is called, use
 * [Mat::release()](#d3/d63/classcv_1_1Mat_1ae48d4913285518e2c21a3457017e716e}).
 *
 * The next important thing to learn about the array class is element access. This manual already
 * described how to compute an address of each array element. Normally, you are not required to use the
 * formula directly in the code. If you know the array element type (which can be retrieved using the
 * method [Mat::type()](#d3/d63/classcv_1_1Mat_1af2d2652e552d7de635988f18a84b53e5}) ), you can access
 * the element `$M_{ij}$` of a 2-dimensional array as:
 *
 * ```cpp
 * M.at<double>(i,j) += 1.f;
 * ```
 *
 *  assuming that `M` is a double-precision floating-point array. There are several variants of the
 * method at for a different number of dimensions.
 *
 * If you need to process a whole row of a 2D array, the most efficient way is to get the pointer to
 * the row first, and then just use the plain C operator [] :
 *
 * ```cpp
 * // compute sum of positive matrix elements
 * // (assuming that M is a double-precision matrix)
 * double sum=0;
 * for(int i = 0; i < M.rows; i++)
 * {
 *     const double* Mi = M.ptr<double>(i);
 *     for(int j = 0; j < M.cols; j++)
 *         sum += std::max(Mi[j], 0.);
 * }
 * ```
 *
 *  Some operations, like the one above, do not actually depend on the array shape. They just process
 * elements of an array one by one (or elements from multiple arrays that have the same coordinates,
 * for example, array addition). Such operations are called *element-wise*. It makes sense to check
 * whether all the input/output arrays are continuous, namely, have no gaps at the end of each row. If
 * yes, process them as a long single row:
 *
 * ```cpp
 * // compute the sum of positive matrix elements, optimized variant
 * double sum=0;
 * int cols = M.cols, rows = M.rows;
 * if(M.isContinuous())
 * {
 *     cols *= rows;
 *     rows = 1;
 * }
 * for(int i = 0; i < rows; i++)
 * {
 *     const double* Mi = M.ptr<double>(i);
 *     for(int j = 0; j < cols; j++)
 *         sum += std::max(Mi[j], 0.);
 * }
 * ```
 *
 *  In case of the continuous matrix, the outer loop body is executed just once. So, the overhead is
 * smaller, which is especially noticeable in case of small matrices.
 *
 * Finally, there are STL-style iterators that are smart enough to skip gaps between successive rows:
 *
 * ```cpp
 * // compute sum of positive matrix elements, iterator-based variant
 * double sum=0;
 * MatConstIterator_<double> it = M.begin<double>(), it_end = M.end<double>();
 * for(; it != it_end; ++it)
 *     sum += std::max(*it, 0.);
 * ```
 *
 *  The matrix iterators are random-access iterators, so they can be passed to any STL algorithm,
 * including [std::sort()](#d2/de8/group__core__array_1ga45dd56da289494ce874be2324856898f}).
 *
 * Matrix Expressions and arithmetic see [MatExpr](#d1/d10/classcv_1_1MatExpr})
 *
 * Source:
 * [opencv2/core/mat.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/mat.hpp#L2073).
 *
 */
export declare class Mat extends EmscriptenEmbindInstance {
	allocator: MatAllocator;
	cols: int;
	data: Uint8Array;
	data8S: Int8Array;
	data8U: Uint8Array;
	data16U: Uint16Array;
	data16S: Int16Array;
	data32U: Uint32Array;
	data32S: Int32Array;
	data32F: Float32Array;
	data64F: Float64Array;
	dataend: uchar;
	datalimit: uchar;
	datastart: uchar;
	dims: int;
	/**
	 *   includes several bit-fields:
	 *
	 * the magic signature
	 * continuity flag
	 * depth
	 * number of channels
	 *
	 */
	flags: int;
	rows: int;
	size: MatSize;
	step: MatStep;
	u: UMatData;
	/**
	 *   These are various constructors that form a matrix. As noted in the AutomaticAllocation, often the
	 * default constructor is enough, and the proper matrix will be allocated by an OpenCV function. The
	 * constructed matrix can further be assigned to another matrix or matrix expression or can be
	 * allocated with [Mat::create] . In the former case, the old content is de-referenced.
	 */
	constructor();
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param rows Number of rows in a 2D array.
	 *
	 * @param cols Number of columns in a 2D array.
	 *
	 * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),
	 * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.
	 */
	constructor(rows: int, cols: int, type: int);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param size 2D array size: Size(cols, rows) . In the Size() constructor, the number of rows and
	 * the number of columns go in the reverse order.
	 *
	 * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),
	 * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.
	 */
	constructor(size: Size, type: int);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param rows Number of rows in a 2D array.
	 *
	 * @param cols Number of columns in a 2D array.
	 *
	 * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),
	 * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.
	 *
	 * @param s An optional value to initialize each matrix element with. To set all the matrix elements
	 * to the particular value after the construction, use the assignment operator Mat::operator=(const
	 * Scalar& value) .
	 */
	constructor(rows: int, cols: int, type: int, s: Scalar);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param size 2D array size: Size(cols, rows) . In the Size() constructor, the number of rows and
	 * the number of columns go in the reverse order.
	 *
	 * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),
	 * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.
	 *
	 * @param s An optional value to initialize each matrix element with. To set all the matrix elements
	 * to the particular value after the construction, use the assignment operator Mat::operator=(const
	 * Scalar& value) .
	 */
	constructor(size: Size, type: int, s: Scalar);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param ndims Array dimensionality.
	 *
	 * @param sizes Array of integers specifying an n-dimensional array shape.
	 *
	 * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),
	 * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.
	 */
	constructor(ndims: int, sizes: any, type: int);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param sizes Array of integers specifying an n-dimensional array shape.
	 *
	 * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),
	 * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.
	 */
	constructor(sizes: any, type: int);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param ndims Array dimensionality.
	 *
	 * @param sizes Array of integers specifying an n-dimensional array shape.
	 *
	 * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),
	 * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.
	 *
	 * @param s An optional value to initialize each matrix element with. To set all the matrix elements
	 * to the particular value after the construction, use the assignment operator Mat::operator=(const
	 * Scalar& value) .
	 */
	constructor(ndims: int, sizes: any, type: int, s: Scalar);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param sizes Array of integers specifying an n-dimensional array shape.
	 *
	 * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),
	 * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.
	 *
	 * @param s An optional value to initialize each matrix element with. To set all the matrix elements
	 * to the particular value after the construction, use the assignment operator Mat::operator=(const
	 * Scalar& value) .
	 */
	constructor(sizes: any, type: int, s: Scalar);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param m Array that (as a whole or partly) is assigned to the constructed matrix. No data is
	 * copied by these constructors. Instead, the header pointing to m data or its sub-array is constructed
	 * and associated with it. The reference counter, if any, is incremented. So, when you modify the
	 * matrix formed using such a constructor, you also modify the corresponding elements of m . If you
	 * want to have an independent copy of the sub-array, use Mat::clone() .
	 */
	constructor(m: Mat);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param rows Number of rows in a 2D array.
	 *
	 * @param cols Number of columns in a 2D array.
	 *
	 * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),
	 * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.
	 *
	 * @param data Pointer to the user data. Matrix constructors that take data and step parameters do
	 * not allocate matrix data. Instead, they just initialize the matrix header that points to the
	 * specified data, which means that no data is copied. This operation is very efficient and can be used
	 * to process external data using OpenCV functions. The external data is not automatically deallocated,
	 * so you should take care of it.
	 *
	 * @param step Number of bytes each matrix row occupies. The value should include the padding bytes
	 * at the end of each row, if any. If the parameter is missing (set to AUTO_STEP ), no padding is
	 * assumed and the actual step is calculated as cols*elemSize(). See Mat::elemSize.
	 */
	constructor(rows: int, cols: int, type: int, data: any, step?: size_t);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param size 2D array size: Size(cols, rows) . In the Size() constructor, the number of rows and
	 * the number of columns go in the reverse order.
	 *
	 * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),
	 * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.
	 *
	 * @param data Pointer to the user data. Matrix constructors that take data and step parameters do
	 * not allocate matrix data. Instead, they just initialize the matrix header that points to the
	 * specified data, which means that no data is copied. This operation is very efficient and can be used
	 * to process external data using OpenCV functions. The external data is not automatically deallocated,
	 * so you should take care of it.
	 *
	 * @param step Number of bytes each matrix row occupies. The value should include the padding bytes
	 * at the end of each row, if any. If the parameter is missing (set to AUTO_STEP ), no padding is
	 * assumed and the actual step is calculated as cols*elemSize(). See Mat::elemSize.
	 */
	constructor(size: Size, type: int, data: any, step?: size_t);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param ndims Array dimensionality.
	 *
	 * @param sizes Array of integers specifying an n-dimensional array shape.
	 *
	 * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),
	 * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.
	 *
	 * @param data Pointer to the user data. Matrix constructors that take data and step parameters do
	 * not allocate matrix data. Instead, they just initialize the matrix header that points to the
	 * specified data, which means that no data is copied. This operation is very efficient and can be used
	 * to process external data using OpenCV functions. The external data is not automatically deallocated,
	 * so you should take care of it.
	 *
	 * @param steps Array of ndims-1 steps in case of a multi-dimensional array (the last step is always
	 * set to the element size). If not specified, the matrix is assumed to be continuous.
	 */
	constructor(ndims: int, sizes: any, type: int, data: any, steps?: any);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param sizes Array of integers specifying an n-dimensional array shape.
	 *
	 * @param type Array type. Use CV_8UC1, ..., CV_64FC4 to create 1-4 channel matrices, or CV_8UC(n),
	 * ..., CV_64FC(n) to create multi-channel (up to CV_CN_MAX channels) matrices.
	 *
	 * @param data Pointer to the user data. Matrix constructors that take data and step parameters do
	 * not allocate matrix data. Instead, they just initialize the matrix header that points to the
	 * specified data, which means that no data is copied. This operation is very efficient and can be used
	 * to process external data using OpenCV functions. The external data is not automatically deallocated,
	 * so you should take care of it.
	 *
	 * @param steps Array of ndims-1 steps in case of a multi-dimensional array (the last step is always
	 * set to the element size). If not specified, the matrix is assumed to be continuous.
	 */
	constructor(sizes: any, type: int, data: any, steps?: any);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param m Array that (as a whole or partly) is assigned to the constructed matrix. No data is
	 * copied by these constructors. Instead, the header pointing to m data or its sub-array is constructed
	 * and associated with it. The reference counter, if any, is incremented. So, when you modify the
	 * matrix formed using such a constructor, you also modify the corresponding elements of m . If you
	 * want to have an independent copy of the sub-array, use Mat::clone() .
	 *
	 * @param rowRange Range of the m rows to take. As usual, the range start is inclusive and the range
	 * end is exclusive. Use Range::all() to take all the rows.
	 *
	 * @param colRange Range of the m columns to take. Use Range::all() to take all the columns.
	 */
	constructor(m: Mat, rowRange: Range, colRange?: Range);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param m Array that (as a whole or partly) is assigned to the constructed matrix. No data is
	 * copied by these constructors. Instead, the header pointing to m data or its sub-array is constructed
	 * and associated with it. The reference counter, if any, is incremented. So, when you modify the
	 * matrix formed using such a constructor, you also modify the corresponding elements of m . If you
	 * want to have an independent copy of the sub-array, use Mat::clone() .
	 *
	 * @param roi Region of interest.
	 */
	constructor(m: Mat, roi: Rect);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param m Array that (as a whole or partly) is assigned to the constructed matrix. No data is
	 * copied by these constructors. Instead, the header pointing to m data or its sub-array is constructed
	 * and associated with it. The reference counter, if any, is incremented. So, when you modify the
	 * matrix formed using such a constructor, you also modify the corresponding elements of m . If you
	 * want to have an independent copy of the sub-array, use Mat::clone() .
	 *
	 * @param ranges Array of selected ranges of m along each dimensionality.
	 */
	constructor(m: Mat, ranges: Range);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param m Array that (as a whole or partly) is assigned to the constructed matrix. No data is
	 * copied by these constructors. Instead, the header pointing to m data or its sub-array is constructed
	 * and associated with it. The reference counter, if any, is incremented. So, when you modify the
	 * matrix formed using such a constructor, you also modify the corresponding elements of m . If you
	 * want to have an independent copy of the sub-array, use Mat::clone() .
	 *
	 * @param ranges Array of selected ranges of m along each dimensionality.
	 */
	constructor(m: Mat, ranges: Range);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param vec STL vector whose elements form the matrix. The matrix has a single column and the
	 * number of rows equal to the number of vector elements. Type of the matrix matches the type of vector
	 * elements. The constructor can handle arbitrary types, for which there is a properly declared
	 * DataType . This means that the vector elements must be primitive numbers or uni-type numerical
	 * tuples of numbers. Mixed-type structures are not supported. The corresponding constructor is
	 * explicit. Since STL vectors are not automatically converted to Mat instances, you should write
	 * Mat(vec) explicitly. Unless you copy the data into the matrix ( copyData=true ), no new elements
	 * will be added to the vector because it can potentially yield vector data reallocation, and, thus,
	 * the matrix data pointer will be invalid.
	 *
	 * @param copyData Flag to specify whether the underlying data of the STL vector should be copied to
	 * (true) or shared with (false) the newly constructed matrix. When the data is copied, the allocated
	 * buffer is managed using Mat reference counting mechanism. While the data is shared, the reference
	 * counter is NULL, and you should not deallocate the data until the matrix is not destructed.
	 */
	constructor(arg3: any, vec: any, copyData?: bool);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	constructor(arg4: any, arg5?: typename, list?: any);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	constructor(arg6: any, sizes: any, list: any);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	constructor(arg7: any, _Nm: size_t, arr: any, copyData?: bool);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	constructor(arg8: any, n: int, vec: Vec, copyData?: bool);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	constructor(arg9: any, m: int, n: int, mtx: Matx, copyData?: bool);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	constructor(arg10: any, pt: Point_, copyData?: bool);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	constructor(arg11: any, pt: Point3_, copyData?: bool);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	constructor(arg12: any, commaInitializer: MatCommaInitializer_);
	constructor(m: any);
	constructor(m: Mat);
	/**
	 *   The method increments the reference counter associated with the matrix data. If the matrix header
	 * points to an external data set (see [Mat::Mat] ), the reference counter is NULL, and the method has
	 * no effect in this case. Normally, to avoid memory leaks, the method should not be called explicitly.
	 * It is called implicitly by the matrix assignment operator. The reference counter increment is an
	 * atomic operation on the platforms that support it. Thus, it is safe to operate on the same matrices
	 * asynchronously in different threads.
	 */
	addref(): void;
	/**
	 *   The method is complimentary to [Mat::locateROI] . The typical use of these functions is to
	 * determine the submatrix position within the parent matrix and then shift the position somehow.
	 * Typically, it can be required for filtering operations when pixels outside of the ROI should be
	 * taken into account. When all the method parameters are positive, the ROI needs to grow in all
	 * directions by the specified amount, for example:
	 *
	 *   ```cpp
	 *   A.adjustROI(2, 2, 2, 2);
	 *   ```
	 *
	 *    In this example, the matrix size is increased by 4 elements in each direction. The matrix is
	 * shifted by 2 elements to the left and 2 elements up, which brings in all the necessary pixels for
	 * the filtering with the 5x5 kernel.
	 *
	 *   adjustROI forces the adjusted ROI to be inside of the parent matrix that is boundaries of the
	 * adjusted ROI are constrained by boundaries of the parent matrix. For example, if the submatrix A is
	 * located in the first row of a parent matrix and you called A.adjustROI(2, 2, 2, 2) then A will not
	 * be increased in the upward direction.
	 *
	 *   The function is used internally by the OpenCV filtering functions, like filter2D , morphological
	 * operations, and so on.
	 *
	 *   [copyMakeBorder]
	 *
	 * @param dtop Shift of the top submatrix boundary upwards.
	 *
	 * @param dbottom Shift of the bottom submatrix boundary downwards.
	 *
	 * @param dleft Shift of the left submatrix boundary to the left.
	 *
	 * @param dright Shift of the right submatrix boundary to the right.
	 */
	adjustROI(dtop: int, dbottom: int, dleft: int, dright: int): Mat;
	/**
	 *   The methods return the matrix read-only or read-write iterators. The use of matrix iterators is
	 * very similar to the use of bi-directional STL iterators. In the example below, the alpha blending
	 * function is rewritten using the matrix iterators:
	 *
	 *   ```cpp
	 *   template<typename T>
	 *   void alphaBlendRGBA(const Mat& src1, const Mat& src2, Mat& dst)
	 *   {
	 *       typedef Vec<T, 4> VT;
	 *
	 *       const float alpha_scale = (float)std::numeric_limits<T>::max(),
	 *                   inv_scale = 1.f/alpha_scale;
	 *
	 *       CV_Assert( src1.type() == src2.type() &&
	 *                  src1.type() == traits::Type<VT>::value &&
	 *                  src1.size() == src2.size());
	 *       Size size = src1.size();
	 *       dst.create(size, src1.type());
	 *
	 *       MatConstIterator_<VT> it1 = src1.begin<VT>(), it1_end = src1.end<VT>();
	 *       MatConstIterator_<VT> it2 = src2.begin<VT>();
	 *       MatIterator_<VT> dst_it = dst.begin<VT>();
	 *
	 *       for( ; it1 != it1_end; ++it1, ++it2, ++dst_it )
	 *       {
	 *           VT pix1 = *it1, pix2 = *it2;
	 *           float alpha = pix1[3]*inv_scale, beta = pix2[3]*inv_scale;
	 * dst_it = VT(saturate_cast<T>(pix1[0]*alpha + pix2[0]*beta),
	 *                        saturate_cast<T>(pix1[1]*alpha + pix2[1]*beta),
	 *                        saturate_cast<T>(pix1[2]*alpha + pix2[2]*beta),
	 *                        saturate_cast<T>((1 - (1-alpha)*(1-beta))*alpha_scale));
	 *       }
	 *   }
	 *   ```
	 */
	begin(arg25: any): MatIterator_;
	begin(arg26: any): MatConstIterator_;
	/**
	 *   The method returns the number of matrix channels.
	 */
	channels(): int;
	/**
	 *   -1 if the requirement is not satisfied. Otherwise, it returns the number of elements in the
	 * matrix. Note that an element may have multiple channels.
	 *   The following code demonstrates its usage for a 2-d matrix:
	 *
	 *   ```cpp
	 *       cv::Mat mat(20, 1, CV_32FC2);
	 *       int n = mat.checkVector(2);
	 *       CV_Assert(n == 20); // mat has 20 elements
	 *
	 *       mat.create(20, 2, CV_32FC1);
	 *       n = mat.checkVector(1);
	 *       CV_Assert(n == -1); // mat is neither a column nor a row vector
	 *
	 *       n = mat.checkVector(2);
	 *       CV_Assert(n == 20); // the 2 columns are considered as 1 element
	 *   ```
	 *
	 *    The following code demonstrates its usage for a 3-d matrix:
	 *
	 *   ```cpp
	 *       int dims[] = {1, 3, 5}; // 1 plane, every plane has 3 rows and 5 columns
	 *       mat.create(3, dims, CV_32FC1); // for 3-d mat, it MUST have only 1 channel
	 *       n = mat.checkVector(5); // the 5 columns are considered as 1 element
	 *       CV_Assert(n == 3);
	 *
	 *       int dims2[] = {3, 1, 5}; // 3 planes, every plane has 1 row and 5 columns
	 *       mat.create(3, dims2, CV_32FC1);
	 *       n = mat.checkVector(5); // the 5 columns are considered as 1 element
	 *       CV_Assert(n == 3);
	 *   ```
	 *
	 * @param elemChannels Number of channels or number of columns the matrix should have. For a 2-D
	 * matrix, when the matrix has only 1 column, then it should have elemChannels channels; When the
	 * matrix has only 1 channel, then it should have elemChannels columns. For a 3-D matrix, it should
	 * have only one channel. Furthermore, if the number of planes is not one, then the number of rows
	 * within every plane has to be 1; if the number of rows within every plane is not 1, then the number
	 * of planes has to be 1.
	 *
	 * @param depth The depth the matrix should have. Set it to -1 when any depth is fine.
	 *
	 * @param requireContinuous Set it to true to require the matrix to be continuous
	 */
	checkVector(elemChannels: int, depth?: int, requireContinuous?: bool): int;
	/**
	 *   The method creates a full copy of the array. The original step[] is not taken into account. So,
	 * the array copy is a continuous array occupying [total()]*elemSize() bytes.
	 */
	clone(): Mat;
	/**
	 *   The method makes a new header for the specified matrix column and returns it. This is an O(1)
	 * operation, regardless of the matrix size. The underlying data of the new matrix is shared with the
	 * original matrix. See also the [Mat::row] description.
	 *
	 * @param x A 0-based column index.
	 */
	col(x: int): Mat;
	/**
	 *   The method makes a new header for the specified column span of the matrix. Similarly to [Mat::row]
	 * and [Mat::col] , this is an O(1) operation.
	 *
	 * @param startcol An inclusive 0-based start index of the column span.
	 *
	 * @param endcol An exclusive 0-based ending index of the column span.
	 */
	colRange(startcol: int, endcol: int): Mat;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param r Range structure containing both the start and the end indices.
	 */
	colRange(r: Range): Mat;
	/**
	 *   The method converts source pixel values to the target data type. saturate_cast<> is applied at the
	 * end to avoid possible overflows:
	 *
	 *   `\\[m(x,y) = saturate \\_ cast<rType>( \\alpha (*this)(x,y) + \\beta )\\]`
	 *
	 * @param m output matrix; if it does not have a proper size or type before the operation, it is
	 * reallocated.
	 *
	 * @param rtype desired output matrix type or, rather, the depth since the number of channels are the
	 * same as the input has; if rtype is negative, the output matrix will have the same type as the input.
	 *
	 * @param alpha optional scale factor.
	 *
	 * @param beta optional delta added to the scaled values.
	 */
	convertTo(m: Mat, rtype: int, alpha?: double, beta?: double): Mat;
	copySize(m: Mat): Mat;
	/**
	 *   The method copies the matrix data to another matrix. Before copying the data, the method invokes :
	 *
	 *
	 *   ```cpp
	 *   m.create(this->size(), this->type());
	 *   ```
	 *
	 *    so that the destination matrix is reallocated if needed. While m.copyTo(m); works flawlessly, the
	 * function does not handle the case of a partial overlap between the source and the destination
	 * matrices.
	 *
	 *   When the operation mask is specified, if the [Mat::create] call shown above reallocates the
	 * matrix, the newly allocated matrix is initialized with all zeros before copying the data.
	 *
	 * @param m Destination matrix. If it does not have a proper size or type before the operation, it is
	 * reallocated.
	 */
	copyTo(m: Mat): Mat;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param m Destination matrix. If it does not have a proper size or type before the operation, it is
	 * reallocated.
	 *
	 * @param mask Operation mask of the same size as *this. Its non-zero elements indicate which matrix
	 * elements need to be copied. The mask has to be of type CV_8U and can have 1 or multiple channels.
	 */
	copyTo(m: Mat, mask: Mat): Mat;
	/**
	 *   This is one of the key [Mat] methods. Most new-style OpenCV functions and methods that produce
	 * arrays call this method for each output array. The method uses the following algorithm:
	 *
	 * If the current array shape and the type match the new ones, return immediately. Otherwise,
	 * de-reference the previous data by calling [Mat::release].
	 * Initialize the new header.
	 * Allocate the new data of [total()]*elemSize() bytes.
	 * Allocate the new, associated with the data, reference counter and set it to 1.
	 *
	 *   Such a scheme makes the memory management robust and efficient at the same time and helps avoid
	 * extra typing for you. This means that usually there is no need to explicitly allocate output arrays.
	 * That is, instead of writing:
	 *
	 *   ```cpp
	 *   Mat color;
	 *   ...
	 *   Mat gray(color.rows, color.cols, color.depth());
	 *   cvtColor(color, gray, COLOR_BGR2GRAY);
	 *   ```
	 *
	 *    you can simply write:
	 *
	 *   ```cpp
	 *   Mat color;
	 *   ...
	 *   Mat gray;
	 *   cvtColor(color, gray, COLOR_BGR2GRAY);
	 *   ```
	 *
	 *    because cvtColor, as well as the most of OpenCV functions, calls [Mat::create()] for the output
	 * array internally.
	 *
	 * @param rows New number of rows.
	 *
	 * @param cols New number of columns.
	 *
	 * @param type New matrix type.
	 */
	create(rows: int, cols: int, type: int): void;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param size Alternative new matrix size specification: Size(cols, rows)
	 *
	 * @param type New matrix type.
	 */
	create(size: Size, type: int): Size;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param ndims New array dimensionality.
	 *
	 * @param sizes Array of integers specifying a new array shape.
	 *
	 * @param type New matrix type.
	 */
	create(ndims: int, sizes: any, type: int): void;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param sizes Array of integers specifying a new array shape.
	 *
	 * @param type New matrix type.
	 */
	create(sizes: any, type: int): void;
	/**
	 *   The method computes a cross-product of two 3-element vectors. The vectors must be 3-element
	 * floating-point vectors of the same shape and size. The result is another 3-element vector of the
	 * same shape and type as operands.
	 *
	 * @param m Another cross-product operand.
	 */
	cross(m: Mat): Mat;
	deallocate(): void;
	/**
	 *   The method returns the identifier of the matrix element depth (the type of each individual
	 * channel). For example, for a 16-bit signed element array, the method returns CV_16S . A complete
	 * list of matrix types contains the following values:
	 *
	 * CV_8U - 8-bit unsigned integers ( 0..255 )
	 * CV_8S - 8-bit signed integers ( -128..127 )
	 * CV_16U - 16-bit unsigned integers ( 0..65535 )
	 * CV_16S - 16-bit signed integers ( -32768..32767 )
	 * CV_32S - 32-bit signed integers ( -2147483648..2147483647 )
	 * CV_32F - 32-bit floating-point numbers ( -FLT_MAX..FLT_MAX, INF, NAN )
	 * CV_64F - 64-bit floating-point numbers ( -DBL_MAX..DBL_MAX, INF, NAN )
	 */
	depth(): int;
	/**
	 *   The method makes a new header for the specified matrix diagonal. The new matrix is represented as
	 * a single-column matrix. Similarly to [Mat::row] and [Mat::col], this is an O(1) operation.
	 *
	 * @param d index of the diagonal, with the following values:
	 *   d=0 is the main diagonal.d<0 is a diagonal from the lower half. For example, d=-1 means the
	 * diagonal is set immediately below the main one.d>0 is a diagonal from the upper half. For example,
	 * d=1 means the diagonal is set immediately above the main one. For example: Matm=(Mat_<int>(3,3)<<
	 *   1,2,3,
	 *   4,5,6,
	 *   7,8,9);
	 *   Matd0=m.diag(0);
	 *   Matd1=m.diag(1);
	 *   Matd_1=m.diag(-1);
	 *    The resulting matrices are d0=
	 *   [1;
	 *   5;
	 *   9]
	 *   d1=
	 *   [2;
	 *   6]
	 *   d_1=
	 *   [4;
	 *   8]
	 */
	diag(d?: int): Mat;
	/**
	 *   The method computes a dot-product of two matrices. If the matrices are not single-column or
	 * single-row vectors, the top-to-bottom left-to-right scan ordering is used to treat them as 1D
	 * vectors. The vectors must have the same size and type. If the matrices have more than one channel,
	 * the dot products from all the channels are summed together.
	 *
	 * @param m another dot-product operand.
	 */
	dot(m: Mat): Mat;
	/**
	 *   The method returns the matrix element size in bytes. For example, if the matrix type is CV_16SC3 ,
	 * the method returns 3*sizeof(short) or 6.
	 */
	elemSize(): size_t;
	/**
	 *   The method returns the matrix element channel size in bytes, that is, it ignores the number of
	 * channels. For example, if the matrix type is CV_16SC3 , the method returns sizeof(short) or 2.
	 */
	elemSize1(): size_t;
	/**
	 *   The method returns true if [Mat::total()] is 0 or if [Mat::data] is NULL. Because of [pop_back()]
	 * and [resize()] methods `[M.total()] == 0` does not imply that `M.data == NULL`.
	 */
	empty(): bool;
	/**
	 *   The methods return the matrix read-only or read-write iterators, set to the point following the
	 * last matrix element.
	 */
	end(arg27: any): MatIterator_;
	end(arg28: any): MatConstIterator_;
	/**
	 *   The operation passed as argument has to be a function pointer, a function object or a
	 * lambda(C++11).
	 *
	 *   Example 1. All of the operations below put 0xFF the first channel of all matrix elements:
	 *
	 *   ```cpp
	 *   Mat image(1920, 1080, CV_8UC3);
	 *   typedef cv::Point3_<uint8_t> Pixel;
	 *
	 *   // first. raw pointer access.
	 *   for (int r = 0; r < image.rows; ++r) {
	 *       Pixel* ptr = image.ptr<Pixel>(r, 0);
	 *       const Pixel* ptr_end = ptr + image.cols;
	 *       for (; ptr != ptr_end; ++ptr) {
	 *           ptr->x = 255;
	 *       }
	 *   }
	 *
	 *   // Using MatIterator. (Simple but there are a Iterator's overhead)
	 *   for (Pixel &p : cv::Mat_<Pixel>(image)) {
	 *       p.x = 255;
	 *   }
	 *
	 *   // Parallel execution with function object.
	 *   struct Operator {
	 *       void operator ()(Pixel &pixel, const int * position) {
	 *           pixel.x = 255;
	 *       }
	 *   };
	 *   image.forEach<Pixel>(Operator());
	 *
	 *   // Parallel execution using C++11 lambda.
	 *   image.forEach<Pixel>([](Pixel &p, const int * position) -> void {
	 *       p.x = 255;
	 *   });
	 *   ```
	 *
	 *    Example 2. Using the pixel's position:
	 *
	 *   ```cpp
	 *   // Creating 3D matrix (255 x 255 x 255) typed uint8_t
	 *   // and initialize all elements by the value which equals elements position.
	 *   // i.e. pixels (x,y,z) = (1,2,3) is (b,g,r) = (1,2,3).
	 *
	 *   int sizes[] = { 255, 255, 255 };
	 *   typedef cv::Point3_<uint8_t> Pixel;
	 *
	 *   Mat_<Pixel> image = Mat::zeros(3, sizes, CV_8UC3);
	 *
	 *   image.forEach<Pixel>([&](Pixel& pixel, const int position[]) -> void {
	 *       pixel.x = position[0];
	 *       pixel.y = position[1];
	 *       pixel.z = position[2];
	 *   });
	 *   ```
	 */
	forEach(arg29: any, arg30: any, operation: any): any;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	forEach(arg31: any, arg32: any, operation: any): any;
	getUMat(accessFlags: AccessFlag, usageFlags?: UMatUsageFlags): UMat;
	/**
	 *   The method performs a matrix inversion by means of matrix expressions. This means that a temporary
	 * matrix inversion object is returned by the method and can be used further as a part of more complex
	 * matrix expressions or can be assigned to a matrix.
	 *
	 * @param method Matrix inversion method. One of cv::DecompTypes
	 */
	inv(method?: int): MatExpr;
	/**
	 *   The method returns true if the matrix elements are stored continuously without gaps at the end of
	 * each row. Otherwise, it returns false. Obviously, 1x1 or 1xN matrices are always continuous.
	 * Matrices created with [Mat::create] are always continuous. But if you extract a part of the matrix
	 * using [Mat::col], [Mat::diag], and so on, or constructed a matrix header for externally allocated
	 * data, such matrices may no longer have this property.
	 *
	 *   The continuity flag is stored as a bit in the [Mat::flags] field and is computed automatically
	 * when you construct a matrix header. Thus, the continuity check is a very fast operation, though
	 * theoretically it could be done as follows:
	 *
	 *   ```cpp
	 *   // alternative implementation of Mat::isContinuous()
	 *   bool myCheckMatContinuity(const Mat& m)
	 *   {
	 *       //return (m.flags & Mat::CONTINUOUS_FLAG) != 0;
	 *       return m.rows == 1 || m.step == m.cols*m.elemSize();
	 *   }
	 *   ```
	 *
	 *    The method is used in quite a few of OpenCV functions. The point is that element-wise operations
	 * (such as arithmetic and logical operations, math functions, alpha blending, color space
	 * transformations, and others) do not depend on the image geometry. Thus, if all the input and output
	 * arrays are continuous, the functions can process them as very long single-row vectors. The example
	 * below illustrates how an alpha-blending function can be implemented:
	 *
	 *   ```cpp
	 *   template<typename T>
	 *   void alphaBlendRGBA(const Mat& src1, const Mat& src2, Mat& dst)
	 *   {
	 *       const float alpha_scale = (float)std::numeric_limits<T>::max(),
	 *                   inv_scale = 1.f/alpha_scale;
	 *
	 *       CV_Assert( src1.type() == src2.type() &&
	 *                  src1.type() == CV_MAKETYPE(traits::Depth<T>::value, 4) &&
	 *                  src1.size() == src2.size());
	 *       Size size = src1.size();
	 *       dst.create(size, src1.type());
	 *
	 *       // here is the idiom: check the arrays for continuity and,
	 *       // if this is the case,
	 *       // treat the arrays as 1D vectors
	 *       if( src1.isContinuous() && src2.isContinuous() && dst.isContinuous() )
	 *       {
	 *           size.width *= size.height;
	 *           size.height = 1;
	 *       }
	 *       size.width *= 4;
	 *
	 *       for( int i = 0; i < size.height; i++ )
	 *       {
	 *           // when the arrays are continuous,
	 *           // the outer loop is executed only once
	 *           const T* ptr1 = src1.ptr<T>(i);
	 *           const T* ptr2 = src2.ptr<T>(i);
	 *           T* dptr = dst.ptr<T>(i);
	 *
	 *           for( int j = 0; j < size.width; j += 4 )
	 *           {
	 *               float alpha = ptr1[j+3]*inv_scale, beta = ptr2[j+3]*inv_scale;
	 *               dptr[j] = saturate_cast<T>(ptr1[j]*alpha + ptr2[j]*beta);
	 *               dptr[j+1] = saturate_cast<T>(ptr1[j+1]*alpha + ptr2[j+1]*beta);
	 *               dptr[j+2] = saturate_cast<T>(ptr1[j+2]*alpha + ptr2[j+2]*beta);
	 *               dptr[j+3] = saturate_cast<T>((1 - (1-alpha)*(1-beta))*alpha_scale);
	 *           }
	 *       }
	 *   }
	 *   ```
	 *
	 *    This approach, while being very simple, can boost the performance of a simple element-operation
	 * by 10-20 percents, especially if the image is rather small and the operation is quite simple.
	 *
	 *   Another OpenCV idiom in this function, a call of [Mat::create] for the destination array, that
	 * allocates the destination array unless it already has the proper size and type. And while the newly
	 * allocated arrays are always continuous, you still need to check the destination array because
	 * [Mat::create] does not always allocate a new matrix.
	 */
	isContinuous(): bool;
	/**
	 *   After you extracted a submatrix from a matrix using [Mat::row], [Mat::col], [Mat::rowRange],
	 * [Mat::colRange], and others, the resultant submatrix points just to the part of the original big
	 * matrix. However, each submatrix contains information (represented by datastart and dataend fields)
	 * that helps reconstruct the original matrix size and the position of the extracted submatrix within
	 * the original matrix. The method locateROI does exactly that.
	 *
	 * @param wholeSize Output parameter that contains the size of the whole matrix containing this as a
	 * part.
	 *
	 * @param ofs Output parameter that contains an offset of this inside the whole matrix.
	 */
	locateROI(wholeSize: Size, ofs: Point): Size;
	/**
	 *   The method returns a temporary object encoding per-element array multiplication, with optional
	 * scale. Note that this is not a matrix multiplication that corresponds to a simpler "\\*" operator.
	 *
	 *   Example:
	 *
	 *   ```cpp
	 *   Mat C = A.mul(5/B); // equivalent to divide(A, B, C, 5)
	 *   ```
	 *
	 * @param m Another array of the same type and the same size as *this, or a matrix expression.
	 *
	 * @param scale Optional scale factor.
	 */
	mul(m: Mat, scale?: double): MatExpr;
	/**
	 *   The method removes one or more rows from the bottom of the matrix.
	 *
	 * @param nelems Number of removed rows. If it is greater than the total number of rows, an exception
	 * is thrown.
	 */
	pop_back(nelems?: size_t): void;
	/**
	 *   The methods return `uchar*` or typed pointer to the specified matrix row. See the sample in
	 * [Mat::isContinuous] to know how to use these methods.
	 *
	 * @param i0 A 0-based row index.
	 */
	ptr(i0?: int): uchar;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	ptr(i0?: int): uchar;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param row Index along the dimension 0
	 *
	 * @param col Index along the dimension 1
	 */
	ptr(row: int, col: int): uchar;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param row Index along the dimension 0
	 *
	 * @param col Index along the dimension 1
	 */
	ptr(row: int, col: int): uchar;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	ptr(i0: int, i1: int, i2: int): uchar;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	ptr(i0: int, i1: int, i2: int): uchar;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	ptr(idx: any): uchar;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	ptr(idx: any): uchar;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	ptr(n: int, idx: Vec): uchar;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	ptr(n: int, idx: Vec): uchar;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	ptr(arg37: any, i0?: int): any;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	ptr(arg38: any, i0?: int): any;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param row Index along the dimension 0
	 *
	 * @param col Index along the dimension 1
	 */
	ptr(arg39: any, row: int, col: int): any;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param row Index along the dimension 0
	 *
	 * @param col Index along the dimension 1
	 */
	ptr(arg40: any, row: int, col: int): any;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	ptr(arg41: any, i0: int, i1: int, i2: int): any;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	ptr(arg42: any, i0: int, i1: int, i2: int): any;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	ptr(arg43: any, idx: any): any;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	ptr(arg44: any, idx: any): any;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	ptr(arg45: any, n: int, idx: Vec): Vec;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	ptr(arg46: any, n: int, idx: Vec): Vec;
	/**
	 *   The methods add one or more elements to the bottom of the matrix. They emulate the corresponding
	 * method of the STL vector class. When elem is [Mat] , its type and the number of columns must be the
	 * same as in the container matrix.
	 *
	 * @param elem Added element(s).
	 */
	push_back(arg47: any, elem: any): any;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param elem Added element(s).
	 */
	push_back(arg48: any, elem: Mat): Mat;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param elem Added element(s).
	 */
	push_back(arg49: any, elem: any): any;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param m Added line(s).
	 */
	push_back(m: Mat): Mat;
	push_back_(elem: any): void;
	/**
	 *   The method decrements the reference counter associated with the matrix data. When the reference
	 * counter reaches 0, the matrix data is deallocated and the data and the reference counter pointers
	 * are set to NULL's. If the matrix header points to an external data set (see [Mat::Mat] ), the
	 * reference counter is NULL, and the method has no effect in this case.
	 *
	 *   This method can be called manually to force the matrix data deallocation. But since this method is
	 * automatically called in the destructor, or by any other method that changes the data pointer, it is
	 * usually not needed. The reference counter decrement and check for 0 is an atomic operation on the
	 * platforms that support it. Thus, it is safe to operate on the same matrices asynchronously in
	 * different threads.
	 */
	release(): void;
	/**
	 *   The method reserves space for sz rows. If the matrix already has enough space to store sz rows,
	 * nothing happens. If the matrix is reallocated, the first [Mat::rows] rows are preserved. The method
	 * emulates the corresponding method of the STL vector class.
	 *
	 * @param sz Number of rows.
	 */
	reserve(sz: size_t): void;
	/**
	 *   The method reserves space for sz bytes. If the matrix already has enough space to store sz bytes,
	 * nothing happens. If matrix has to be reallocated its previous content could be lost.
	 *
	 * @param sz Number of bytes.
	 */
	reserveBuffer(sz: size_t): void;
	/**
	 *   The method makes a new matrix header for *this elements. The new matrix may have a different size
	 * and/or different number of channels. Any combination is possible if:
	 *
	 * No extra elements are included into the new matrix and no elements are excluded. Consequently, the
	 * product rows*cols*channels() must stay the same after the transformation.
	 * No data is copied. That is, this is an O(1) operation. Consequently, if you change the number of
	 * rows, or the operation changes the indices of elements row in some other way, the matrix must be
	 * continuous. See [Mat::isContinuous] .
	 *
	 *   For example, if there is a set of 3D points stored as an STL vector, and you want to represent the
	 * points as a 3xN matrix, do the following:
	 *
	 *   ```cpp
	 *   std::vector<Point3f> vec;
	 *   ...
	 *   Mat pointMat = Mat(vec). // convert vector to Mat, O(1) operation
	 *                     reshape(1). // make Nx3 1-channel matrix out of Nx1 3-channel.
	 *                                 // Also, an O(1) operation
	 *                        t(); // finally, transpose the Nx3 matrix.
	 *                             // This involves copying all the elements
	 *   ```
	 *
	 * @param cn New number of channels. If the parameter is 0, the number of channels remains the same.
	 *
	 * @param rows New number of rows. If the parameter is 0, the number of rows remains the same.
	 */
	reshape(cn: int, rows?: int): Mat;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	reshape(cn: int, newndims: int, newsz: any): Mat;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 */
	reshape(cn: int, newshape: any): Mat;
	/**
	 *   The methods change the number of matrix rows. If the matrix is reallocated, the first
	 * min(Mat::rows, sz) rows are preserved. The methods emulate the corresponding methods of the STL
	 * vector class.
	 *
	 * @param sz New number of rows.
	 */
	resize(sz: size_t): void;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param sz New number of rows.
	 *
	 * @param s Value assigned to the newly added elements.
	 */
	resize(sz: size_t, s: Scalar): Scalar;
	/**
	 *   The method makes a new header for the specified matrix row and returns it. This is an O(1)
	 * operation, regardless of the matrix size. The underlying data of the new matrix is shared with the
	 * original matrix. Here is the example of one of the classical basic matrix processing operations,
	 * axpy, used by LU and many other algorithms:
	 *
	 *   ```cpp
	 *   inline void matrix_axpy(Mat& A, int i, int j, double alpha)
	 *   {
	 *       A.row(i) += A.row(j)*alpha;
	 *   }
	 *   ```
	 *
	 *   In the current implementation, the following code does not work as expected:
	 *
	 *   ```cpp
	 *   Mat A;
	 *   ...
	 *   A.row(i) = A.row(j); // will not work
	 *   ```
	 *
	 *    This happens because A.row(i) forms a temporary header that is further assigned to another
	 * header. Remember that each of these operations is O(1), that is, no data is copied. Thus, the above
	 * assignment is not true if you may have expected the j-th row to be copied to the i-th row. To
	 * achieve that, you should either turn this simple assignment into an expression or use the
	 * [Mat::copyTo] method:
	 *
	 *   ```cpp
	 *   Mat A;
	 *   ...
	 *   // works, but looks a bit obscure.
	 *   A.row(i) = A.row(j) + 0;
	 *   // this is a bit longer, but the recommended method.
	 *   A.row(j).copyTo(A.row(i));
	 *   ```
	 *
	 * @param y A 0-based row index.
	 */
	row(y: int): Mat;
	/**
	 *   The method makes a new header for the specified row span of the matrix. Similarly to [Mat::row]
	 * and [Mat::col] , this is an O(1) operation.
	 *
	 * @param startrow An inclusive 0-based start index of the row span.
	 *
	 * @param endrow An exclusive 0-based ending index of the row span.
	 */
	rowRange(startrow: int, endrow: int): Mat;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param r Range structure containing both the start and the end indices.
	 */
	rowRange(r: Range): Mat;
	/**
	 *   This is an advanced variant of the [Mat::operator=(const Scalar& s)] operator.
	 *
	 * @param value Assigned scalar converted to the actual array type.
	 *
	 * @param mask Operation mask of the same size as *this. Its non-zero elements indicate which matrix
	 * elements need to be copied. The mask has to be of type CV_8U and can have 1 or multiple channels
	 */
	setTo(value: Mat | Scalar, mask?: Mat): Mat;
	/**
	 *   The method returns a matrix step divided by [Mat::elemSize1()] . It can be useful to quickly
	 * access an arbitrary matrix element.
	 */
	step1(i?: int): size_t;
	/**
	 *   The method performs matrix transposition by means of matrix expressions. It does not perform the
	 * actual transposition but returns a temporary matrix transposition object that can be further used as
	 * a part of more complex matrix expressions or can be assigned to a matrix:
	 *
	 *   ```cpp
	 *   Mat A1 = A + Mat::eye(A.size(), A.type())*lambda;
	 *   Mat C = A1.t()*A1; // compute (A + lambda*I)^t * (A + lamda*I)
	 *   ```
	 */
	t(): MatExpr;
	/**
	 *   The method returns the number of array elements (a number of pixels if the array represents an
	 * image).
	 */
	total(): size_t;
	/**
	 *   The method returns the number of elements within a certain sub-array slice with startDim <= dim <
	 * endDim
	 */
	total(startDim: int, endDim?: int): size_t;
	/**
	 *   The method returns a matrix element type. This is an identifier compatible with the CvMat type
	 * system, like CV_16SC3 or 16-bit signed 3-channel array, and so on.
	 */
	type(): int;
	updateContinuityFlag(): void;
	ucharPtr(i: any, j: any): any;
	charPtr(i: any, j: any): any;
	shortPtr(i: any, j: any): any;
	ushortPtr(i: any, j: any): any;
	intPtr(i: any, j: any): any;
	floatPtr(i: any, j: any): any;
	doublePtr(i: any, j: any): any;
	intPtr(i: any, j: any): any;
	charAt(i0: number): number;
	charAt(i0: number, i1: number): number;
	charAt(i0: number, i1: number, i2: number): number;
	ucharAt(i0: number): number;
	ucharAt(i0: number, i1: number): number;
	ucharAt(i0: number, i1: number, i2: number): number;
	shortAt(i0: number): number;
	shortAt(i0: number, i1: number): number;
	shortAt(i0: number, i1: number, i2: number): number;
	ushortAt(i0: number): number;
	ushortAt(i0: number, i1: number): number;
	ushortAt(i0: number, i1: number, i2: number): number;
	intAt(i0: number): number;
	intAt(i0: number, i1: number): number;
	intAt(i0: number, i1: number, i2: number): number;
	floatAt(i0: number): number;
	floatAt(i0: number, i1: number): number;
	floatAt(i0: number, i1: number, i2: number): number;
	doubleAt(i0: number): number;
	doubleAt(i0: number, i1: number): number;
	doubleAt(i0: number, i1: number, i2: number): number;
	setTo(value: Mat | Scalar, mask?: Mat): Mat;
	/**
	 * Sometimes, you will have to play with certain region of images.
	 * For eye detection in images, first face detection is done all
	 * over the image and when face is obtained, we select the face region alone and search for eyes inside it instead of searching whole image.
	 * It improves accuracy (because eyes are always on faces) and performance (because we search for a small area).
	 *
	 * Heads up : in JS seems only one argument is expected.
	 */
	roi(expr: Rect | Mat): Mat;
	/**
	 *   The method creates a square diagonal matrix from specified main diagonal.
	 *
	 * @param d One-dimensional matrix that represents the main diagonal.
	 */
	static diag(d: Mat): Mat;
	/**
	 *   The method returns a Matlab-style identity matrix initializer, similarly to [Mat::zeros].
	 * Similarly to [Mat::ones], you can use a scale operation to create a scaled identity matrix
	 * efficiently:
	 *
	 *   ```cpp
	 *   // make a 4x4 diagonal matrix with 0.1's on the diagonal.
	 *   Mat A = Mat::eye(4, 4, CV_32F)*0.1;
	 *   ```
	 *
	 *   In case of multi-channels type, identity matrix will be initialized only for the first channel,
	 * the others will be set to 0's
	 *
	 * @param rows Number of rows.
	 *
	 * @param cols Number of columns.
	 *
	 * @param type Created matrix type.
	 */
	static eye(rows: int, cols: int, type: int): MatExpr;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param size Alternative matrix size specification as Size(cols, rows) .
	 *
	 * @param type Created matrix type.
	 */
	static eye(size: Size, type: int): MatExpr;
	static getDefaultAllocator(): MatAllocator;
	static getStdAllocator(): MatAllocator;
	/**
	 *   The method returns a Matlab-style 1's array initializer, similarly to [Mat::zeros]. Note that
	 * using this method you can initialize an array with an arbitrary value, using the following Matlab
	 * idiom:
	 *
	 *   ```cpp
	 *   Mat A = Mat::ones(100, 100, CV_8U)*3; // make 100x100 matrix filled with 3.
	 *   ```
	 *
	 *    The above operation does not form a 100x100 matrix of 1's and then multiply it by 3. Instead, it
	 * just remembers the scale factor (3 in this case) and use it when actually invoking the matrix
	 * initializer.
	 *
	 *   In case of multi-channels type, only the first channel will be initialized with 1's, the others
	 * will be set to 0's.
	 *
	 * @param rows Number of rows.
	 *
	 * @param cols Number of columns.
	 *
	 * @param type Created matrix type.
	 */
	static ones(rows: int, cols: int, type: int): MatExpr;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param size Alternative to the matrix size specification Size(cols, rows) .
	 *
	 * @param type Created matrix type.
	 */
	static ones(size: Size, type: int): MatExpr;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param ndims Array dimensionality.
	 *
	 * @param sz Array of integers specifying the array shape.
	 *
	 * @param type Created matrix type.
	 */
	static ones(ndims: int, sz: any, type: int): MatExpr;
	static setDefaultAllocator(allocator: MatAllocator): MatAllocator;
	/**
	 *   The method returns a Matlab-style zero array initializer. It can be used to quickly form a
	 * constant array as a function parameter, part of a matrix expression, or as a matrix initializer:
	 *
	 *   ```cpp
	 *   Mat A;
	 *   A = Mat::zeros(3, 3, CV_32F);
	 *   ```
	 *
	 *    In the example above, a new matrix is allocated only if A is not a 3x3 floating-point matrix.
	 * Otherwise, the existing matrix A is filled with zeros.
	 *
	 * @param rows Number of rows.
	 *
	 * @param cols Number of columns.
	 *
	 * @param type Created matrix type.
	 */
	static zeros(rows: int, cols: int, type: int): MatExpr;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param size Alternative to the matrix size specification Size(cols, rows) .
	 *
	 * @param type Created matrix type.
	 */
	static zeros(size: Size, type: int): MatExpr;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param ndims Array dimensionality.
	 *
	 * @param sz Array of integers specifying the array shape.
	 *
	 * @param type Created matrix type.
	 */
	static zeros(ndims: int, sz: any, type: int): MatExpr;
}
export declare const MAGIC_VAL: any;
export declare const AUTO_STEP: any;
export declare const CONTINUOUS_FLAG: any;
export declare const SUBMATRIX_FLAG: any;
export declare const MAGIC_MASK: any;
export declare const TYPE_MASK: any;
export declare const DEPTH_MASK: any;
/**
 * <a name="d1/d10/classcv_1_1MatExpr_1MatrixExpressions"></a>This is a list of implemented matrix
 * operations that can be combined in arbitrary complex expressions (here A, B stand for matrices (
 * [Mat](#d3/d63/classcv_1_1Mat}) ), s for a scalar ( Scalar ), alpha for a real-valued scalar ( double
 * )):
 *
 * Addition, subtraction, negation: `A+B`, `A-B`, `A+s`, `A-s`, `s+A`, `s-A`, `-A`
 * Scaling: `A*alpha`
 * Per-element multiplication and division: `A.mul(B)`, `A/B`, `alpha/A`
 * Matrix multiplication: `A*B`
 * Transposition: `A.t()` (means A)
 * Matrix inversion and pseudo-inversion, solving linear systems and least-squares problems:
 * `A.inv([method]) (~ A<sup>-1</sup>)`, `A.inv([method])*B (~ X: AX=B)`
 * Comparison: `A cmpop B`, `A cmpop alpha`, `alpha cmpop A`, where *cmpop* is one of `>`, `>=`, `==`,
 * `!=`, `<=`, `<`. The result of comparison is an 8-bit single channel mask whose elements are set to
 * 255 (if the particular element or pair of elements satisfy the condition) or 0.
 * Bitwise logical operations: `A logicop B`, `A logicop s`, `s logicop A`, `~A`, where *logicop* is
 * one of `&`, `|`, `^`.
 * Element-wise minimum and maximum: `min(A, B)`, `min(A, alpha)`, `max(A, B)`, `max(A, alpha)`
 * Element-wise absolute value: `abs(A)`
 * Cross-product, dot-product: `A.cross(B)`, `A.dot(B)`
 * Any function of matrix or matrices and scalars that returns a matrix or a scalar, such as norm,
 * mean, sum, countNonZero, trace, determinant, repeat, and others.
 * Matrix initializers ( [Mat::eye()](#d3/d63/classcv_1_1Mat_1a2cf9b9acde7a9852542bbc20ef851ed2}),
 * [Mat::zeros()](#d3/d63/classcv_1_1Mat_1a0b57b6a326c8876d944d188a46e0f556}),
 * [Mat::ones()](#d3/d63/classcv_1_1Mat_1a69ae0402d116fc9c71908d8508dc2f09}) ), matrix comma-separated
 * initializers, matrix constructors and operators that extract sub-matrices (see
 * [Mat](#d3/d63/classcv_1_1Mat}) description).
 * Mat_<destination_type>() constructors to cast the result to the proper type.
 *
 * Comma-separated initializers and probably some other operations may require additional explicit
 * Mat() or Mat_<T>() constructor calls to resolve a possible ambiguity.
 * Here are examples of matrix expressions:
 *
 * ```cpp
 * // compute pseudo-inverse of A, equivalent to A.inv(DECOMP_SVD)
 * SVD svd(A);
 * Mat pinvA = svd.vt.t()*Mat::diag(1./svd.w)*svd.u.t();
 *
 * // compute the new vector of parameters in the Levenberg-Marquardt algorithm
 * x -= (A.t()*A + lambda*Mat::eye(A.cols,A.cols,A.type())).inv(DECOMP_CHOLESKY)*(A.t()*err);
 *
 * // sharpen image using "unsharp mask" algorithm
 * Mat blurred; double sigma = 1, threshold = 5, amount = 1;
 * GaussianBlur(img, blurred, Size(), sigma, sigma);
 * Mat lowContrastMask = abs(img - blurred) < threshold;
 * Mat sharpened = img*(1+amount) + blurred*(-amount);
 * img.copyTo(sharpened, lowContrastMask);
 * ```
 *
 * Source:
 * [opencv2/core/mat.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/mat.hpp#L3557).
 *
 */
export declare class MatExpr extends Mat {
	a: Mat;
	alpha: double;
	b: Mat;
	beta: double;
	c: Mat;
	flags: int;
	op: MatOp;
	s: Scalar;
	constructor();
	constructor(m: Mat);
	constructor(_op: MatOp, _flags: int, _a?: Mat, _b?: Mat, _c?: Mat, _alpha?: double, _beta?: double, _s?: Scalar);
	col(x: int): MatExpr;
	cross(m: Mat): Mat;
	diag(d?: int): MatExpr;
	dot(m: Mat): Mat;
	inv(method?: int): MatExpr;
	mul(e: MatExpr, scale?: double): MatExpr;
	mul(m: Mat, scale?: double): MatExpr;
	row(y: int): MatExpr;
	t(): MatExpr;
	type(): int;
}
export declare class MatOp {
	constructor();
	abs(expr: MatExpr, res: MatExpr): MatExpr;
	add(expr1: MatExpr, expr2: MatExpr, res: MatExpr): MatExpr;
	add(expr1: MatExpr, s: Scalar, res: MatExpr): MatExpr;
	assign(expr: MatExpr, m: Mat, type?: int): MatExpr;
	augAssignAdd(expr: MatExpr, m: Mat): MatExpr;
	augAssignAnd(expr: MatExpr, m: Mat): MatExpr;
	augAssignDivide(expr: MatExpr, m: Mat): MatExpr;
	augAssignMultiply(expr: MatExpr, m: Mat): MatExpr;
	augAssignOr(expr: MatExpr, m: Mat): MatExpr;
	augAssignSubtract(expr: MatExpr, m: Mat): MatExpr;
	augAssignXor(expr: MatExpr, m: Mat): MatExpr;
	diag(expr: MatExpr, d: int, res: MatExpr): MatExpr;
	divide(expr1: MatExpr, expr2: MatExpr, res: MatExpr, scale?: double): MatExpr;
	divide(s: double, expr: MatExpr, res: MatExpr): MatExpr;
	elementWise(expr: MatExpr): MatExpr;
	invert(expr: MatExpr, method: int, res: MatExpr): MatExpr;
	matmul(expr1: MatExpr, expr2: MatExpr, res: MatExpr): MatExpr;
	multiply(expr1: MatExpr, expr2: MatExpr, res: MatExpr, scale?: double): MatExpr;
	multiply(expr1: MatExpr, s: double, res: MatExpr): MatExpr;
	roi(expr: MatExpr, rowRange: Range, colRange: Range, res: MatExpr): MatExpr;
	size(expr: MatExpr): Size;
	subtract(expr1: MatExpr, expr2: MatExpr, res: MatExpr): MatExpr;
	subtract(s: Scalar, expr: MatExpr, res: MatExpr): Scalar;
	transpose(expr: MatExpr, res: MatExpr): MatExpr;
	type(expr: MatExpr): MatExpr;
}
/**
 * If you need a more flexible type, use [Mat](#d3/d63/classcv_1_1Mat}) . The elements of the matrix M
 * are accessible using the M(i,j) notation. Most of the common matrix operations (see also
 * [MatrixExpressions](#d1/d10/classcv_1_1MatExpr_1MatrixExpressions}) ) are available. To do an
 * operation on [Matx](#de/de1/classcv_1_1Matx}) that is not implemented, you can easily convert the
 * matrix to [Mat](#d3/d63/classcv_1_1Mat}) and backwards:
 *
 * ```cpp
 * Matx33f m(1, 2, 3,
 *           4, 5, 6,
 *           7, 8, 9);
 * cout << sum(Mat(m*m.t())) << endl;
 * ```
 *
 *  Except of the plain constructor which takes a list of elements, [Matx](#de/de1/classcv_1_1Matx})
 * can be initialized from a C-array:
 *
 * ```cpp
 * float values[] = { 1, 2, 3};
 * Matx31f m(values);
 * ```
 *
 *  In case if C++11 features are available, std::initializer_list can be also used to initialize
 * [Matx](#de/de1/classcv_1_1Matx}):
 *
 * ```cpp
 * Matx31f m = { 1, 2, 3};
 * ```
 *
 * Source:
 * [opencv2/core/matx.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/matx.hpp#L1185).
 *
 */
export declare class Matx {
	val: _Tp;
	constructor();
	constructor(v0: _Tp);
	constructor(v0: _Tp, v1: _Tp);
	constructor(v0: _Tp, v1: _Tp, v2: _Tp);
	constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp);
	constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp);
	constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp);
	constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp);
	constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp);
	constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp, v8: _Tp);
	constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp, v8: _Tp, v9: _Tp);
	constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp, v8: _Tp, v9: _Tp, v10: _Tp, v11: _Tp);
	constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp, v8: _Tp, v9: _Tp, v10: _Tp, v11: _Tp, v12: _Tp, v13: _Tp);
	constructor(v0: _Tp, v1: _Tp, v2: _Tp, v3: _Tp, v4: _Tp, v5: _Tp, v6: _Tp, v7: _Tp, v8: _Tp, v9: _Tp, v10: _Tp, v11: _Tp, v12: _Tp, v13: _Tp, v14: _Tp, v15: _Tp);
	constructor(vals: any);
	constructor(arg334: any);
	constructor(a: Matx, b: Matx, arg335: Matx_AddOp);
	constructor(a: Matx, b: Matx, arg336: Matx_SubOp);
	constructor(arg337: any, a: Matx, alpha: _T2, arg338: Matx_ScaleOp);
	constructor(a: Matx, b: Matx, arg339: Matx_MulOp);
	constructor(a: Matx, b: Matx, arg340: Matx_DivOp);
	constructor(l: int, a: Matx, b: Matx, arg341: Matx_MatMulOp);
	constructor(a: Matx, arg342: Matx_TOp);
	col(i: int): Matx;
	ddot(v: Matx): Matx;
	diag(): diag_type;
	div(a: Matx): Matx;
	dot(v: Matx): Matx;
	get_minor(m1: int, n1: int, base_row: int, base_col: int): Matx;
	inv(method?: int, p_is_ok?: any): Matx;
	mul(a: Matx): Matx;
	reshape(m1: int, n1: int): Matx;
	row(i: int): Matx;
	solve(l: int, rhs: Matx, flags?: int): Matx;
	solve(rhs: Vec, method: int): Vec;
	t(): Matx;
	static all(alpha: _Tp): Matx;
	static diag(d: diag_type): Matx;
	static eye(): Matx;
	static ones(): Matx;
	static randn(a: _Tp, b: _Tp): Matx;
	static randu(a: _Tp, b: _Tp): Matx;
	static zeros(): Matx;
}
export declare const rows: any;
export declare const cols: any;
export declare const channels: any;
export declare const shortdim: any;
declare class Node$1 {
	/**
	 *   Class index normalized to 0..class_count-1 range and assigned to the node. It is used internally
	 * in classification trees and tree ensembles.
	 *
	 */
	classIdx: int;
	/**
	 *   Default direction where to go (-1: left or +1: right). It helps in the case of missing values.
	 *
	 */
	defaultDir: int;
	left: int;
	parent: int;
	right: int;
	split: int;
	/**
	 *   Value at the node: a class label in case of classification or estimated function value in case of
	 * regression.
	 *
	 */
	value: double;
	constructor();
}
export declare function createFaceDetectionMaskGenerator(): any;
/**
 * The function is a wrapper for the generic function partition . It clusters all the input rectangles
 * using the rectangle equivalence criteria that combines rectangles with similar sizes and similar
 * locations. The similarity is defined by eps. When eps=0 , no clustering is done at all. If
 * `$\\texttt{eps}\\rightarrow +\\inf$` , all the rectangles are put in one cluster. Then, the small
 * clusters containing less than or equal to groupThreshold rectangles are rejected. In each other
 * cluster, the average rectangle is computed and put into the output rectangle list.
 *
 * @param rectList Input/output vector of rectangles. Output vector includes retained and grouped
 * rectangles. (The Python list is not modified in place.)
 *
 * @param groupThreshold Minimum possible number of rectangles minus 1. The threshold is used in a
 * group of rectangles to retain it.
 *
 * @param eps Relative difference between sides of the rectangles to merge them into a group.
 */
export declare function groupRectangles(rectList: any, groupThreshold: int, eps?: double): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function groupRectangles(rectList: any, weights: any, groupThreshold: int, eps?: double): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function groupRectangles(rectList: any, groupThreshold: int, eps: double, weights: any, levelWeights: any): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function groupRectangles(rectList: any, rejectLevels: any, levelWeights: any, groupThreshold: int, eps?: double): void;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function groupRectangles_meanshift(rectList: any, foundWeights: any, foundScales: any, detectThreshold?: double, winDetSize?: Size): void;
export declare const CASCADE_DO_CANNY_PRUNING: any;
export declare const CASCADE_SCALE_IMAGE: any;
export declare const CASCADE_FIND_BIGGEST_OBJECT: any;
export declare const CASCADE_DO_ROUGH_SEARCH: any;
/**
 * https://docs.opencv.org/4.10.0/db/d95/classcv_1_1ORB.html
 */
export declare class ORB extends Feature2D {
	constructor(nfeatures?: int, scaleFactor?: float, nlevels?: int, edgeThreshold?: int, firstLevel?: int, WTA_K?: int, scoreType?: ORBScoreType, patchSize?: int, fastThreshold?: int);
}
export type ORBScoreType = int;
export declare const ORB_HARRIS_SCORE: ORBScoreType;
export declare const ORB_FAST_SCORE: ORBScoreType;
/**
 * The class is used to calculate a special basis for a set of vectors. The basis will consist of
 * eigenvectors of the covariance matrix calculated from the input set of vectors. The class PCA can
 * also transform vectors to/from the new coordinate space defined by the basis. Usually, in this new
 * coordinate system, each vector from the original set (and any linear combination of such vectors)
 * can be quite accurately approximated by taking its first few components, corresponding to the
 * eigenvectors of the largest eigenvalues of the covariance matrix. Geometrically it means that you
 * calculate a projection of the vector to a subspace formed by a few eigenvectors corresponding to the
 * dominant eigenvalues of the covariance matrix. And usually such a projection is very close to the
 * original vector. So, you can represent the original vector from a high-dimensional space with a much
 * shorter vector consisting of the projected vector's coordinates in the subspace. Such a
 * transformation is also known as Karhunen-Loeve Transform, or KLT. See
 *
 * The sample below is the function that takes two matrices. The first function stores a set of vectors
 * (a row per vector) that is used to calculate [PCA](#d3/d8d/classcv_1_1PCA}). The second function
 * stores another "test" set of vectors (a row per vector). First, these vectors are compressed with
 * [PCA](#d3/d8d/classcv_1_1PCA}), then reconstructed back, and then the reconstruction error norm is
 * computed and printed for each vector. :
 *
 * ```cpp
 * using namespace cv;
 *
 * PCA compressPCA(const Mat& pcaset, int maxComponents,
 *                 const Mat& testset, Mat& compressed)
 * {
 *     PCA pca(pcaset, // pass the data
 *             Mat(), // we do not have a pre-computed mean vector,
 *                    // so let the PCA engine to compute it
 *             PCA::DATA_AS_ROW, // indicate that the vectors
 *                                 // are stored as matrix rows
 *                                 // (use PCA::DATA_AS_COL if the vectors are
 *                                 // the matrix columns)
 *             maxComponents // specify, how many principal components to retain
 *             );
 *     // if there is no test data, just return the computed basis, ready-to-use
 *     if( !testset.data )
 *         return pca;
 *     CV_Assert( testset.cols == pcaset.cols );
 *
 *     compressed.create(testset.rows, maxComponents, testset.type());
 *
 *     Mat reconstructed;
 *     for( int i = 0; i < testset.rows; i++ )
 *     {
 *         Mat vec = testset.row(i), coeffs = compressed.row(i), reconstructed;
 *         // compress the vector, the result will be stored
 *         // in the i-th row of the output matrix
 *         pca.project(vec, coeffs);
 *         // and then reconstruct it
 *         pca.backProject(coeffs, reconstructed);
 *         // and measure the error
 *         printf("%d. diff = %g\\n", i, norm(vec, reconstructed, NORM_L2));
 *     }
 *     return pca;
 * }
 * ```
 *
 * [calcCovarMatrix](#d2/de8/group__core__array_1gae6ffa9354633f984246945d52823165d}),
 * [mulTransposed](#d2/de8/group__core__array_1gadc4e49f8f7a155044e3be1b9e3b270ab}),
 * [SVD](#df/df7/classcv_1_1SVD}),
 * [dft](#d2/de8/group__core__array_1gadd6cf9baf2b8b704a11b5f04aaf4f39d}),
 * [dct](#d2/de8/group__core__array_1ga85aad4d668c01fbd64825f589e3696d4})
 *
 * Source:
 * [opencv2/core.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core.hpp#L2393).
 *
 */
export declare class PCA {
	eigenvalues: Mat;
	eigenvectors: Mat;
	mean: Mat;
	/**
	 *   The default constructor initializes an empty PCA structure. The other constructors initialize the
	 * structure and call [PCA::operator()()].
	 */
	constructor();
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param data input samples stored as matrix rows or matrix columns.
	 *
	 * @param mean optional mean value; if the matrix is empty (noArray()), the mean is computed from the
	 * data.
	 *
	 * @param flags operation flags; currently the parameter is only used to specify the data layout
	 * (PCA::Flags)
	 *
	 * @param maxComponents maximum number of components that PCA should retain; by default, all the
	 * components are retained.
	 */
	constructor(data: Mat, mean: Mat, flags: int, maxComponents?: int);
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param data input samples stored as matrix rows or matrix columns.
	 *
	 * @param mean optional mean value; if the matrix is empty (noArray()), the mean is computed from the
	 * data.
	 *
	 * @param flags operation flags; currently the parameter is only used to specify the data layout
	 * (PCA::Flags)
	 *
	 * @param retainedVariance Percentage of variance that PCA should retain. Using this parameter will
	 * let the PCA decided how many components to retain but it will always keep at least 2.
	 */
	constructor(data: Mat, mean: Mat, flags: int, retainedVariance: double);
	/**
	 *   The methods are inverse operations to [PCA::project]. They take PC coordinates of projected
	 * vectors and reconstruct the original vectors. Unless all the principal components have been
	 * retained, the reconstructed vectors are different from the originals. But typically, the difference
	 * is small if the number of components is large enough (but still much smaller than the original
	 * vector dimensionality). As a result, [PCA] is used.
	 *
	 * @param vec coordinates of the vectors in the principal component subspace, the layout and size are
	 * the same as of PCA::project output vectors.
	 */
	backProject(vec: Mat): Mat;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param vec coordinates of the vectors in the principal component subspace, the layout and size are
	 * the same as of PCA::project output vectors.
	 *
	 * @param result reconstructed vectors; the layout and size are the same as of PCA::project input
	 * vectors.
	 */
	backProject(vec: Mat, result: Mat): Mat;
	/**
	 *   The methods project one or more vectors to the principal component subspace, where each vector
	 * projection is represented by coefficients in the principal component basis. The first form of the
	 * method returns the matrix that the second form writes to the result. So the first form can be used
	 * as a part of expression while the second form can be more efficient in a processing loop.
	 *
	 * @param vec input vector(s); must have the same dimensionality and the same layout as the input
	 * data used at PCA phase, that is, if DATA_AS_ROW are specified, then vec.cols==data.cols (vector
	 * dimensionality) and vec.rows is the number of vectors to project, and the same is true for the
	 * PCA::DATA_AS_COL case.
	 */
	project(vec: Mat): Mat;
	/**
	 *   This is an overloaded member function, provided for convenience. It differs from the above
	 * function only in what argument(s) it accepts.
	 *
	 * @param vec input vector(s); must have the same dimensionality and the same layout as the input
	 * data used at PCA phase, that is, if DATA_AS_ROW are specified, then vec.cols==data.cols (vector
	 * dimensionality) and vec.rows is the number of vectors to project, and the same is true for the
	 * PCA::DATA_AS_COL case.
	 *
	 * @param result output vectors; in case of PCA::DATA_AS_COL, the output matrix has as many columns
	 * as the number of input vectors, this means that result.cols==vec.cols and the number of rows match
	 * the number of principal components (for example, maxComponents parameter passed to the constructor).
	 */
	project(vec: Mat, result: Mat): Mat;
	/**
	 *   Loads [eigenvalues] [eigenvectors] and [mean] from specified [FileNode]
	 */
	read(fn: FileNode): FileNode;
	/**
	 *   Writes [eigenvalues] [eigenvectors] and [mean] to specified [FileStorage]
	 */
	write(fs: FileStorage): FileStorage;
}
export declare const DATA_AS_ROW: Flags;
export declare const DATA_AS_COL: Flags;
export declare const USE_AVG: Flags;
export type Flags = any;
/**
 * The function reconstructs the selected image area from the pixel near the area boundary. The
 * function may be used to remove dust and scratches from a scanned photo, or to remove undesirable
 * objects from still images or video. See  for more details.
 *
 * An example using the inpainting technique can be found at opencv_source_code/samples/cpp/inpaint.cpp
 * (Python) An example using the inpainting technique can be found at
 * opencv_source_code/samples/python/inpaint.py
 *
 * @param src Input 8-bit, 16-bit unsigned or 32-bit float 1-channel or 8-bit 3-channel image.
 *
 * @param inpaintMask Inpainting mask, 8-bit 1-channel image. Non-zero pixels indicate the area that
 * needs to be inpainted.
 *
 * @param dst Output image with the same size and type as src .
 *
 * @param inpaintRadius Radius of a circular neighborhood of each point inpainted that is considered by
 * the algorithm.
 *
 * @param flags Inpainting method that could be cv::INPAINT_NS or cv::INPAINT_TELEA
 */
export declare function inpaint(src: Mat, inpaintMask: Mat, dst: Mat, inpaintRadius: double, flags: int): void;
export declare const INPAINT_NS: any;
export declare const INPAINT_TELEA: any;
/**
 * Each rectangle is specified by the center point (mass center), length of each side (represented by
 * [Size2f](#dc/d84/group__core__basic_1gab34496d2466b5f69930ab74c70f117d4}) structure) and the
 * rotation angle in degrees.
 *
 * The sample below demonstrates how to use [RotatedRect](#db/dd6/classcv_1_1RotatedRect}):
 *
 * ```cpp
 *     Mat test_image(200, 200, CV_8UC3, Scalar(0));
 *     RotatedRect rRect = RotatedRect(Point2f(100,100), Size2f(100,50), 30);
 *
 *     Point2f vertices[4];
 *     rRect.points(vertices);
 *     for (int i = 0; i < 4; i++)
 *         line(test_image, vertices[i], vertices[(i+1)%4], Scalar(0,255,0), 2);
 *
 *     Rect brect = rRect.boundingRect();
 *     rectangle(test_image, brect, Scalar(255,0,0), 2);
 *
 *     imshow("rectangles", test_image);
 *     waitKey(0);
 * ```
 *
 * [CamShift](#dc/d6b/group__video__track_1gaef2bd39c8356f423124f1fe7c44d54a1}),
 * [fitEllipse](#d3/dc0/group__imgproc__shape_1gaf259efaad93098103d6c27b9e4900ffa}),
 * [minAreaRect](#d3/dc0/group__imgproc__shape_1ga3d476a3417130ae5154aea421ca7ead9}), CvBox2D
 *
 * Source:
 * [opencv2/core/types.hpp](https://github.com/opencv/opencv/tree/master/modules/core/include/opencv2/core/types.hpp#L534).
 *
 */
export declare class RotatedRect {
	angle: float;
	center: Point;
	size: Size;
	constructor();
	/**
	 *   full constructor
	 *
	 * @param center The rectangle mass center.
	 *
	 * @param size Width and height of the rectangle.
	 *
	 * @param angle The rotation angle in a clockwise direction. When the angle is 0, 90, 180, 270 etc.,
	 * the rectangle becomes an up-right rectangle.
	 */
	constructor(center: Point, size: Size, angle: float);
	/**
	 *   Any 3 end points of the [RotatedRect]. They must be given in order (either clockwise or
	 * anticlockwise).
	 */
	constructor(point1: Point, point2: Point, point3: Point);
	boundingRect(): Rect;
	boundingRect2f(): Rect;
	/**
	 *   returns 4 vertices of the rectangle
	 *
	 * @param pts The points array for storing rectangle vertices. The order is bottomLeft, topLeft,
	 * topRight, bottomRight.
	 */
	points(pts: Point): Point;
}
export declare class softdouble {
	v: uint64_t;
	constructor();
	constructor(c: softdouble);
	constructor(arg159: uint32_t);
	constructor(arg160: uint64_t);
	constructor(arg161: int32_t);
	constructor(arg162: int64_t);
	constructor(a: any);
	getExp(): int;
	/**
	 *   Returns a number 1 <= x < 2 with the same significand
	 */
	getFrac(): softdouble;
	getSign(): bool;
	isInf(): bool;
	isNaN(): bool;
	isSubnormal(): bool;
	setExp(e: int): softdouble;
	/**
	 *   Constructs a copy of a number with significand taken from parameter
	 */
	setFrac(s: softdouble): softdouble;
	setSign(sign: bool): softdouble;
	static eps(): softdouble;
	/**
	 *   Builds new value from raw binary representation
	 */
	static fromRaw(a: uint64_t): softdouble;
	static inf(): softdouble;
	static max(): softdouble;
	static min(): softdouble;
	static nan(): softdouble;
	static one(): softdouble;
	static pi(): softdouble;
	static zero(): softdouble;
}
export declare class softfloat {
	v: uint32_t;
	constructor();
	constructor(c: softfloat);
	constructor(arg174: uint32_t);
	constructor(arg175: uint64_t);
	constructor(arg176: int32_t);
	constructor(arg177: int64_t);
	constructor(a: any);
	getExp(): int;
	/**
	 *   Returns a number 1 <= x < 2 with the same significand
	 */
	getFrac(): softfloat;
	getSign(): bool;
	isInf(): bool;
	isNaN(): bool;
	isSubnormal(): bool;
	setExp(e: int): softfloat;
	/**
	 *   Constructs a copy of a number with significand taken from parameter
	 */
	setFrac(s: softfloat): softfloat;
	setSign(sign: bool): softfloat;
	static eps(): softfloat;
	/**
	 *   Builds new value from raw binary representation
	 */
	static fromRaw(a: uint32_t): softfloat;
	static inf(): softfloat;
	static max(): softfloat;
	static min(): softfloat;
	static nan(): softfloat;
	static one(): softfloat;
	static pi(): softfloat;
	static zero(): softfloat;
}
/**
 * number of levels in constructed pyramid. Can be less than maxLevel.
 *
 * @param img 8-bit input image.
 *
 * @param pyramid output pyramid.
 *
 * @param winSize window size of optical flow algorithm. Must be not less than winSize argument of
 * calcOpticalFlowPyrLK. It is needed to calculate required padding for pyramid levels.
 *
 * @param maxLevel 0-based maximal pyramid level number.
 *
 * @param withDerivatives set to precompute gradients for the every pyramid level. If pyramid is
 * constructed without the gradients then calcOpticalFlowPyrLK will calculate them internally.
 *
 * @param pyrBorder the border mode for pyramid layers.
 *
 * @param derivBorder the border mode for gradients.
 *
 * @param tryReuseInputImage put ROI of input image into the pyramid if possible. You can pass false to
 * force data copying.
 */
export declare function buildOpticalFlowPyramid(img: Mat, pyramid: MatVector, winSize: Size, maxLevel: int, withDerivatives?: bool, pyrBorder?: int, derivBorder?: int, tryReuseInputImage?: bool): int;
/**
 * The function finds an optical flow for each prev pixel using the Farneback2003 algorithm so that
 *
 * `\\[\\texttt{prev} (y,x) \\sim \\texttt{next} ( y + \\texttt{flow} (y,x)[1], x + \\texttt{flow}
 * (y,x)[0])\\]`
 *
 * An example using the optical flow algorithm described by Gunnar Farneback can be found at
 * opencv_source_code/samples/cpp/fback.cpp
 * (Python) An example using the optical flow algorithm described by Gunnar Farneback can be found at
 * opencv_source_code/samples/python/opt_flow.py
 *
 * @param prev first 8-bit single-channel input image.
 *
 * @param next second input image of the same size and the same type as prev.
 *
 * @param flow computed flow image that has the same size as prev and type CV_32FC2.
 *
 * @param pyr_scale parameter, specifying the image scale (<1) to build pyramids for each image;
 * pyr_scale=0.5 means a classical pyramid, where each next layer is twice smaller than the previous
 * one.
 *
 * @param levels number of pyramid layers including the initial image; levels=1 means that no extra
 * layers are created and only the original images are used.
 *
 * @param winsize averaging window size; larger values increase the algorithm robustness to image noise
 * and give more chances for fast motion detection, but yield more blurred motion field.
 *
 * @param iterations number of iterations the algorithm does at each pyramid level.
 *
 * @param poly_n size of the pixel neighborhood used to find polynomial expansion in each pixel; larger
 * values mean that the image will be approximated with smoother surfaces, yielding more robust
 * algorithm and more blurred motion field, typically poly_n =5 or 7.
 *
 * @param poly_sigma standard deviation of the Gaussian that is used to smooth derivatives used as a
 * basis for the polynomial expansion; for poly_n=5, you can set poly_sigma=1.1, for poly_n=7, a good
 * value would be poly_sigma=1.5.
 *
 * @param flags operation flags that can be a combination of the following:
 * OPTFLOW_USE_INITIAL_FLOW uses the input flow as an initial flow
 * approximation.OPTFLOW_FARNEBACK_GAUSSIAN uses the Gaussian $\texttt{winsize}\times\texttt{winsize}$
 * filter instead of a box filter of the same size for optical flow estimation; usually, this option
 * gives z more accurate flow than with a box filter, at the cost of lower speed; normally, winsize for
 * a Gaussian window should be set to a larger value to achieve the same level of robustness.
 */
export declare function calcOpticalFlowFarneback(prev: Mat, next: Mat, flow: Mat, pyr_scale: double, levels: int, winsize: int, iterations: int, poly_n: int, poly_sigma: double, flags: int): void;
/**
 * The function implements a sparse iterative version of the Lucas-Kanade optical flow in pyramids. See
 * Bouguet00 . The function is parallelized with the TBB library.
 *
 * An example using the Lucas-Kanade optical flow algorithm can be found at
 * opencv_source_code/samples/cpp/lkdemo.cpp
 * (Python) An example using the Lucas-Kanade optical flow algorithm can be found at
 * opencv_source_code/samples/python/lk_track.py
 * (Python) An example using the Lucas-Kanade tracker for homography matching can be found at
 * opencv_source_code/samples/python/lk_homography.py
 *
 * @param prevImg first 8-bit input image or pyramid constructed by buildOpticalFlowPyramid.
 *
 * @param nextImg second input image or pyramid of the same size and the same type as prevImg.
 *
 * @param prevPts vector of 2D points for which the flow needs to be found; point coordinates must be
 * single-precision floating-point numbers.
 *
 * @param nextPts output vector of 2D points (with single-precision floating-point coordinates)
 * containing the calculated new positions of input features in the second image; when
 * OPTFLOW_USE_INITIAL_FLOW flag is passed, the vector must have the same size as in the input.
 *
 * @param status output status vector (of unsigned chars); each element of the vector is set to 1 if
 * the flow for the corresponding features has been found, otherwise, it is set to 0.
 *
 * @param err output vector of errors; each element of the vector is set to an error for the
 * corresponding feature, type of the error measure can be set in flags parameter; if the flow wasn't
 * found then the error is not defined (use the status parameter to find such cases).
 *
 * @param winSize size of the search window at each pyramid level.
 *
 * @param maxLevel 0-based maximal pyramid level number; if set to 0, pyramids are not used (single
 * level), if set to 1, two levels are used, and so on; if pyramids are passed to input then algorithm
 * will use as many levels as pyramids have but no more than maxLevel.
 *
 * @param criteria parameter, specifying the termination criteria of the iterative search algorithm
 * (after the specified maximum number of iterations criteria.maxCount or when the search window moves
 * by less than criteria.epsilon.
 *
 * @param flags operation flags:
 * OPTFLOW_USE_INITIAL_FLOW uses initial estimations, stored in nextPts; if the flag is not set, then
 * prevPts is copied to nextPts and is considered the initial estimate.OPTFLOW_LK_GET_MIN_EIGENVALS use
 * minimum eigen values as an error measure (see minEigThreshold description); if the flag is not set,
 * then L1 distance between patches around the original and a moved point, divided by number of pixels
 * in a window, is used as a error measure.
 *
 * @param minEigThreshold the algorithm calculates the minimum eigen value of a 2x2 normal matrix of
 * optical flow equations (this matrix is called a spatial gradient matrix in Bouguet00), divided by
 * number of pixels in a window; if this value is less than minEigThreshold, then a corresponding
 * feature is filtered out and its flow is not processed, so it allows to remove bad points and get a
 * performance boost.
 */
export declare function calcOpticalFlowPyrLK(prevImg: Mat, nextImg: Mat, prevPts: Mat, nextPts: Mat, status: Mat, err: Mat, winSize?: Size, maxLevel?: int, criteria?: TermCriteria, flags?: int, minEigThreshold?: double): void;
/**
 * See the OpenCV sample camshiftdemo.c that tracks colored objects.
 *
 * (Python) A sample explaining the camshift tracking algorithm can be found at
 * opencv_source_code/samples/python/camshift.py
 *
 * @param probImage Back projection of the object histogram. See calcBackProject.
 *
 * @param window Initial search window.
 *
 * @param criteria Stop criteria for the underlying meanShift. returns (in old interfaces) Number of
 * iterations CAMSHIFT took to converge The function implements the CAMSHIFT object tracking algorithm
 * Bradski98 . First, it finds an object center using meanShift and then adjusts the window size and
 * finds the optimal rotation. The function returns the rotated rectangle structure that includes the
 * object position, size, and orientation. The next position of the search window can be obtained with
 * RotatedRect::boundingRect()
 */
export declare function CamShift(probImage: Mat, window: any, criteria: TermCriteria): RotatedRect;
/**
 * [findTransformECC]
 *
 * @param templateImage single-channel template image; CV_8U or CV_32F array.
 *
 * @param inputImage single-channel input image to be warped to provide an image similar to
 * templateImage, same type as templateImage.
 *
 * @param inputMask An optional mask to indicate valid values of inputImage.
 */
export declare function computeECC(templateImage: Mat, inputImage: Mat, inputMask?: Mat): double;
/**
 * The function finds an optimal affine transform *[A|b]* (a 2 x 3 floating-point matrix) that
 * approximates best the affine transformation between:  In case of point sets, the problem is
 * formulated as follows: you need to find a 2x2 matrix *A* and 2x1 vector *b* so that:
 *
 * `\\[[A^*|b^*] = arg \\min _{[A|b]} \\sum _i \\| \\texttt{dst}[i] - A { \\texttt{src}[i]}^T - b \\|
 * ^2\\]` where src[i] and dst[i] are the i-th points in src and dst, respectively `$[A|b]$` can be
 * either arbitrary (when fullAffine=true ) or have a form of `\\[\\begin{bmatrix} a_{11} & a_{12} &
 * b_1 \\\\ -a_{12} & a_{11} & b_2 \\end{bmatrix}\\]` when fullAffine=false.
 *
 * [estimateAffine2D], [estimateAffinePartial2D], [getAffineTransform], [getPerspectiveTransform],
 * [findHomography]
 *
 * @param src First input 2D point set stored in std::vector or Mat, or an image stored in Mat.
 *
 * @param dst Second input 2D point set of the same size and the same type as A, or another image.
 *
 * @param fullAffine If true, the function finds an optimal affine transformation with no additional
 * restrictions (6 degrees of freedom). Otherwise, the class of transformations to choose from is
 * limited to combinations of translation, rotation, and uniform scaling (4 degrees of freedom).
 */
export declare function estimateRigidTransform(src: Mat, dst: Mat, fullAffine: bool): Mat;
/**
 * The function estimates the optimum transformation (warpMatrix) with respect to ECC criterion (EP08),
 * that is
 *
 * `\\[\\texttt{warpMatrix} = \\texttt{warpMatrix} = \\arg\\max_{W}
 * \\texttt{ECC}(\\texttt{templateImage}(x,y),\\texttt{inputImage}(x',y'))\\]`
 *
 * where
 *
 * `\\[\\begin{bmatrix} x' \\\\ y' \\end{bmatrix} = W \\cdot \\begin{bmatrix} x \\\\ y \\\\ 1
 * \\end{bmatrix}\\]`
 *
 * (the equation holds with homogeneous coordinates for homography). It returns the final enhanced
 * correlation coefficient, that is the correlation coefficient between the template image and the
 * final warped input image. When a `$3\\times 3$` matrix is given with motionType =0, 1 or 2, the
 * third row is ignored.
 *
 * Unlike findHomography and estimateRigidTransform, the function findTransformECC implements an
 * area-based alignment that builds on intensity similarities. In essence, the function updates the
 * initial transformation that roughly aligns the images. If this information is missing, the identity
 * warp (unity matrix) is used as an initialization. Note that if images undergo strong
 * displacements/rotations, an initial transformation that roughly aligns the images is necessary
 * (e.g., a simple euclidean/similarity transform that allows for the images showing the same image
 * content approximately). Use inverse warping in the second image to take an image close to the first
 * one, i.e. use the flag WARP_INVERSE_MAP with warpAffine or warpPerspective. See also the OpenCV
 * sample image_alignment.cpp that demonstrates the use of the function. Note that the function throws
 * an exception if algorithm does not converges.
 *
 * [computeECC], [estimateAffine2D], [estimateAffinePartial2D], [findHomography]
 *
 * @param templateImage single-channel template image; CV_8U or CV_32F array.
 *
 * @param inputImage single-channel input image which should be warped with the final warpMatrix in
 * order to provide an image similar to templateImage, same type as templateImage.
 *
 * @param warpMatrix floating-point $2\times 3$ or $3\times 3$ mapping matrix (warp).
 *
 * @param motionType parameter, specifying the type of motion:
 * MOTION_TRANSLATION sets a translational motion model; warpMatrix is $2\times 3$ with the first
 * $2\times 2$ part being the unity matrix and the rest two parameters being estimated.MOTION_EUCLIDEAN
 * sets a Euclidean (rigid) transformation as motion model; three parameters are estimated; warpMatrix
 * is $2\times 3$.MOTION_AFFINE sets an affine motion model (DEFAULT); six parameters are estimated;
 * warpMatrix is $2\times 3$.MOTION_HOMOGRAPHY sets a homography as a motion model; eight parameters
 * are estimated;`warpMatrix` is $3\times 3$.
 *
 * @param criteria parameter, specifying the termination criteria of the ECC algorithm;
 * criteria.epsilon defines the threshold of the increment in the correlation coefficient between two
 * iterations (a negative criteria.epsilon makes criteria.maxcount the only termination criterion).
 * Default values are shown in the declaration above.
 *
 * @param inputMask An optional mask to indicate valid values of inputImage.
 *
 * @param gaussFiltSize An optional value indicating size of gaussian blur filter; (DEFAULT: 5)
 */
export declare function findTransformECC(templateImage: Mat, inputImage: Mat, warpMatrix: Mat, motionType: int, criteria: TermCriteria, inputMask: Mat, gaussFiltSize: int): double;
/**
 * This is an overloaded member function, provided for convenience. It differs from the above function
 * only in what argument(s) it accepts.
 */
export declare function findTransformECC(templateImage: Mat, inputImage: Mat, warpMatrix: Mat, motionType?: int, criteria?: TermCriteria, inputMask?: Mat): double;
/**
 * @param probImage Back projection of the object histogram. See calcBackProject for details.
 *
 * @param window Initial search window.
 *
 * @param criteria Stop criteria for the iterative search algorithm. returns : Number of iterations
 * CAMSHIFT took to converge. The function implements the iterative object search algorithm. It takes
 * the input back projection of an object and the initial position. The mass center in window of the
 * back projection image is computed and the search window center shifts to the mass center. The
 * procedure is repeated until the specified number of iterations criteria.maxCount is done or until
 * the window center shifts by less than criteria.epsilon. The algorithm is used inside CamShift and,
 * unlike CamShift , the search window size or orientation do not change during the search. You can
 * simply pass the output of calcBackProject to this function. But better results can be obtained if
 * you pre-filter the back projection and remove the noise. For example, you can do this by retrieving
 * connected components with findContours , throwing away contours with small area ( contourArea ), and
 * rendering the remaining contours with drawContours.
 */
export declare function meanShift(probImage: Mat, window: any, criteria: TermCriteria): int;
/**
 * The function readOpticalFlow loads a flow field from a file and returns it as a single matrix.
 * Resulting [Mat] has a type CV_32FC2 - floating-point, 2-channel. First channel corresponds to the
 * flow in the horizontal direction (u), second - vertical (v).
 *
 * @param path Path to the file to be loaded
 */
export declare function readOpticalFlow(path: any): Mat;
/**
 * The function stores a flow field in a file, returns true on success, false otherwise. The flow field
 * must be a 2-channel, floating-point matrix (CV_32FC2). First channel corresponds to the flow in the
 * horizontal direction (u), second - vertical (v).
 *
 * @param path Path to the file to be written
 *
 * @param flow Flow field to be stored
 */
export declare function writeOpticalFlow(path: any, flow: Mat): bool;
export declare const OPTFLOW_USE_INITIAL_FLOW: any;
export declare const OPTFLOW_LK_GET_MIN_EIGENVALS: any;
export declare const OPTFLOW_FARNEBACK_GAUSSIAN: any;
export declare const MOTION_TRANSLATION: any;
export declare const MOTION_EUCLIDEAN: any;
export declare const MOTION_AFFINE: any;
export declare const MOTION_HOMOGRAPHY: any;
declare class Range$1 {
	start: number;
	end: number;
	constructor(start: number, end: number);
}
export declare class Scalar extends Array<number> {
	static all(...v: number[]): Scalar;
}
export declare class Point {
	constructor(x: number, y: number);
	x: number;
	y: number;
}
export declare class Circle {
	constructor(center: Point, radius: number);
	center: Point;
	radius: number;
}
export declare class Size {
	constructor(width: number, height: number);
	width: number;
	height: number;
}
export declare class Rect {
	constructor();
	constructor(point: Point, size: Size);
	constructor(x: number, y: number, width: number, height: number);
	x: number;
	y: number;
	width: number;
	height: number;
}
export declare class TermCriteria {
	type: number;
	maxCount: number;
	epsilon: number;
	constructor();
	constructor(type: number, maxCount: number, epsilon: number);
}
export declare const TermCriteria_EPS: any;
export declare const TermCriteria_COUNT: any;
export declare const TermCriteria_MAX_ITER: any;
export declare class MinMaxLoc {
	minVal: number;
	maxVal: number;
	minLoc: Point;
	maxLoc: Point;
	constructor();
	constructor(minVal: number, maxVal: number, minLoc: Point, maxLoc: Point);
}
export declare function exceptionFromPtr(err: number): any;
export declare function onRuntimeInitialized(): any;
export declare function FS_createDataFile(arg0: string, path: string, data: Uint8Array, arg3: boolean, arg4: boolean, arg5: boolean): any;
/**
 * Base class for Contrast Limited Adaptive Histogram Equalization.
 */
export declare class CLAHE extends Algorithm$1 {
	/**
	 * @param clipLimit Threshold for contrast limiting. Default.  40.0,
	 * @param totalGridSize Size of grid for histogram equalization. Input image will be divided into equally sized rectangular tiles. tileGridSize defines the number of tiles in row and column. Default: Size(8, 8)
	 */
	constructor(clipLimit?: double, totalGridSize?: Size);
	/**
	 * Equalizes the histogram of a grayscale image using Contrast Limited Adaptive Histogram Equalization.
	 * @param src Source image of type CV_8UC1 or CV_16UC1.
	 * @param dst Destination image.
	 */
	apply(src: Mat, dst: Mat): void;
	collectGarbage(): void;
	/**
	 * Returns threshold value for contrast limiting.
	 */
	getClipLimit(): double;
	/**
	 * Returns Size defines the number of tiles in row and column.
	 */
	getTilesGridSize(): Size;
	/**
	 * Sets threshold for contrast limiting.
	 */
	setClipLimit(clipLimit: double): void;
	/**
	 * Sets size of grid for histogram equalization. Input image will be divided into equally sized rectangular tiles.
	 * @param tileGridSize defines the number of tiles in row and column.
	 */
	setTilesGridSize(tileGridSize: Size): void;
}
export declare function getInheritedInstanceCount(...a: any[]): any;
export declare function getLiveInheritedInstances(...a: any[]): any;
export declare function flushPendingDeletes(...a: any[]): any;
export declare function setDelayFunction(...a: any[]): any;
export declare class EmscriptenEmbindInstance {
	isAliasOf(other: any): bool;
	clone(): any;
	delete(): any;
	isDeleted(): boolean;
	deleteLater(): any;
}
export declare class InternalError extends Error {
}
export declare class BindingError extends Error {
}
export declare class UnboundTypeError extends Error {
}
export declare class PureVirtualError extends Error {
}
export declare class Vector<T> extends EmscriptenEmbindInstance {
	get(i: number): T;
	get(i: number, j: number, data: any): T;
	set(i: number, t: T): void;
	put(i: number, j: number, data: any): any;
	size(): number;
	push_back(n: T): any;
	resize(count: number, value?: T): void;
}
export declare class Vec3d extends Vector<any> {
}
export declare class IntVector extends Vector<number> {
}
export declare class FloatVector extends Vector<number> {
}
export declare class DoubleVector extends Vector<number> {
}
export declare class PointVector extends Vector<Point> {
}
export declare class KeyPointVector extends Vector<any> {
}
export declare class DMatchVector extends Vector<any> {
}
export declare class DMatchVectorVector extends Vector<Vector<any>> {
}
export declare class MatVector extends Vector<Mat> {
}
export declare class RectVector extends Rect implements Vector<Rect> {
	get(i: number): Rect;
	isAliasOf(...a: any[]): any;
	clone(...a: any[]): any;
	delete(...a: any[]): any;
	isDeleted(...a: any[]): any;
	deleteLater(...a: any[]): any;
	set(i: number, t: Rect): void;
	put(i: number, j: number, data: any): any;
	size(): number;
	push_back(n: Rect): void;
	resize(count: number, value?: Rect | undefined): void;
	delete(): void;
}
export declare class VideoCapture {
	constructor(videoSource: HTMLVideoElement | string);
	read(m: Mat): any;
	video: HTMLVideoElement;
}
export type MatSize = () => Size;
export declare function matFromImageData(imageData: ImageData$1): Mat;
export declare function matFromArray(rows: number, cols: number, type: any, array: number[] | ArrayBufferView): Mat;
declare class ImageData$1 {
	data: ArrayBufferView;
	width: number;
	height: number;
}
export declare const CV_8U: CVDataType;
export declare const CV_8UC1: CVDataType;
export declare const CV_8UC2: CVDataType;
export declare const CV_8UC3: CVDataType;
export declare const CV_8UC4: CVDataType;
export declare const CV_8S: CVDataType;
export declare const CV_8SC1: CVDataType;
export declare const CV_8SC2: CVDataType;
export declare const CV_8SC3: CVDataType;
export declare const CV_8SC4: CVDataType;
export declare const CV_16U: CVDataType;
export declare const CV_16UC1: CVDataType;
export declare const CV_16UC2: CVDataType;
export declare const CV_16UC3: CVDataType;
export declare const CV_16UC4: CVDataType;
export declare const CV_16S: CVDataType;
export declare const CV_16SC1: CVDataType;
export declare const CV_16SC2: CVDataType;
export declare const CV_16SC3: CVDataType;
export declare const CV_16SC4: CVDataType;
export declare const CV_32S: CVDataType;
export declare const CV_32SC1: CVDataType;
export declare const CV_32SC2: CVDataType;
export declare const CV_32SC3: CVDataType;
export declare const CV_32SC4: CVDataType;
export declare const CV_32F: CVDataType;
export declare const CV_32FC1: CVDataType;
export declare const CV_32FC2: CVDataType;
export declare const CV_32FC3: CVDataType;
export declare const CV_32FC4: CVDataType;
export declare const CV_64F: CVDataType;
export declare const CV_64FC1: CVDataType;
export declare const CV_64FC2: CVDataType;
export declare const CV_64FC3: CVDataType;
export declare const CV_64FC4: CVDataType;
export type CVDataType = any;
export declare function ellipse1(dst: Mat, rotatedRect: RotatedRect, ellipseColor: Scalar, arg0: number, line: LineTypes): void;
export declare function imread(canvasOrImageHtmlElement: HTMLElement | string): Mat;
export declare function norm1(a: Mat, b: Mat, type: NormTypes): number;
export declare function imshow(canvasSource: HTMLElement | string, mat: Mat): void;
export declare function matFromArray(rows: number, cols: number, type: any, array: any): Mat;
export type Mat4 = any;
export type Mat3 = any;
export type Vec3 = any;
export type float_type = any;
export type int = number;
export type bool = boolean;
export type FileNode = any;
export type FileStorage = any;
export type Ptr = any;
export type size_t = any;
export type double = number;
export type float = number;
export type UMat = any;
export type Matrix = any;
export type BucketKey = any;
export type Bucket = any;
export type LshStats = any;
export type MatAllocator = any;
export type uchar = any;
export type MatStep = any;
export type UMatData = any;
export type typename = any;
export type Vec = any;
export type Point_ = any;
export type Point3_ = any;
export type MatCommaInitializer_ = any;
export type MatIterator_ = any;
export type MatConstIterator_ = any;
export type AccessFlag = any;
export type UMatUsageFlags = any;
export type _Tp = any;
export type Matx_AddOp = any;
export type Matx_SubOp = any;
export type _T2 = any;
export type Matx_ScaleOp = any;
export type Matx_MulOp = any;
export type Matx_DivOp = any;
export type Matx_MatMulOp = any;
export type Matx_TOp = any;
export type diag_type = any;
export type _EqPredicate = any;
export type cvhalDFT = any;
export type schar = any;
export type ushort = any;
export type short = any;
export type int64 = any;
type ErrorCallback$1 = any;
export type unsigned = any;
export type uint64 = any;
export type float16_t = any;
export type AsyncArray = any;
export type Net = any;
export type Moments = any;
export type uint64_t = any;
export type uint32_t = any;
export type int32_t = any;
export type int64_t = any;
export declare class Tracker {
}
export declare class TrackerMIL extends Tracker {
}
// export type CV = typeof _cv;

declare namespace _cv {
	export { ADAPTIVE_THRESH_GAUSSIAN_C, ADAPTIVE_THRESH_MEAN_C, AUTO_STEP, AccessFlag, AdaptiveThresholdTypes, Affine3, Algorithm$1 as Algorithm, AsyncArray, AutoBuffer, BFMatcher, BORDER_CONSTANT, BORDER_DEFAULT, BORDER_ISOLATED, BORDER_REFLECT, BORDER_REFLECT101, BORDER_REFLECT_101, BORDER_REPLICATE, BORDER_TRANSPARENT, BORDER_WRAP, BOWTrainer, BRUTEFORCE, BRUTEFORCE_HAMMING, BRUTEFORCE_HAMMINGLUT, BRUTEFORCE_L1, BRUTEFORCE_SL2, Backend, BindingError, BorderTypes, Bucket, BucketKey, CALIB_CB_ACCURACY, CALIB_CB_ADAPTIVE_THRESH, CALIB_CB_ASYMMETRIC_GRID, CALIB_CB_CLUSTERING, CALIB_CB_EXHAUSTIVE, CALIB_CB_FAST_CHECK, CALIB_CB_FILTER_QUADS, CALIB_CB_NORMALIZE_IMAGE, CALIB_CB_SYMMETRIC_GRID, CALIB_FIX_ASPECT_RATIO, CALIB_FIX_FOCAL_LENGTH, CALIB_FIX_INTRINSIC, CALIB_FIX_K1, CALIB_FIX_K2, CALIB_FIX_K3, CALIB_FIX_K4, CALIB_FIX_K5, CALIB_FIX_K6, CALIB_FIX_PRINCIPAL_POINT, CALIB_FIX_S1_S2_S3_S4, CALIB_FIX_TANGENT_DIST, CALIB_FIX_TAUX_TAUY, CALIB_HAND_EYE_ANDREFF, CALIB_HAND_EYE_DANIILIDIS, CALIB_HAND_EYE_HORAUD, CALIB_HAND_EYE_PARK, CALIB_HAND_EYE_TSAI, CALIB_NINTRINSIC, CALIB_RATIONAL_MODEL, CALIB_SAME_FOCAL_LENGTH, CALIB_THIN_PRISM_MODEL, CALIB_TILTED_MODEL, CALIB_USE_EXTRINSIC_GUESS, CALIB_USE_INTRINSIC_GUESS, CALIB_USE_LU, CALIB_USE_QR, CALIB_ZERO_DISPARITY, CALIB_ZERO_TANGENT_DIST, CASCADE_DO_CANNY_PRUNING, CASCADE_DO_ROUGH_SEARCH, CASCADE_FIND_BIGGEST_OBJECT, CASCADE_SCALE_IMAGE, CCL_DEFAULT, CCL_GRANA, CCL_WU, CC_STAT_AREA, CC_STAT_HEIGHT, CC_STAT_LEFT, CC_STAT_MAX, CC_STAT_TOP, CC_STAT_WIDTH, CHAIN_APPROX_NONE, CHAIN_APPROX_SIMPLE, CHAIN_APPROX_TC89_KCOS, CHAIN_APPROX_TC89_L1, CLAHE, CMP_EQ, CMP_GE, CMP_GT, CMP_LE, CMP_LT, CMP_NE, COLOR_BGR2BGR555, COLOR_BGR2BGR565, COLOR_BGR2BGRA, COLOR_BGR2GRAY, COLOR_BGR2HLS, COLOR_BGR2HLS_FULL, COLOR_BGR2HSV, COLOR_BGR2HSV_FULL, COLOR_BGR2Lab, COLOR_BGR2Luv, COLOR_BGR2RGB, COLOR_BGR2RGBA, COLOR_BGR2XYZ, COLOR_BGR2YCrCb, COLOR_BGR2YUV, COLOR_BGR2YUV_I420, COLOR_BGR2YUV_IYUV, COLOR_BGR2YUV_YV12, COLOR_BGR5552BGR, COLOR_BGR5552BGRA, COLOR_BGR5552GRAY, COLOR_BGR5552RGB, COLOR_BGR5552RGBA, COLOR_BGR5652BGR, COLOR_BGR5652BGRA, COLOR_BGR5652GRAY, COLOR_BGR5652RGB, COLOR_BGR5652RGBA, COLOR_BGRA2BGR, COLOR_BGRA2BGR555, COLOR_BGRA2BGR565, COLOR_BGRA2GRAY, COLOR_BGRA2RGB, COLOR_BGRA2RGBA, COLOR_BGRA2YUV_I420, COLOR_BGRA2YUV_IYUV, COLOR_BGRA2YUV_YV12, COLOR_BayerBG2BGR, COLOR_BayerBG2BGRA, COLOR_BayerBG2BGR_EA, COLOR_BayerBG2BGR_VNG, COLOR_BayerBG2GRAY, COLOR_BayerBG2RGB, COLOR_BayerBG2RGBA, COLOR_BayerBG2RGB_EA, COLOR_BayerBG2RGB_VNG, COLOR_BayerGB2BGR, COLOR_BayerGB2BGRA, COLOR_BayerGB2BGR_EA, COLOR_BayerGB2BGR_VNG, COLOR_BayerGB2GRAY, COLOR_BayerGB2RGB, COLOR_BayerGB2RGBA, COLOR_BayerGB2RGB_EA, COLOR_BayerGB2RGB_VNG, COLOR_BayerGR2BGR, COLOR_BayerGR2BGRA, COLOR_BayerGR2BGR_EA, COLOR_BayerGR2BGR_VNG, COLOR_BayerGR2GRAY, COLOR_BayerGR2RGB, COLOR_BayerGR2RGBA, COLOR_BayerGR2RGB_EA, COLOR_BayerGR2RGB_VNG, COLOR_BayerRG2BGR, COLOR_BayerRG2BGRA, COLOR_BayerRG2BGR_EA, COLOR_BayerRG2BGR_VNG, COLOR_BayerRG2GRAY, COLOR_BayerRG2RGB, COLOR_BayerRG2RGBA, COLOR_BayerRG2RGB_EA, COLOR_BayerRG2RGB_VNG, COLOR_COLORCVT_MAX, COLOR_GRAY2BGR, COLOR_GRAY2BGR555, COLOR_GRAY2BGR565, COLOR_GRAY2BGRA, COLOR_GRAY2RGB, COLOR_GRAY2RGBA, COLOR_HLS2BGR, COLOR_HLS2BGR_FULL, COLOR_HLS2RGB, COLOR_HLS2RGB_FULL, COLOR_HSV2BGR, COLOR_HSV2BGR_FULL, COLOR_HSV2RGB, COLOR_HSV2RGB_FULL, COLOR_LBGR2Lab, COLOR_LBGR2Luv, COLOR_LRGB2Lab, COLOR_LRGB2Luv, COLOR_Lab2BGR, COLOR_Lab2LBGR, COLOR_Lab2LRGB, COLOR_Lab2RGB, COLOR_Luv2BGR, COLOR_Luv2LBGR, COLOR_Luv2LRGB, COLOR_Luv2RGB, COLOR_RGB2BGR, COLOR_RGB2BGR555, COLOR_RGB2BGR565, COLOR_RGB2BGRA, COLOR_RGB2GRAY, COLOR_RGB2HLS, COLOR_RGB2HLS_FULL, COLOR_RGB2HSV, COLOR_RGB2HSV_FULL, COLOR_RGB2Lab, COLOR_RGB2Luv, COLOR_RGB2RGBA, COLOR_RGB2XYZ, COLOR_RGB2YCrCb, COLOR_RGB2YUV, COLOR_RGB2YUV_I420, COLOR_RGB2YUV_IYUV, COLOR_RGB2YUV_YV12, COLOR_RGBA2BGR, COLOR_RGBA2BGR555, COLOR_RGBA2BGR565, COLOR_RGBA2BGRA, COLOR_RGBA2GRAY, COLOR_RGBA2RGB, COLOR_RGBA2YUV_I420, COLOR_RGBA2YUV_IYUV, COLOR_RGBA2YUV_YV12, COLOR_RGBA2mRGBA, COLOR_XYZ2BGR, COLOR_XYZ2RGB, COLOR_YCrCb2BGR, COLOR_YCrCb2RGB, COLOR_YUV2BGR, COLOR_YUV2BGRA_I420, COLOR_YUV2BGRA_IYUV, COLOR_YUV2BGRA_NV12, COLOR_YUV2BGRA_NV21, COLOR_YUV2BGRA_UYNV, COLOR_YUV2BGRA_UYVY, COLOR_YUV2BGRA_Y422, COLOR_YUV2BGRA_YUNV, COLOR_YUV2BGRA_YUY2, COLOR_YUV2BGRA_YUYV, COLOR_YUV2BGRA_YV12, COLOR_YUV2BGRA_YVYU, COLOR_YUV2BGR_I420, COLOR_YUV2BGR_IYUV, COLOR_YUV2BGR_NV12, COLOR_YUV2BGR_NV21, COLOR_YUV2BGR_UYNV, COLOR_YUV2BGR_UYVY, COLOR_YUV2BGR_Y422, COLOR_YUV2BGR_YUNV, COLOR_YUV2BGR_YUY2, COLOR_YUV2BGR_YUYV, COLOR_YUV2BGR_YV12, COLOR_YUV2BGR_YVYU, COLOR_YUV2GRAY_420, COLOR_YUV2GRAY_I420, COLOR_YUV2GRAY_IYUV, COLOR_YUV2GRAY_NV12, COLOR_YUV2GRAY_NV21, COLOR_YUV2GRAY_UYNV, COLOR_YUV2GRAY_UYVY, COLOR_YUV2GRAY_Y422, COLOR_YUV2GRAY_YUNV, COLOR_YUV2GRAY_YUY2, COLOR_YUV2GRAY_YUYV, COLOR_YUV2GRAY_YV12, COLOR_YUV2GRAY_YVYU, COLOR_YUV2RGB, COLOR_YUV2RGBA_I420, COLOR_YUV2RGBA_IYUV, COLOR_YUV2RGBA_NV12, COLOR_YUV2RGBA_NV21, COLOR_YUV2RGBA_UYNV, COLOR_YUV2RGBA_UYVY, COLOR_YUV2RGBA_Y422, COLOR_YUV2RGBA_YUNV, COLOR_YUV2RGBA_YUY2, COLOR_YUV2RGBA_YUYV, COLOR_YUV2RGBA_YV12, COLOR_YUV2RGBA_YVYU, COLOR_YUV2RGB_I420, COLOR_YUV2RGB_IYUV, COLOR_YUV2RGB_NV12, COLOR_YUV2RGB_NV21, COLOR_YUV2RGB_UYNV, COLOR_YUV2RGB_UYVY, COLOR_YUV2RGB_Y422, COLOR_YUV2RGB_YUNV, COLOR_YUV2RGB_YUY2, COLOR_YUV2RGB_YUYV, COLOR_YUV2RGB_YV12, COLOR_YUV2RGB_YVYU, COLOR_YUV420p2BGR, COLOR_YUV420p2BGRA, COLOR_YUV420p2GRAY, COLOR_YUV420p2RGB, COLOR_YUV420p2RGBA, COLOR_YUV420sp2BGR, COLOR_YUV420sp2BGRA, COLOR_YUV420sp2GRAY, COLOR_YUV420sp2RGB, COLOR_YUV420sp2RGBA, COLOR_mRGBA2RGBA, CONTINUOUS_FLAG, CONTOURS_MATCH_I1, CONTOURS_MATCH_I2, CONTOURS_MATCH_I3, CPU_AVX, CPU_AVX2, CPU_AVX512_CEL, CPU_AVX512_CNL, CPU_AVX512_COMMON, CPU_AVX512_ICL, CPU_AVX512_KNL, CPU_AVX512_KNM, CPU_AVX512_SKX, CPU_AVX_5124FMAPS, CPU_AVX_5124VNNIW, CPU_AVX_512BITALG, CPU_AVX_512BW, CPU_AVX_512CD, CPU_AVX_512DQ, CPU_AVX_512ER, CPU_AVX_512F, CPU_AVX_512IFMA, CPU_AVX_512IFMA512, CPU_AVX_512PF, CPU_AVX_512VBMI, CPU_AVX_512VBMI2, CPU_AVX_512VL, CPU_AVX_512VNNI, CPU_AVX_512VPOPCNTDQ, CPU_FMA3, CPU_FP16, CPU_MAX_FEATURE, CPU_MMX, CPU_NEON, CPU_POPCNT, CPU_SSE, CPU_SSE2, CPU_SSE3, CPU_SSE4_1, CPU_SSE4_2, CPU_SSSE3, CPU_VSX, CPU_VSX3, CVDataType, CV_16S, CV_16SC1, CV_16SC2, CV_16SC3, CV_16SC4, CV_16U, CV_16UC1, CV_16UC2, CV_16UC3, CV_16UC4, CV_32F, CV_32FC1, CV_32FC2, CV_32FC3, CV_32FC4, CV_32S, CV_32SC1, CV_32SC2, CV_32SC3, CV_32SC4, CV_64F, CV_64FC1, CV_64FC2, CV_64FC3, CV_64FC4, CV_8S, CV_8SC1, CV_8SC2, CV_8SC3, CV_8SC4, CV_8U, CV_8UC1, CV_8UC2, CV_8UC3, CV_8UC4, CV_XADD, CamShift, Canny, CascadeClassifier, Cholesky, Circle, CmpTypes, ColorConversionCodes, ConnectedComponentsAlgorithmsTypes, ConnectedComponentsTypes, ContourApproximationModes, CpuFeatures, DATA_AS_COL, DATA_AS_ROW, DCT_INVERSE, DCT_ROWS, DECOMP_CHOLESKY, DECOMP_EIG, DECOMP_LU, DECOMP_NORMAL, DECOMP_QR, DECOMP_SVD, DEFAULT, DEFAULT_NLEVELS, DEPTH_MASK, DESCR_FORMAT_COL_BY_COL, DESCR_FORMAT_ROW_BY_ROW, DFT_COMPLEX_INPUT, DFT_COMPLEX_OUTPUT, DFT_INVERSE, DFT_REAL_OUTPUT, DFT_ROWS, DFT_SCALE, DIST_C, DIST_FAIR, DIST_HUBER, DIST_L1, DIST_L12, DIST_L2, DIST_LABEL_CCOMP, DIST_LABEL_PIXEL, DIST_MASK_3, DIST_MASK_5, DIST_MASK_PRECISE, DIST_USER, DIST_WELSCH, DMatchVector, DMatchVectorVector, DNN_BACKEND_DEFAULT, DNN_BACKEND_HALIDE, DNN_BACKEND_INFERENCE_ENGINE, DNN_BACKEND_OPENCV, DNN_BACKEND_VKCOM, DNN_TARGET_CPU, DNN_TARGET_FPGA, DNN_TARGET_MYRIAD, DNN_TARGET_OPENCL, DNN_TARGET_OPENCL_FP16, DNN_TARGET_VULKAN, DRAW_OVER_OUTIMG, DRAW_RICH_KEYPOINTS, DecompTypes, DescriptorMatcher, DescriptorStorageFormat, DftFlags, DistanceTransformLabelTypes, DistanceTransformMasks, DistanceTypes, DoubleVector, DrawMatchesFlags, DynamicBitset, EMD, EmscriptenEmbindInstance, ErrorCallback$1 as ErrorCallback, Exception, FILLED, FILTER_SCHARR, FLANNBASED, FLOODFILL_FIXED_RANGE, FLOODFILL_MASK_ONLY, FM_7POINT, FM_8POINT, FM_LMEDS, FM_RANSAC, FONT_HERSHEY_COMPLEX, FONT_HERSHEY_COMPLEX_SMALL, FONT_HERSHEY_DUPLEX, FONT_HERSHEY_PLAIN, FONT_HERSHEY_SCRIPT_COMPLEX, FONT_HERSHEY_SCRIPT_SIMPLEX, FONT_HERSHEY_SIMPLEX, FONT_HERSHEY_TRIPLEX, FONT_ITALIC, FS_createDataFile, Feature2D, FileNode, FileStorage, Flags, FlannBasedMatcher, FloatVector, FloodFillFlags, GC_BGD, GC_EVAL, GC_EVAL_FREEZE_MODEL, GC_FGD, GC_INIT_WITH_MASK, GC_INIT_WITH_RECT, GC_PR_BGD, GC_PR_FGD, GEMM_1_T, GEMM_2_T, GEMM_3_T, GaussianBlur, GemmFlags, GrabCutClasses, GrabCutModes, HISTCMP_BHATTACHARYYA, HISTCMP_CHISQR, HISTCMP_CHISQR_ALT, HISTCMP_CORREL, HISTCMP_HELLINGER, HISTCMP_INTERSECT, HISTCMP_KL_DIV, HOGDescriptor, HOUGH_GRADIENT, HOUGH_MULTI_SCALE, HOUGH_PROBABILISTIC, HOUGH_STANDARD, HandEyeCalibrationMethod, HersheyFonts, HistCompMethods, HistogramNormType, HoughCircles, HoughLines, HoughLinesP, HoughLinesPointSet, HoughModes, HuMoments, INPAINT_NS, INPAINT_TELEA, INTERSECT_FULL, INTERSECT_NONE, INTERSECT_PARTIAL, INTER_AREA, INTER_BITS, INTER_BITS2, INTER_CUBIC, INTER_LANCZOS4, INTER_LINEAR, INTER_LINEAR_EXACT, INTER_MAX, INTER_NEAREST, INTER_TAB_SIZE, INTER_TAB_SIZE2, ImageData$1 as ImageData, IntVector, InternalError, InterpolationFlags, InterpolationMasks, KeyPointVector, L2Hys, LINE_4, LINE_8, LINE_AA, LMEDS, LSD_REFINE_ADV, LSD_REFINE_NONE, LSD_REFINE_STD, LU, LUT, Laplacian, LineSegmentDetectorModes, LineTypes, Logger, LshStats, LshTable, MAGIC_MASK, MAGIC_VAL, MARKER_CROSS, MARKER_DIAMOND, MARKER_SQUARE, MARKER_STAR, MARKER_TILTED_CROSS, MARKER_TRIANGLE_DOWN, MARKER_TRIANGLE_UP, MORPH_BLACKHAT, MORPH_CLOSE, MORPH_CROSS, MORPH_DILATE, MORPH_ELLIPSE, MORPH_ERODE, MORPH_GRADIENT, MORPH_HITMISS, MORPH_OPEN, MORPH_RECT, MORPH_TOPHAT, MOTION_AFFINE, MOTION_EUCLIDEAN, MOTION_HOMOGRAPHY, MOTION_TRANSLATION, Mahalanobis, MarkerTypes, Mat, Mat as InputArray, Mat as InputOutputArray, Mat as OutputArray, Mat3, Mat4, MatAllocator, MatCommaInitializer_, MatConstIterator_, MatExpr, MatIterator_, MatOp, MatSize, MatStep, MatVector, MatVector as InputArrayOfArrays, MatVector as InputOutputArrayOfArrays, MatVector as OutputArrayOfArrays, MatcherType, Matrix, Matx, Matx_AddOp, Matx_DivOp, Matx_MatMulOp, Matx_MulOp, Matx_ScaleOp, Matx_SubOp, Matx_TOp, MinMaxLoc, Moments, MorphShapes, MorphTypes, NMSBoxes, NORM_HAMMING, NORM_HAMMING2, NORM_INF, NORM_L1, NORM_L2, NORM_L2SQR, NORM_MINMAX, NORM_RELATIVE, NORM_TYPE_MASK, NOT_DRAW_SINGLE_POINTS, Net, Node$1 as Node, NormTypes, OPTFLOW_FARNEBACK_GAUSSIAN, OPTFLOW_LK_GET_MIN_EIGENVALS, OPTFLOW_USE_INITIAL_FLOW, ORB, ORB_FAST_SCORE, ORB_HARRIS_SCORE, PCA, PCABackProject, PCACompute, PCAProject, PROJ_SPHERICAL_EQRECT, PROJ_SPHERICAL_ORTHO, PSNR, Point, Point as KeyPoint, Point as Point2f, Point as Point2l, Point3_, PointVector, Point_, Ptr, PureVirtualError, RANSAC, RETR_CCOMP, RETR_EXTERNAL, RETR_FLOODFILL, RETR_LIST, RETR_TREE, RHO, ROTATE_180, ROTATE_90_CLOCKWISE, ROTATE_90_COUNTERCLOCKWISE, RQDecomp3x3, Range$1 as Range, Rect, Rect as Rect_, RectVector, RectanglesIntersectTypes, RetrievalModes, Rodrigues, RotateFlags, RotatedRect, SOLVEPNP_AP3P, SOLVEPNP_DLS, SOLVEPNP_EPNP, SOLVEPNP_IPPE, SOLVEPNP_IPPE_SQUARE, SOLVEPNP_ITERATIVE, SOLVEPNP_P3P, SOLVEPNP_UPNP, SORT_ASCENDING, SORT_DESCENDING, SORT_EVERY_COLUMN, SORT_EVERY_ROW, SUBMATRIX_FLAG, SVBackSubst, SVDecomp, Scalar, Scalar as GScalar, Scharr, ShapeMatchModes, Size, Size as Point2d, Size as Size2d, Size as Size2f, Size as Size2l, Sobel, SolvePnPMethod, SortFlags, SpecialFilter, SpeedLevel, THRESH_BINARY, THRESH_BINARY_INV, THRESH_MASK, THRESH_OTSU, THRESH_TOZERO, THRESH_TOZERO_INV, THRESH_TRIANGLE, THRESH_TRUNC, TM_CCOEFF, TM_CCOEFF_NORMED, TM_CCORR, TM_CCORR_NORMED, TM_SQDIFF, TM_SQDIFF_NORMED, TYPE_MASK, Target, TemplateMatchModes, TermCriteria, TermCriteria_COUNT, TermCriteria_EPS, TermCriteria_MAX_ITER, ThresholdTypes, Tracker, TrackerMIL, UMat, UMatData, UMatUsageFlags, USE_AVG, UnboundTypeError, UndistortTypes, Vec, Vec3, Vec3d, Vector, VideoCapture, WARP_FILL_OUTLIERS, WARP_INVERSE_MAP, WARP_POLAR_LINEAR, WARP_POLAR_LOG, WarpPolarMode, _EqPredicate, _T2, _Tp, absdiff, adaptiveThreshold, add, addWeighted, alignPtr, alignSize, approxPolyDP, arcLength, arrowedLine, batchDistance, bilateralFilter, bitwise_and, bitwise_not, bitwise_or, bitwise_xor, blendLinear, blobFromImage, blobFromImages, blur$1 as blur, bool, borderInterpolate, boundingRect, boxFilter, boxPoints, buildOpticalFlowPyramid, buildPyramid, calcBackProject, calcCovarMatrix, calcHist, calcOpticalFlowFarneback, calcOpticalFlowPyrLK, calibrateCamera, calibrateCameraRO, calibrateHandEye, calibrationMatrixValues, cartToPolar, channels, checkChessboard, checkHardwareSupport, checkRange, circle, clipLine, cols, compare, compareHist, completeSymm, composeRT, computeCorrespondEpilines, computeECC, connectedComponents, connectedComponentsWithStats, contourArea, convertFp16, convertMaps, convertPointsFromHomogeneous, convertPointsHomogeneous, convertPointsToHomogeneous, convertScaleAbs, convexHull, convexityDefects, copyMakeBorder, copyTo, cornerEigenValsAndVecs, cornerHarris, cornerMinEigenVal, cornerSubPix, correctMatches, countNonZero, createCLAHE, createFaceDetectionMaskGenerator, createGeneralizedHoughBallard, createGeneralizedHoughGuil, createLineSegmentDetector, cubeRoot, cvCeil, cvFloor, cvIsInf, cvIsNaN, cvRound, cv_abs, cvhalDFT, cvtColor, cvtColorTwoPlane, dct, decomposeEssentialMat, decomposeHomographyMat, decomposeProjectionMatrix, demosaicing, determinant, dft, diag_type, dilate, distanceTransform, divUp, divide, double, drawChessboardCorners, drawContours, drawFrameAxes, drawKeypoints, drawMarker, drawMatches, dumpInputArray, dumpInputArrayOfArrays, dumpInputOutputArray, dumpInputOutputArrayOfArrays, eigen, eigenNonSymmetric, ellipse, ellipse1, ellipse2Poly, equalizeHist, erode, error, estimateAffine2D, estimateAffine3D, estimateAffinePartial2D, estimateRigidTransform, exceptionFromPtr, exp, extractChannel, fastAtan2, fastFree, fastMalloc, fillConvexPoly, fillPoly, filter2D, filterHomographyDecompByVisibleRefpoints, filterSpeckles, find4QuadCornerSubpix, findChessboardCorners, findChessboardCornersSB, findCirclesGrid, findContours, findEssentialMat, findFundamentalMat, findHomography, findNonZero, findTransformECC, fisheye_initUndistortRectifyMap, fitEllipse, fitEllipseAMS, fitEllipseDirect, fitLine, flip, float, float16_t, float_type, floodFill, flushPendingDeletes, forEach_impl, gemm, getAffineTransform, getAvailableBackends, getAvailableTargets, getBuildInformation, getCPUFeaturesLine, getCPUTickCount, getDefaultNewCameraMatrix, getDerivKernels, getElemSize, getFontScaleFromHeight, getGaborKernel, getGaussianKernel, getHardwareFeatureName, getInheritedInstanceCount, getLiveInheritedInstances, getNumThreads, getNumberOfCPUs, getOptimalDFTSize, getOptimalNewCameraMatrix, getPerspectiveTransform, getRectSubPix, getRotationMatrix2D, getStructuringElement, getTextSize, getThreadNum, getTickCount, getTickFrequency, getValidDisparityROI, getVersionMajor, getVersionMinor, getVersionRevision, getVersionString, glob, goodFeaturesToTrack, grabCut, groupRectangles, groupRectangles_meanshift, hal_ni_dct2D, hal_ni_dctFree2D, hal_ni_dctInit2D, hal_ni_dft1D, hal_ni_dft2D, hal_ni_dftFree1D, hal_ni_dftFree2D, hal_ni_dftInit1D, hal_ni_dftInit2D, hal_ni_minMaxIdx, hconcat, idct, idft, imagesFromBlob, imread, imshow, inRange, initCameraMatrix2D, initUndistortRectifyMap, initWideAngleProjMap, inpaint, insertChannel, int, int32_t, int64, int64_t, integral, intersectConvexConvex, invert, invertAffineTransform, isContourConvex, kArray, kBitsetHash, kHash, kmeans, line, linearPolar, log, logPolar, magnitude, matFromArray, matFromImageData, matMulDeriv, matchShapes, matchTemplate, max, mean, meanShift, meanStdDev, medianBlur, merge, min, minAreaRect, minEnclosingCircle, minEnclosingTriangle, minMaxIdx, minMaxLoc, mixChannels, moments, morphologyDefaultBorderValue, morphologyEx, mulSpectrums, mulTransposed, multiply, norm, norm1, normInf, normL1, normL2Sqr, normalize, onRuntimeInitialized, parallel_for_, partition, patchNaNs, perspectiveTransform, phase, pointPolygonTest, polarToCart, polylines, pow, preCornerDetect, projectPoints, putText, pyrDown, pyrMeanShiftFiltering, pyrUp, randShuffle, randn, randu, readNet, readNetFromCaffe, readNetFromDarknet, readNetFromModelOptimizer, readNetFromONNX, readNetFromTensorflow, readNetFromTorch, readOpticalFlow, readTensorFromONNX, readTorchBlob, recoverPose, rectangle, rectify3Collinear, redirectError, reduce, remap, repeat, reprojectImageTo3D, resize, rotate, rotatedRectangleIntersection, roundUp, rows, sampsonDistance, saturate_cast, scaleAdd, schar, sepFilter2D, setBreakOnError, setDelayFunction, setIdentity, setNumThreads, setRNGSeed, setUseOptimized, short, shortdim, shrinkCaffeModel, size_t, softdouble, softfloat, solve, solveCubic, solveP3P, solvePnP, solvePnPGeneric, solvePnPRansac, solvePnPRefineLM, solvePnPRefineVVS, solvePoly, sort, sortIdx, spatialGradient, split, sqrBoxFilter, sqrt, stereoCalibrate, stereoRectify, stereoRectifyUncalibrated, subtract, sum, tempfile, testAsyncArray, testAsyncException, theRNG, threshold, trace, transform, transpose, triangulatePoints, typename, uchar, uint32_t, uint64, uint64_t, undistort, undistortPoints, unsigned, useOptimized, ushort, validateDisparity, vconcat, warpAffine, warpPerspective, warpPolar, watershed, wrapperEMD, writeOpticalFlow, writeTextGraph };
}

export {
	Algorithm$1 as Algorithm,
	ErrorCallback$1 as ErrorCallback,
	ImageData$1 as ImageData,
	Mat as InputArray,
	Mat as InputOutputArray,
	Mat as OutputArray,
	MatVector as InputArrayOfArrays,
	MatVector as InputOutputArrayOfArrays,
	MatVector as OutputArrayOfArrays,
	Node$1 as Node,
	Point as KeyPoint,
	Point as Point2f,
	Point as Point2l,
	Range$1 as Range,
	Rect as Rect_,
	Scalar as GScalar,
	Size as Point2d,
	Size as Size2d,
	Size as Size2f,
	Size as Size2l,
	blur$1 as blur,
};

export {};
